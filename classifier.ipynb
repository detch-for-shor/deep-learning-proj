{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports and Preliminary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/luciahrovatin/Desktop/deep-learning-proj/venv/lib/python3.9/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from torch import Tensor\n",
    "from torchvision import models\n",
    "from torch.autograd import Variable\n",
    "from torch.nn import CrossEntropyLoss\n",
    "from torch.optim import RMSprop, Adagrad\n",
    "from overrides import overrides, final\n",
    "from abc import abstractmethod"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The SymNet architecture presents an overall training function based on compositionality. Hence, different modules are embedded, as reported below following the original implementation of [Zhang](https://arxiv.org/abs/1904.04663) <i>et al.</i> (see § 3.3). \n",
    "\n",
    "$$ \\displaystyle\\min_{C^s, C^t, C^{st}} \\large \\mathcal{E}_{task}^{(s)}(G, C^{(s)}) +  \\large\\mathcal{E}_{task}^{(t)}(G, C^{(t)}) + \\large\\mathcal{E}_{domain}^{(st)}(G, C^{(st)})$$\n",
    "$$\\displaystyle\\min_{G} \\large \\mathcal{F}_{category}^{(st)}(G, C^{(st)}) + \\lambda [\\large \\mathcal{F}_{domain}^{(st)}(G, C^{(st)}) + \\large \\mathcal{M}^{(st)}(G, C^{(st)})]$$\n",
    "\n",
    "In order to reproduce the original implementation and its multiple losses, a tree structure has been chosen. The root is represented by the class <code>_Loss</code>, which inherits from the <code>[torch.nn.Module](https://pytorch.org/docs/stable/generated/torch.nn.Module.html)</code> and has two subclasses:\n",
    "<code>EntropyMinimizationLoss</code> and <code>SplitLoss</code>. The former is used to implement the <i>Entropy Minimization Principle</i> (§ 3.2.1 [Zhang](https://arxiv.org/abs/1904.04663) <i>et al.</i>), whereas the latter is further subdefined in modules, as depicted below:\n",
    "\n",
    "```\n",
    "_Loss\n",
    "│\n",
    "├── EntropyMinimizationLoss\n",
    "│ \n",
    "└── SplitLoss\n",
    "    │   \n",
    "    └── SplitCrossEntropyLoss\n",
    "    |\n",
    "    └── DomainDiscriminationLoss\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class _Loss(torch.nn.Module):\n",
    "    \n",
    "    _THRESHOLD = 1e-20\n",
    "    \n",
    "    def __init__(self):\n",
    "        super(_Loss, self).__init__()\n",
    "        \n",
    "    def forward(self, input: Tensor):\n",
    "        prob = self.to_softmax(input)\n",
    "        return self.loss(prob)\n",
    "        \n",
    "    @final\n",
    "    def add_threshold(self, prob: Tensor):\n",
    "        '''\n",
    "        Check whether the probability distribution after the softmax \n",
    "        is equal to 0 in any cell. If this holds, a standard threshold\n",
    "        is added in order to avoid log(0) case. \n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        prob: Tensor\n",
    "            output tensor of the softmax operation\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        Tensor\n",
    "            updated tensor (in case the condition above holds)\n",
    "        '''\n",
    "        zeros = (prob == 0)\n",
    "        if torch.any(zeros):\n",
    "            thre_tensor = torch.zeros(zeros.shape)\n",
    "            thre_tensor[zeros] = self._THRESHOLD\n",
    "            prob += thre_tensor\n",
    "        return prob\n",
    "    \n",
    "    def to_softmax(self, features: Tensor):\n",
    "        '''\n",
    "        Apply the softmax operation on the features tensor, \n",
    "        being the output of a feature extractor. \n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        features: Tensor\n",
    "            input tensor of the softmax operation\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        Tensor\n",
    "            probability distribution with (possible) threshold\n",
    "        '''\n",
    "        prob = F.softmax(features, dim=1)\n",
    "        return self.add_threshold(prob)\n",
    "    \n",
    "    @abstractmethod\n",
    "    def loss(self, prob: Tensor):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Entropy Minimization Principle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "The Entropy Minimization objective is here ([Zhang](https://arxiv.org/abs/1904.04663) <i>et al.</i>, § 3.2.1) adopted as regularizer. Specifically, it updates the feature extractor (<i>G</i>) and enhances the discrimination among task categories. This avoids having target samples stucked into wrong category predictions during the early training stages.\n",
    "\n",
    "$$\\displaystyle\\min_{G}\\mathcal{M}^{(st)}(G, C^{(t)}) = -\\frac{1}{n_t}\\sum_{j=1}^{n_t}\\sum_{k=1}^{K}q_k^{(st)}(x_j^{(t)}log\\bigg(q_k^{(st)}(x_j^{(t)})\\bigg)$$\n",
    "<br /><br />\n",
    "<b>TODO:</b> Add formulas and check this all again --> REGULARIZER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EntropyMinimizationLoss(_Loss):\n",
    "    \n",
    "    def __init__(self, n_classes: int):\n",
    "        super(EntropyMinimizationLoss, self).__init__()\n",
    "        self.n_classes = n_classes\n",
    "    \n",
    "    @overrides\n",
    "    def loss(self, prob: Tensor):\n",
    "        prob_source = prob[:, :self.n_classes]\n",
    "        prob_target = prob[:, self.n_classes:]\n",
    "        prob_sum = prob_source + prob_target\n",
    "        return -(prob_sum.log().mul(prob_sum).sum(dim=1).mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cross-Entropy Based Losses"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The losses contained in the overall training objective described aforeahead are implemented starting from the class <code>_CrossEntropyLoss_</code>. Thus, they are either normal cross entropy loss or a combination of two losses (i.e., <i>two-way cross entropy loss</i>). Specifically, two classifiers have been implemented to solve the classification task on the <i>source</i> ($ C^{(s)} $) and <i>target</i> ($ C^{(t)} $) domain. A <code>task classifier</code>, $ C^{(s)} $, is trained using the following cross-entropy loss over the <i>labeled</i> source samples: <br />\n",
    "$$ \\displaystyle\\min_{C^{(s)}}\\mathcal{E}_{task}^{(s)}(G, C^{(s)})=-\\frac{1}{n_{s}}\\sum_{i=1}^{n_{s}}log\\bigg(p_{y_{i}^{(s)}}^{(s)}({\\bf x}_{i}^{(s)})\\bigg) $$\n",
    "<br />In the formula above, $G$ represents the <code>feature extractor</code>, ${\\bf x}_{i}^{(s)}$ the output vector of $ C^{(s)} $, and $p_{y_{i}^{(s)}}^{(s)}({\\bf x}_{i}^{(s)}) \\in [0,1]^{K}$ the distribution of probability after the <code>  [softmax](https://pytorch.org/docs/stable/generated/torch.nn.functional.softmax.html)</code>  operation. \n",
    "\n",
    "Since target samples are <i>unlabeled</i>, there exists no direct\n",
    "supervision signals to learn a task classifier $ C^{(t)} $. Therefore, [Zhang](https://arxiv.org/abs/1904.04663) <i>et al.</i> leverage the <i>labeled</i> source samples by using the following cross-entropy loss: \n",
    "$$ \\displaystyle\\min_{C^{(t)}}\\mathcal{E}_{task}^{(t)}(G, C^{(t)})=-\\frac{1}{n_{s}}\\sum_{i=1}^{n_{s}}log\\bigg(p_{y_{i}^{(s)}}^{(t)}({\\bf x}_{i}^{(s)})\\bigg) $$\n",
    "<br />It is worth noticing that $C^{(t)}$ will be distinguishable from $C^{(s)}$ through the domain discrimination training of the classifier $C^{(st)}$, reported below. Moreover, [Zhang](https://arxiv.org/abs/1904.04663) <i>et al.</i> stress the use of <i>labeled</i> source samples to enhance $C^{(t)}$ performance in discriminating among task categories.\n",
    "\n",
    "$$ \\displaystyle\\min_{C^{(st)}}\\mathcal{E}_{domain}^{(st)}(G, C^{(t)})=-\\frac{1}{n_{t}}\\sum_{j=1}^{n_t}log\\bigg(\\sum_{k=1}^{K} p_{k+K}^{(st)}({\\bf x}_{j}^{(t)})\\bigg)-\\frac{1}{n_s}\\sum_{i=1}^{n_s}log\\bigg(\\sum_{k=1}^{K}p_{k}^{(st)}({\\bf x}_{i}^{(s)})\\bigg) $$\n",
    "\n",
    "Furthermore a two-level confusion loss is applied. Whereas a first loss is category level and relies on the source labels, the second loss is domain level and focuses on the target. The formula are respectely: \n",
    "\n",
    "$$ \\displaystyle\\min_{G}\\mathcal{F}_{category}^{(st)}(G, C^{(t)})=-\\frac{1}{2n_{s}}\\sum_{i=1}^{n_s}log(p_{y_i^s+K}^{(st)}({\\bf x}_{i}^{(s)}))-\\frac{1}{2n_s}\\sum_{i=1}^{n_s}log(p_{y_i^s}^{(st)}({\\bf x}_{i}^{(s)})) $$\n",
    "\n",
    "$$ \\displaystyle\\min_{G}\\mathcal{F}_{domain}^{(st)}(G, C^{(t)})=-\\frac{1}{2n_{t}}\\sum_{j=1}^{n_t}log\\bigg(\\sum_{k=1}^{K} p_{k+K}^{(st)}({\\bf x}_{j}^{(t)})\\bigg)-\\frac{1}{2n_t}\\sum_{j=1}^{n_t}log\\bigg(\\sum_{k=1}^{K}p_{k}^{(st)}({\\bf x}_{j}^{(t)})\\bigg) $$\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>TODO:</b> Add the two-way cross-entropy loss with definition of domain discrimination and confusion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SplitLoss(_Loss):\n",
    "    \n",
    "    def __init__(self, n_classes: int, source: bool, split_first: bool):\n",
    "        super(SplitLoss, self).__init__()\n",
    "        self.n_classes = n_classes\n",
    "        self._is_source = source\n",
    "        self._split_first = split_first\n",
    "    \n",
    "    @overrides\n",
    "    def to_softmax(self, features: Tensor):\n",
    "        if self._split_first:\n",
    "            prob = self.split_vector(features)\n",
    "            prob = F.softmax(prob, dim=1)\n",
    "        else:\n",
    "            prob = F.softmax(features, dim=1)\n",
    "            prob = self.split_vector(prob)\n",
    "        return self.add_threshold(prob)\n",
    "    \n",
    "    @final\n",
    "    def split_vector(self, prob: Tensor):\n",
    "        return prob[:,:self.n_classes] if self._is_source else prob[:,self.n_classes:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SplitCrossEntropyLoss(SplitLoss):\n",
    "    \n",
    "    def _get_y_labels(self):\n",
    "        return self._y_labels\n",
    "    def _set_y_labels(self, y_labels: Variable):\n",
    "        if not all(y < self.n_classes for y in y_labels):\n",
    "            raise ValueError('Expected all y labels < n_classes')\n",
    "        self._y_labels = y_labels\n",
    "    y_labels = property(fget=_get_y_labels, fset=_set_y_labels)\n",
    "    \n",
    "    def __init__(self, n_classes: int, source: bool, split_first: bool):\n",
    "        super(SplitCrossEntropyLoss, self).__init__(n_classes, source, split_first)\n",
    "        self.cross_entropy_loss = torch.nn.CrossEntropyLoss()\n",
    "    \n",
    "    @overrides\n",
    "    def loss(self, prob: Tensor):\n",
    "        '''Computes cross-entropy loss w.r.t. ground-truth (y label)'''\n",
    "        return self.cross_entropy_loss(prob, self.y_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DomainDiscriminationLoss(SplitLoss):\n",
    "    \n",
    "    def __init__(self, n_classes: int, source: bool):\n",
    "        super(DomainDiscriminationLoss, self).__init__(n_classes, source, False)\n",
    "        \n",
    "    @overrides\n",
    "    def loss(self, prob: Tensor):\n",
    "        return -(prob.sum(dim=1).log().mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Accordingly to the [Zhang](https://arxiv.org/abs/1904.04663) <i>et al.</i> (see § 3.3) definition of the overall model's objective, the class <code>TrainingObjectives</code> recall the aforementioned losses. Moreover, the $\\lambda$ trade-off parameter is introduced to suppress noisy signals for domain confusion loss and entropy. $\\lambda$ value depends on the number of epochs, as it is interatively computer through the following formula:\n",
    "$$\\lambda = \\frac{2}{1 + e^{(- \\gamma \\cdot \\frac{ep}{n_{ep}})}} - 1$$\n",
    "where $\\gamma$ is usually set to $10$ ([Zhang](https://arxiv.org/abs/1904.04663) <i>et al.</i>, see § 4.1) and $\\frac{ep}{n_{ep}}$ is iteratively updated at each epoch, as it represents the current epoch over the total. Therefore, $\\lambda$ parameter will start from $0$ and gradually increase (i.e., $\\displaystyle\\lim_{ep \\rightarrow n_{ep}}\\lambda = 1$). Thus, the penalty of $\\lambda$ on domain confusion loss and entropy decreases over time. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TrainingObjectives:\n",
    "    \n",
    "    @staticmethod\n",
    "    def domain_discrimination_loss(src_dom_discrim_loss, tgt_dom_discrim_loss):\n",
    "        return src_dom_discrim_loss + tgt_dom_discrim_loss\n",
    "    \n",
    "    @staticmethod\n",
    "    def category_confusion_loss(src_cat_conf_loss, tgt_cat_conf_loss):\n",
    "        return 0.5 * (src_cat_conf_loss + tgt_cat_conf_loss)\n",
    "    \n",
    "    @staticmethod\n",
    "    def domain_confusion_loss(src_dom_conf_loss, tgt_dom_conf_loss):\n",
    "        return 0.5 * (src_dom_conf_loss + tgt_dom_conf_loss)\n",
    "    \n",
    "    @staticmethod\n",
    "    def overall_classifier_loss(src_task_class_loss, tgt_task_class_loss, domain_discrim_loss):\n",
    "        return src_task_class_loss + tgt_task_class_loss + domain_discrim_loss\n",
    "    \n",
    "    @staticmethod\n",
    "    def overall_generator_loss(cat_conf_loss, dom_conf_loss, tgt_entropy_loss, curr_epoch, tot_epochs):\n",
    "        lambda_trade_off = 2 / (1 + math.exp(-1 * 10 * curr_epoch / tot_epochs)) - 1\n",
    "        return cat_conf_loss + lambda_trade_off * (dom_conf_loss + tgt_entropy_loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Examples testing the correctness of the class implemented above:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example of batch size and num of classes\n",
    "batch_size = 2\n",
    "num_classes = 3\n",
    "\n",
    "# Example of model's outcome after batch of inputs from source domain\n",
    "# X_source_features = torch.randn(batch_size, num_classes * 2)\n",
    "X_source_features = torch.tensor(\n",
    "    [[-1.3382,  0.6833,  1.3363, -0.0465,  0.8953, -1.4505],\n",
    "    [ 0.2133, -1.5612, -1.6918, -1.9907,  0.9956, -0.2287]])\n",
    "\n",
    "# Example of model's outcome after batch of inputs from target domain\n",
    "# X_target_features = torch.randn(batch_size, num_classes * 2)\n",
    "X_target_features = torch.tensor(\n",
    "    [[ 0.3109, -1.7531, -1.5460,  0.3308,  1.3116, -0.3035],\n",
    "    [ 0.2777, -0.2101,  0.1629,  1.6425,  0.8126,  0.5605]])\n",
    "\n",
    "# Example of labels associated with batch of inputs from source domain\n",
    "y_source_labels = torch.tensor([2, 0])\n",
    "y_source_labels_var = Variable(y_source_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overall Classifier Loss = tensor(3.3036)\n",
      "Overall Generator Loss = tensor(2.8974)\n"
     ]
    }
   ],
   "source": [
    "# Source Task Classifier Loss\n",
    "source_task_class_loss = SplitCrossEntropyLoss(n_classes=3, source=True, split_first=True)\n",
    "source_task_class_loss.y_labels = y_source_labels_var\n",
    "_src_task_class_loss = source_task_class_loss(X_source_features)\n",
    "\n",
    "# (Cross-Domain) Target Task Classifier Loss\n",
    "target_task_class_loss = SplitCrossEntropyLoss(n_classes=3, source=False, split_first=True)\n",
    "target_task_class_loss.y_labels = y_source_labels_var\n",
    "_tgt_task_class_loss = target_task_class_loss(X_source_features)\n",
    "\n",
    "# Domain Discrimination Loss\n",
    "source_dom_discrim_loss = DomainDiscriminationLoss(n_classes=3, source=True)\n",
    "target_dom_discrim_loss = DomainDiscriminationLoss(n_classes=3, source=False)\n",
    "_src_dom_discrim_loss = source_dom_discrim_loss(X_source_features)\n",
    "_tgt_dom_discrim_loss = target_dom_discrim_loss(X_target_features)\n",
    "_domain_discrim_loss = TrainingObjectives.domain_discrimination_loss(_src_dom_discrim_loss, _tgt_dom_discrim_loss)\n",
    "\n",
    "# Category-level Confusion Loss\n",
    "source_cat_conf_loss = SplitCrossEntropyLoss(n_classes=3, source=True, split_first=False)\n",
    "target_cat_conf_loss = SplitCrossEntropyLoss(n_classes=3, source=False, split_first=False)\n",
    "source_cat_conf_loss.y_labels = y_source_labels_var\n",
    "target_cat_conf_loss.y_labels = y_source_labels_var\n",
    "_src_cat_conf_loss = source_cat_conf_loss(X_source_features)\n",
    "_tgt_cat_conf_loss = target_cat_conf_loss(X_source_features)\n",
    "_category_conf_loss = TrainingObjectives.category_confusion_loss(_src_cat_conf_loss, _tgt_cat_conf_loss)\n",
    "\n",
    "# Domain-level Confusion Loss\n",
    "source_dom_conf_loss = DomainDiscriminationLoss(n_classes=3, source=True)\n",
    "target_dom_conf_loss = DomainDiscriminationLoss(n_classes=3, source=False)\n",
    "_src_dom_conf_loss = source_dom_conf_loss(X_target_features)\n",
    "_tgt_dom_conf_loss = target_dom_conf_loss(X_target_features)\n",
    "_domain_conf_loss = TrainingObjectives.domain_confusion_loss(_src_dom_conf_loss, _tgt_dom_conf_loss)\n",
    "\n",
    "# Entropy Minimization Principle\n",
    "target_entropy_loss = EntropyMinimizationLoss(n_classes=3)\n",
    "_tgt_entropy_loss = target_entropy_loss(X_target_features)\n",
    "\n",
    "# Overall Classifier Loss\n",
    "_overall_classifier_loss = TrainingObjectives.overall_classifier_loss(_src_task_class_loss, _tgt_task_class_loss, _domain_discrim_loss)\n",
    "print('Overall Classifier Loss =', _overall_classifier_loss)\n",
    "\n",
    "# Overall Feature Extractor Loss\n",
    "_overall_generator_loss = TrainingObjectives.overall_generator_loss(_category_conf_loss, _domain_conf_loss, _tgt_entropy_loss, 432, 1000)\n",
    "print('Overall Generator Loss =', _overall_generator_loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Extractor (<i>G</i>) - Resnet18"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>TODO:</b> Describe feature extractor, how it's built up and how it works"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeatureExtractor:\n",
    "    \n",
    "    def __init__(self, n_classes: int, n_layers_trained: int, model='resnet18', optimizer='rmsprop', lr=0.01, weight_decay=0):\n",
    "        \n",
    "        # Upload pretrained model \n",
    "        if model.lower() == 'resnet18': \n",
    "            self.model = models.resnet18(pretrained=True)\n",
    "        elif model.lower() == 'resnet50': \n",
    "            self.model = models.resnet50(pretrained=True)\n",
    "        else:\n",
    "            raise ValueError('Unknown model')\n",
    "        \n",
    "        # Modify last fully-connected layer\n",
    "        self.model.fc = torch.nn.Linear(\n",
    "            in_features = self.model.fc.in_features, \n",
    "            out_features = n_classes * 2\n",
    "        )\n",
    "        \n",
    "        # Freeze pretrained layers\n",
    "        params = list(self.model.parameters())\n",
    "        for i in range(len(params)):\n",
    "            n_layers_frozen = len(params) - i - 1\n",
    "            params[i].requires_grad = (n_layers_frozen < n_layers_trained)\n",
    "        params_to_train = filter(lambda p: p.requires_grad, self.model.parameters())\n",
    "        \n",
    "        # Initialize optimizer\n",
    "        if optimizer.lower() == 'rmsprop':\n",
    "            self.optim = torch.optim.RMSprop(\n",
    "                params = params_to_train,\n",
    "                lr = lr,\n",
    "                weight_decay = weight_decay\n",
    "            )\n",
    "        elif optimizer.lower() == 'adadelta':\n",
    "            self.optim = torch.optim.Adadelta(\n",
    "                params = params_to_train,\n",
    "                lr = lr,\n",
    "                weight_decay = weight_decay\n",
    "            )\n",
    "        elif optimizer.lower() == 'sgd':\n",
    "            self.optim = torch.optim.SGD(\n",
    "                params = params_to_train,\n",
    "                lr = lr,\n",
    "                weight_decay = weight_decay,\n",
    "                nesterov = True\n",
    "            )\n",
    "        else:\n",
    "            raise ValueError('Unknown optimizer')\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ModelTrainer:\n",
    "    \n",
    "    def __init__(self, model: FeatureExtractor, n_classes: int, epochs: int):\n",
    "        self.model = model \n",
    "        self.curr_epoch = 0\n",
    "        self.tot_epochs = epochs\n",
    "        self.n_classes = n_classes\n",
    "        # Task classifier losses\n",
    "        self.src_task_class_loss = SplitCrossEntropyLoss(n_classes=n_classes, source=True, split_first=True).cuda()\n",
    "        self.tgt_task_class_loss = SplitCrossEntropyLoss(n_classes=n_classes, source=False, split_first=True).cuda()\n",
    "        # Domain discrimination losses\n",
    "        self.src_dom_discrim_loss = DomainDiscriminationLoss(n_classes=n_classes, source=True).cuda()\n",
    "        self.tgt_dom_discrim_loss = DomainDiscriminationLoss(n_classes=n_classes, source=False).cuda()\n",
    "        # Category-level confusion losses\n",
    "        self.src_cat_conf_loss = SplitCrossEntropyLoss(n_classes=n_classes, source=True, split_first=False).cuda()\n",
    "        self.tgt_cat_conf_loss = SplitCrossEntropyLoss(n_classes=n_classes, source=False, split_first=False).cuda()\n",
    "        # Domain-level confusion losses\n",
    "        self.src_dom_conf_loss = DomainDiscriminationLoss(n_classes=n_classes, source=True).cuda()\n",
    "        self.tgt_dom_conf_loss = DomainDiscriminationLoss(n_classes=n_classes, source=False).cuda()\n",
    "        # Entropy minimization loss\n",
    "        self.tgt_entropy_loss = EntropyMinimizationLoss(n_classes=n_classes).cuda()\n",
    "        \n",
    "        \n",
    "    def train_one_epoch(self, source_dataloader, target_dataloader):\n",
    "        self.curr_epoch += 1\n",
    "        end_of_epoch = False\n",
    "        source_batch_loader = enumerate(source_dataloader)\n",
    "        target_batch_loader = enumerate(target_dataloader)\n",
    "        \n",
    "        # Train for current epoch\n",
    "        while not end_of_epoch:\n",
    "            try:\n",
    "                # Get next batch for both source and target\n",
    "                (X_source, y_source) = source_batch_loader.__next__()[1]\n",
    "                (X_target, _) = target_batch_loader.__next__()[1]\n",
    "            except StopIteration:\n",
    "                end_of_epoch = True\n",
    "                continue\n",
    "            \n",
    "            # Tell model go training mode\n",
    "            self.model.model.train()\n",
    "            \n",
    "            # Convert to torch.autograd variables\n",
    "            X_source_var = Variable(X_source) \n",
    "            y_source_var = Variable(y_source)\n",
    "            X_target_var = Variable(X_target)\n",
    "            \n",
    "            # Compute features for both inputs\n",
    "            X_source_features = self.model.model(X_source_var)\n",
    "            X_target_features = self.model.model(X_target_var)\n",
    "            \n",
    "            # Compute overall training objective losses\n",
    "            classifier_loss, generator_loss = self.overall_losses(\n",
    "                X_source_features, \n",
    "                X_target_features, \n",
    "                y_source_var\n",
    "            )\n",
    "            \n",
    "            self.model.optim.zero_grad()\n",
    "            classifier_loss.backward(retain_graph=True)  \n",
    "            grad_classifier_tmp = [param.grad.data.clone() for param in self.model.model.parameters()]\n",
    "            \n",
    "            self.model.optim.zero_grad()\n",
    "            generator_loss.backward()\n",
    "            grad_generator_tmp = [param.grad.data.clone() for param in self.model.model.parameters()]\n",
    "            \n",
    "            count = 0 \n",
    "            for p in self.model.model.parameters():\n",
    "                grad_tmp = p.grad.data.clone().zero_() \n",
    "                if count < 159: # FIXME: capire perché 159\n",
    "                    grad_tmp += grad_generator_tmp[count]\n",
    "                else: \n",
    "                    grad_tmp += grad_classifier_tmp[count]\n",
    "                p.grad.data = grad_tmp \n",
    "                count += 1 \n",
    "            self.model.optim.step()\n",
    "\n",
    "    def overall_losses(self, X_source_features, X_target_features, y_source_var):\n",
    "        # Source task classifier loss\n",
    "        self.src_task_class_loss.y_labels = y_source_var\n",
    "        _src_task_class_loss = self.src_task_class_loss(X_source_features)\n",
    "        \n",
    "        # (Cross-domain) Target task classifier loss\n",
    "        self.tgt_task_class_loss.y_labels = y_source_var\n",
    "        _tgt_task_class_loss = self.tgt_task_class_loss(X_source_features)\n",
    "        \n",
    "        # Domain discrimination loss\n",
    "        _src_dom_discrim_loss = self.src_dom_discrim_loss(X_source_features)\n",
    "        _tgt_dom_discrim_loss = self.tgt_dom_discrim_loss(X_target_features)\n",
    "        _domain_discrim_loss = TrainingObjectives.domain_discrimination_loss(\n",
    "            _src_dom_discrim_loss, \n",
    "            _tgt_dom_discrim_loss\n",
    "        )\n",
    "        \n",
    "        # Category-level confusion loss\n",
    "        self.src_cat_conf_loss.y_labels = y_source_var\n",
    "        self.tgt_cat_conf_loss.y_labels = y_source_var\n",
    "        _src_cat_conf_loss = self.src_cat_conf_loss(X_source_features)\n",
    "        _tgt_cat_conf_loss = self.tgt_cat_conf_loss(X_source_features)\n",
    "        _category_conf_loss = TrainingObjectives.category_confusion_loss(\n",
    "            _src_cat_conf_loss, \n",
    "            _tgt_cat_conf_loss\n",
    "        )\n",
    "        \n",
    "        # Domain-level confusion loss\n",
    "        _src_dom_conf_loss = self.src_cat_conf_loss(X_target_features)\n",
    "        _tgt_dom_conf_loss = self.tgt_cat_conf_loss(X_target_features)\n",
    "        _domain_conf_loss = TrainingObjectives.domain_confusion_loss(\n",
    "            _src_dom_conf_loss, \n",
    "            _tgt_dom_conf_loss\n",
    "        )\n",
    "\n",
    "        # Entropy minimization loss\n",
    "        _tgt_entropy_loss = self.tgt_entropy_loss(X_target_features)\n",
    "        \n",
    "        # Overall classifier loss\n",
    "        _overall_classifier_loss = TrainingObjectives.overall_classifier_loss(\n",
    "            _src_task_class_loss, \n",
    "            _tgt_task_class_loss, \n",
    "            _domain_discrim_loss\n",
    "        )\n",
    "\n",
    "        # Overall feature extractor loss\n",
    "        _overall_generator_loss = TrainingObjectives.overall_generator_loss(\n",
    "            _category_conf_loss, \n",
    "            _domain_conf_loss, \n",
    "            _tgt_entropy_loss, \n",
    "            self.curr_epoch, \n",
    "            self.tot_epochs\n",
    "        )\n",
    "        \n",
    "        return _overall_classifier_loss, _overall_generator_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[backward specification](https://stackoverflow.com/questions/46774641/what-does-the-parameter-retain-graph-mean-in-the-variables-backward-method)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "b0fa6594d8f4cbf19f97940f81e996739fb7646882a419484c72d19e05852a7e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
