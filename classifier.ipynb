{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports and Preliminary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from torch import Tensor\n",
    "from torchvision import models\n",
    "from torch.optim import RMSprop, Adagrad\n",
    "from overrides import overrides, final\n",
    "from abc import abstractmethod"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classifiers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Two classifiers have been implemented to solve the classification task on the <i>source</i> ($ C^{(s)} $) and <i>target</i> ($ C^{(t)} $) domain. The former <code>task classifier</code>, $ C^{(s)} $, is trained using the following cross-entropy loss over the <i>labeled</i> source samples: <br />\n",
    "$$ min_{C^{(s)}}\\mathcal{E}_{task}^{(s)}(G, C^{(s)})=-\\frac{1}{n_{s}}\\sum_{i=1}^{n_{s}}log\\bigg(p_{y_{i}^{(s)}}^{(s)}({\\bf x}_{i}^{(s)})\\bigg) $$\n",
    "<br />In the formula above, $G$ represents the <code>feature extractor</code>, ${\\bf x}_{i}^{(s)}$ the output vector of $ C^{(s)} $, and $p_{y_{i}^{(s)}}^{(s)}({\\bf x}_{i}^{(s)}) \\in [0,1]^{K}$ the distribution of probability after the <code>  [softmax](https://pytorch.org/docs/stable/generated/torch.nn.functional.softmax.html)</code>  operation. \n",
    "\n",
    "Since target samples are <i>unlabeled</i>, there exists no direct\n",
    "supervision signals to learn a task classifier $ C^{(t)} $. Therefore, [Zhang](https://arxiv.org/abs/1904.04663) <i>et al.</i> leverage the <i>labeled</i> source samples by using the following cross-entropy loss: \n",
    "$$ min_{C^{(t)}}\\mathcal{E}_{task}^{(t)}(G, C^{(t)})=-\\frac{1}{n_{s}}\\sum_{i=1}^{n_{s}}log\\bigg(p_{y_{i}^{(s)}}^{(t)}({\\bf x}_{i}^{(s)})\\bigg) $$\n",
    "<br />It is worth noticing that $C^{(t)}$ will be distinguishable from $C^{(s)}$ through the domain discrimination training of the classifier $C^{(st)}$. Moreover, [Zhang](https://arxiv.org/abs/1904.04663) <i>et al.</i> stress the use of <i>labeled</i> source samples to enhance $C^{(t)}$ performance in discriminating among task categories. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "class _Loss(torch.nn.Module):\n",
    "    \n",
    "    _THRESHOLD = 1e-20\n",
    "    \n",
    "    def __init__(self):\n",
    "        super(_Loss, self).__init__()\n",
    "        \n",
    "    def forward(self, input: Tensor):\n",
    "        prob = self.to_softmax(input)\n",
    "        return self.loss(prob)\n",
    "        \n",
    "    @final\n",
    "    def add_threshold(self, prob: Tensor):\n",
    "        '''\n",
    "        Check whether the probability distribution after the softmax \n",
    "        is equal to 0 in any cell. If this holds, a standard threshold\n",
    "        is added in order to avoid log(0) case. \n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        prob: Tensor\n",
    "            output tensor of the softmax operation\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        Tensor\n",
    "            updated tensor (in case the condition above holds)\n",
    "        '''\n",
    "        zeros = (prob == 0)\n",
    "        if torch.any(zeros):\n",
    "            thre_tensor = torch.zeros(zeros.shape)\n",
    "            thre_tensor[zeros] = self._THRESHOLD\n",
    "            prob += thre_tensor\n",
    "        return prob\n",
    "    \n",
    "    def to_softmax(self, features: Tensor):\n",
    "        prob = F.softmax(features, dim=1)\n",
    "        return self.add_threshold(prob)\n",
    "    \n",
    "    @abstractmethod\n",
    "    def loss(self, prob: Tensor):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "class _CrossEntropyLoss(_Loss):\n",
    "    \n",
    "    def __init__(self):\n",
    "        super(_CrossEntropyLoss, self).__init__()\n",
    "    \n",
    "    @overrides\n",
    "    def loss(self, prob: Tensor):\n",
    "        return -(prob.log().mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "class _EntropyLoss(_Loss):\n",
    "    \n",
    "    def __init__(self):\n",
    "        super(_EntropyLoss, self).__init__()\n",
    "    \n",
    "    @overrides\n",
    "    def loss(self, prob: Tensor):\n",
    "        return -(prob.log().mul(prob).sum(dim=1).mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SplitCrossEntropyLoss(_CrossEntropyLoss):\n",
    "    \n",
    "    def __init__(self, n_classes: int, source: bool):\n",
    "        super(SplitCrossEntropyLoss, self).__init__()\n",
    "        self.n_classes = n_classes\n",
    "        self._is_source = source\n",
    "    \n",
    "    @overrides\n",
    "    def to_softmax(self, features: Tensor):\n",
    "        prob = F.softmax(features, dim=1)\n",
    "        prob = self.split_softmax(prob)\n",
    "        return self.add_threshold(prob)\n",
    "    \n",
    "    @final\n",
    "    def split_softmax(self, prob: Tensor):\n",
    "        return prob[:,:self.n_classes] if self._is_source else prob[:,self.n_classes:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DomainDiscriminationLoss(SplitCrossEntropyLoss):\n",
    "    \n",
    "    def __init__(self, n_classes: int, source: bool):\n",
    "        super(DomainDiscriminationLoss, self).__init__(n_classes, source)\n",
    "        \n",
    "    @overrides\n",
    "    def loss(self, prob: Tensor):\n",
    "        return -(prob.sum(dim=1).log().mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "source_dom_class_loss = DomainDiscriminationLoss(1000, source=True)\n",
    "target_dom_class_loss = DomainDiscriminationLoss(1000, source=False)\n",
    "domain_class_loss = source_dom_class_loss + target_dom_class_loss\n",
    "\n",
    "source_task_class_loss = SplitCrossEntropyLoss(1000, source=True)\n",
    "target_task_class_loss = SplitCrossEntropyLoss(1000, source=False)\n",
    "\n",
    "source_dom_conf_loss = DomainDiscriminationLoss(1000, source=True)\n",
    "target_dom_conf_loss = DomainDiscriminationLoss(1000, source=False)\n",
    "domain_conf_loss = 0.5 * (source_dom_conf_loss + target_dom_conf_loss)\n",
    "\n",
    "source_cat_conf_loss = SplitCrossEntropyLoss(1000, source=True)\n",
    "target_cat_conf_loss = SplitCrossEntropyLoss(1000, source=False)\n",
    "category_conf_loss = 0.5 * (source_cat_conf_loss + target_cat_conf_loss)\n",
    "\n",
    "target_entropy_loss = _EntropyLoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Extractor (<i>G</i>) - Resnet18"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeatureExtractor:\n",
    "    \n",
    "    def __init__(self, n_classes: int, n_layers_trained: int, model='resnet18', optimizer='rmsprop', lr=0.01, weight_decay=0):\n",
    "        \n",
    "        # Upload pretrained model \n",
    "        if model.lower() == 'resnet18': \n",
    "            self.model = models.resnet18(pretrained=True)\n",
    "        elif model.lower() == 'resnet50': \n",
    "            self.model = models.resnet50(pretrained=True)\n",
    "        else:\n",
    "            raise ValueError('Unknown model')\n",
    "        \n",
    "        # Modify last fully-connected layer\n",
    "        self.model.fc = torch.nn.Linear(\n",
    "            in_features = self.model.fc.in_features, \n",
    "            out_features = n_classes * 2\n",
    "        )\n",
    "        \n",
    "        # Freeze pretrained layers\n",
    "        params = list(self.model.parameters())\n",
    "        for i in range(len(params)):\n",
    "            n_layers_frozen = len(params) - i - 1\n",
    "            params[i].requires_grad = (n_layers_frozen < n_layers_trained)\n",
    "        params_to_train = filter(lambda p: p.requires_grad, self.model.parameters())\n",
    "        \n",
    "        # Initialize optimizer\n",
    "        if optimizer.lower() == 'rmsprop':\n",
    "            self.optim = torch.optim.RMSprop(\n",
    "                params = params_to_train,\n",
    "                lr = lr,\n",
    "                weight_decay = weight_decay\n",
    "            )\n",
    "        elif optimizer.lower() == 'adadelta':\n",
    "            self.optim = torch.optim.Adadelta(\n",
    "                params = params_to_train,\n",
    "                lr = lr,\n",
    "                weight_decay = weight_decay\n",
    "            )\n",
    "        elif optimizer.lower() == 'sgd':\n",
    "            self.optim = torch.optim.SGD(\n",
    "                params = params_to_train,\n",
    "                lr = lr,\n",
    "                weight_decay = weight_decay,\n",
    "                nesterov = True\n",
    "            )\n",
    "        else:\n",
    "            raise ValueError('Unknown optimizer')"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "16cbb1df62e8e69441bdf7dce00d4866841818461557dbda546dabf5a55996e0"
  },
  "kernelspec": {
   "display_name": "Python 3.10.5 64-bit (windows store)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
