{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports and Preliminary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/luciahrovatin/Desktop/deep-learning-proj/venv/lib/python3.9/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from torch import Tensor\n",
    "from torchvision import models\n",
    "from torch.optim import RMSprop, Adagrad\n",
    "from overrides import overrides, final\n",
    "from abc import abstractmethod"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The SymNet architecture presents an overall training function based on compositionality. Hence, different modules are embedded, as reported below following the original implementation of [Zhang](https://arxiv.org/abs/1904.04663) <i>et al.</i>(see Section 3.3). \n",
    "\n",
    "$$ \\displaystyle\\min_{C^s, C^t, C^{st}} \\large \\mathcal{E}_{task}^{(s)}(G, C^{(s)}) +  \\large\\mathcal{E}_{task}^{(t)}(G, C^{(t)}) + \\large\\mathcal{E}_{task}^{(st)}(G, C^{(st)})$$\n",
    "$$\\displaystyle\\min_{G} \\large \\mathcal{F}_{category}^{(st)}(G, C^{(st)}) + \\lambda [\\large \\mathcal{F}_{domain}^{(st)}(G, C^{(st)}) + \\large \\mathcal{M}^{(st)}(G, C^{(st)})]$$\n",
    "\n",
    "In order to reproduce the original implementation and its multiple losses, a tree structure has been chosen. The root is represented by the class <code>_Loss</code>, which inherits from the <code>[torch.nn.Module](https://pytorch.org/docs/stable/generated/torch.nn.Module.html)</code> and has two submodules as children:\n",
    "<code>_CrossEntropyLoss</code> and <code>_EntropyLoss</code>. The latter refers to the <i>Entropy Minimization Principle</i> (Section 3.2.1 [Zhang](https://arxiv.org/abs/1904.04663) <i>et al.</i>), whereas the former is further subdefined in modules:\n",
    "\n",
    "```\n",
    "_Loss\n",
    "│\n",
    "├── _EntropyLoss\n",
    "│ \n",
    "└── _CrossEntropyLoss\n",
    "    │   \n",
    "    └── SplitCrossEntropyLoss\n",
    "        |\n",
    "        └── DomainDiscriminationLoss\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class _Loss(torch.nn.Module):\n",
    "    \n",
    "    _THRESHOLD = 1e-20\n",
    "    \n",
    "    def __init__(self):\n",
    "        super(_Loss, self).__init__()\n",
    "        \n",
    "    def forward(self, input: Tensor):\n",
    "        prob = self.to_softmax(input)\n",
    "        return self.loss(prob)\n",
    "        \n",
    "    @final\n",
    "    def add_threshold(self, prob: Tensor):\n",
    "        '''\n",
    "        Check whether the probability distribution after the softmax \n",
    "        is equal to 0 in any cell. If this holds, a standard threshold\n",
    "        is added in order to avoid log(0) case. \n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        prob: Tensor\n",
    "            output tensor of the softmax operation\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        Tensor\n",
    "            updated tensor (in case the condition above holds)\n",
    "        '''\n",
    "        zeros = (prob == 0)\n",
    "        if torch.any(zeros):\n",
    "            thre_tensor = torch.zeros(zeros.shape)\n",
    "            thre_tensor[zeros] = self._THRESHOLD\n",
    "            prob += thre_tensor\n",
    "        return prob\n",
    "    \n",
    "    def to_softmax(self, features: Tensor):\n",
    "        '''\n",
    "        Apply the softmax operation on the features tensor, \n",
    "        being the output of a classifier. It returns the distribution \n",
    "        of probability withing the range [0,1]. \n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        features: Tensor\n",
    "            input tensor of the softmax operation\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        Tensor\n",
    "            probability distribution with (possible) threshold\n",
    "        '''\n",
    "        prob = F.softmax(features, dim=1)\n",
    "        return self.add_threshold(prob)\n",
    "    \n",
    "    @abstractmethod\n",
    "    def loss(self, prob: Tensor):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Entropy Minimization Principle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Entropy Minimization objective is here ([Zhang](https://arxiv.org/abs/1904.04663) <i>et al.</i>, Section 3.2.1) adopted to update the feature extractor (<i>G</i>) and to enhance the discrimination among task categories. This avoids having target samples stucked into wrong category predictions during the early training stages. \n",
    "#TODO: check this again!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class _EntropyLoss(_Loss):\n",
    "    \n",
    "    def __init__(self):\n",
    "        super(_EntropyLoss, self).__init__()\n",
    "    \n",
    "    @overrides\n",
    "    def loss(self, prob: Tensor):\n",
    "        return -(prob.log().mul(prob).sum(dim=1).mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cross Entropy loss(es)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Two classifiers have been implemented to solve the classification task on the <i>source</i> ($ C^{(s)} $) and <i>target</i> ($ C^{(t)} $) domain. The former <code>task classifier</code>, $ C^{(s)} $, is trained using the following cross-entropy loss over the <i>labeled</i> source samples: <br />\n",
    "$$ min_{C^{(s)}}\\mathcal{E}_{task}^{(s)}(G, C^{(s)})=-\\frac{1}{n_{s}}\\sum_{i=1}^{n_{s}}log\\bigg(p_{y_{i}^{(s)}}^{(s)}({\\bf x}_{i}^{(s)})\\bigg) $$\n",
    "<br />In the formula above, $G$ represents the <code>feature extractor</code>, ${\\bf x}_{i}^{(s)}$ the output vector of $ C^{(s)} $, and $p_{y_{i}^{(s)}}^{(s)}({\\bf x}_{i}^{(s)}) \\in [0,1]^{K}$ the distribution of probability after the <code>  [softmax](https://pytorch.org/docs/stable/generated/torch.nn.functional.softmax.html)</code>  operation. \n",
    "\n",
    "Since target samples are <i>unlabeled</i>, there exists no direct\n",
    "supervision signals to learn a task classifier $ C^{(t)} $. Therefore, [Zhang](https://arxiv.org/abs/1904.04663) <i>et al.</i> leverage the <i>labeled</i> source samples by using the following cross-entropy loss: \n",
    "$$ min_{C^{(t)}}\\mathcal{E}_{task}^{(t)}(G, C^{(t)})=-\\frac{1}{n_{s}}\\sum_{i=1}^{n_{s}}log\\bigg(p_{y_{i}^{(s)}}^{(t)}({\\bf x}_{i}^{(s)})\\bigg) $$\n",
    "<br />It is worth noticing that $C^{(t)}$ will be distinguishable from $C^{(s)}$ through the domain discrimination training of the classifier $C^{(st)}$. Moreover, [Zhang](https://arxiv.org/abs/1904.04663) <i>et al.</i> stress the use of <i>labeled</i> source samples to enhance $C^{(t)}$ performance in discriminating among task categories. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class _CrossEntropyLoss(_Loss):\n",
    "    \n",
    "    def __init__(self):\n",
    "        super(_CrossEntropyLoss, self).__init__()\n",
    "    \n",
    "    @overrides\n",
    "    def loss(self, prob: Tensor):\n",
    "        return -(prob.log().mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SplitCrossEntropyLoss(_CrossEntropyLoss):\n",
    "    \n",
    "    def __init__(self, n_classes: int, source: bool):\n",
    "        super(SplitCrossEntropyLoss, self).__init__()\n",
    "        self.n_classes = n_classes\n",
    "        self._is_source = source\n",
    "    \n",
    "    @overrides\n",
    "    def to_softmax(self, features: Tensor):\n",
    "        prob = F.softmax(features, dim=1)\n",
    "        prob = self.split_softmax(prob)\n",
    "        return self.add_threshold(prob)\n",
    "    \n",
    "    @final\n",
    "    def split_softmax(self, prob: Tensor):\n",
    "        return prob[:,:self.n_classes] if self._is_source else prob[:,self.n_classes:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DomainDiscriminationLoss(SplitCrossEntropyLoss):\n",
    "    \n",
    "    def __init__(self, n_classes: int, source: bool):\n",
    "        super(DomainDiscriminationLoss, self).__init__(n_classes, source)\n",
    "        \n",
    "    @overrides\n",
    "    def loss(self, prob: Tensor):\n",
    "        return -(prob.sum(dim=1).log().mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "source_dom_class_loss = DomainDiscriminationLoss(n_classes=1000, source=True)\n",
    "target_dom_class_loss = DomainDiscriminationLoss(n_classes=1000, source=False)\n",
    "domain_class_loss = source_dom_class_loss + target_dom_class_loss\n",
    "\n",
    "source_task_class_loss = SplitCrossEntropyLoss(n_classes=1000, source=True)\n",
    "target_task_class_loss = SplitCrossEntropyLoss(n_classes=1000, source=False)\n",
    "\n",
    "source_dom_conf_loss = DomainDiscriminationLoss(n_classes=1000, source=True)\n",
    "target_dom_conf_loss = DomainDiscriminationLoss(n_classes=1000, source=False)\n",
    "domain_conf_loss = 0.5 * (source_dom_conf_loss + target_dom_conf_loss)\n",
    "\n",
    "source_cat_conf_loss = SplitCrossEntropyLoss(n_classes=1000, source=True)\n",
    "target_cat_conf_loss = SplitCrossEntropyLoss(n_classes=1000, source=False)\n",
    "category_conf_loss = 0.5 * (source_cat_conf_loss + target_cat_conf_loss)\n",
    "\n",
    "target_entropy_loss = _EntropyLoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Extractor (<i>G</i>) - Resnet18"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeatureExtractor:\n",
    "    \n",
    "    def __init__(self, n_classes: int, n_layers_trained: int, model='resnet18', optimizer='rmsprop', lr=0.01, weight_decay=0):\n",
    "        \n",
    "        # Upload pretrained model \n",
    "        if model.lower() == 'resnet18': \n",
    "            self.model = models.resnet18(pretrained=True)\n",
    "        elif model.lower() == 'resnet50': \n",
    "            self.model = models.resnet50(pretrained=True)\n",
    "        else:\n",
    "            raise ValueError('Unknown model')\n",
    "        \n",
    "        # Modify last fully-connected layer\n",
    "        self.model.fc = torch.nn.Linear(\n",
    "            in_features = self.model.fc.in_features, \n",
    "            out_features = n_classes * 2\n",
    "        )\n",
    "        \n",
    "        # Freeze pretrained layers\n",
    "        params = list(self.model.parameters())\n",
    "        for i in range(len(params)):\n",
    "            n_layers_frozen = len(params) - i - 1\n",
    "            params[i].requires_grad = (n_layers_frozen < n_layers_trained)\n",
    "        params_to_train = filter(lambda p: p.requires_grad, self.model.parameters())\n",
    "        \n",
    "        # Initialize optimizer\n",
    "        if optimizer.lower() == 'rmsprop':\n",
    "            self.optim = torch.optim.RMSprop(\n",
    "                params = params_to_train,\n",
    "                lr = lr,\n",
    "                weight_decay = weight_decay\n",
    "            )\n",
    "        elif optimizer.lower() == 'adadelta':\n",
    "            self.optim = torch.optim.Adadelta(\n",
    "                params = params_to_train,\n",
    "                lr = lr,\n",
    "                weight_decay = weight_decay\n",
    "            )\n",
    "        elif optimizer.lower() == 'sgd':\n",
    "            self.optim = torch.optim.SGD(\n",
    "                params = params_to_train,\n",
    "                lr = lr,\n",
    "                weight_decay = weight_decay,\n",
    "                nesterov = True\n",
    "            )\n",
    "        else:\n",
    "            raise ValueError('Unknown optimizer')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 ('venv': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "6f0d290f0742685d306541f8dcebbe79a177e37269f78587a0fc5052fa8d446c"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
