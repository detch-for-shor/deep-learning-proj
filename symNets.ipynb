{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import  libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/luciahrovatin/Desktop/deep-learning-proj/venv/lib/python3.9/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os \n",
    "import imagesize\n",
    "import zipfile \n",
    "import statistics \n",
    "import math\n",
    "import torch\n",
    "import torchvision\n",
    "\n",
    "import numpy as np\n",
    "import torchvision.transforms as T\n",
    "import torch.nn.functional as F\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "from tqdm import tqdm\n",
    "from pathlib import Path\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch import Tensor\n",
    "from torchvision import models\n",
    "from torch.autograd import Variable\n",
    "from torch.nn import CrossEntropyLoss\n",
    "from torch.optim import RMSprop, Adagrad\n",
    "from overrides import overrides, final\n",
    "from abc import abstractmethod\n",
    "#from google.colab import drive\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "classes = [\"backpack\", \"bookcase\", \"car jack\", \"comb\", \"crown\", \"file cabinet\", \"flat iron\", \"game controller\", \"glasses\",\n",
    "           \"helicopter\", \"ice skates\", \"letter tray\", \"monitor\", \"mug\", \"network switch\", \"over-ear headphones\", \"pen\",\n",
    "           \"purse\", \"stand mixer\", \"stroller\"]\n",
    "\n",
    "domains = [\"product_images\", \"real_life\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_transformation(resize_dim = 256, crop_dim = 224, grayscale = True, crop_center = True):\n",
    "    \n",
    "    transform_lst = []\n",
    "    transform_lst.append(T.Resize((resize_dim)))                                                          \n",
    "    \n",
    "    if grayscale:\n",
    "        transform_lst.append(T.Grayscale(num_output_channels=3))                        \n",
    "    \n",
    "    if crop_center:\n",
    "        transform_lst.append(T.CenterCrop((crop_dim)))\n",
    "    else:\n",
    "        transform_lst.append(T.RandomCrop((crop_dim)))\n",
    "    \n",
    "    transform_lst.append(T.RandomHorizontalFlip(p=0.5))                                  \n",
    "    transform_lst.append(T.ToTensor())                                             \n",
    "        \n",
    "    return T.Compose(transform_lst)  \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalization(dataset):\n",
    "    ds_length = len(dataset)\n",
    "    for i in tqdm(range(ds_length)):\n",
    "        r_mean, g_mean, b_mean = torch.mean(dataset[i][0], dim = [1,2])\n",
    "        r_std, g_std, b_std = torch.std(dataset[i][0], dim = [1,2])\n",
    "        T.functional.normalize(\n",
    "            tensor = dataset[i][0], \n",
    "            mean = [r_mean, g_mean, b_mean],\n",
    "            std = [r_std, g_std, b_std],\n",
    "            inplace=True\n",
    "            )\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2000/2000 [01:38<00:00, 20.37it/s]\n",
      "100%|██████████| 2000/2000 [02:51<00:00, 11.64it/s]\n"
     ]
    }
   ],
   "source": [
    "source = \"product_images\"\n",
    "target = \"real_life\"\n",
    "resize_dim = 256\n",
    "crop_dim = 224\n",
    "grayscale = False\n",
    "crop_center = True \n",
    "\n",
    "\n",
    "source_ds = torchvision.datasets.ImageFolder(\n",
    "    root = f\"data/Adaptiope/{source}\",\n",
    "    transform = data_transformation(resize_dim, crop_dim, grayscale, crop_center)\n",
    "    )\n",
    "\n",
    "target_ds = torchvision.datasets.ImageFolder(\n",
    "    root = f\"data/Adaptiope/{target}\",\n",
    "    transform = data_transformation(resize_dim, crop_dim, grayscale, crop_center)\n",
    "    ) \n",
    "\n",
    "if not grayscale:\n",
    "    normalization(source_ds)\n",
    "    normalization(target_ds)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data(dataset, test_split=0.2, batch_size=32):\n",
    "    \n",
    "    train_indices, val_indices = train_test_split(\n",
    "        list(range(len(dataset.targets))),\n",
    "        test_size = test_split,\n",
    "        stratify = dataset.targets, \n",
    "        random_state = 42\n",
    "        )\n",
    "    \n",
    "    train_dataset = torch.utils.data.Subset(dataset, train_indices)\n",
    "    val_dataset = torch.utils.data.Subset(dataset, val_indices)\n",
    "\n",
    "    train_data_loader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "    val_data_loader = torch.utils.data.DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "    \n",
    "    return train_data_loader, val_data_loader\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "test_split = 0.2\n",
    "\n",
    "source_train_loader, source_val_loader = get_data(source_ds, test_split, batch_size)\n",
    "target_train_loader, target_val_loader = get_data(target_ds, test_split, batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class _Loss(torch.nn.Module):\n",
    "    \n",
    "    _THRESHOLD = 1e-20\n",
    "    \n",
    "    def __init__(self):\n",
    "        super(_Loss, self).__init__()\n",
    "        \n",
    "    def forward(self, input: Tensor):\n",
    "        prob = self.to_softmax(input)\n",
    "        return self.loss(prob)\n",
    "        \n",
    "    @final\n",
    "    def add_threshold(self, prob: Tensor):\n",
    "        '''\n",
    "        Check whether the probability distribution after the softmax \n",
    "        is equal to 0 in any cell. If this holds, a standard threshold\n",
    "        is added in order to avoid log(0) case. \n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        prob: Tensor\n",
    "            output tensor of the softmax operation\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        Tensor\n",
    "            updated tensor (in case the condition above holds)\n",
    "        '''\n",
    "        zeros = (prob == 0)\n",
    "        if torch.any(zeros):\n",
    "            thre_tensor = torch.zeros(zeros.shape)\n",
    "            thre_tensor[zeros] = self._THRESHOLD\n",
    "            prob += thre_tensor\n",
    "        return prob\n",
    "    \n",
    "    def to_softmax(self, features: Tensor):\n",
    "        '''\n",
    "        Apply the softmax operation on the features tensor, \n",
    "        being the output of a feature extractor. \n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        features: Tensor\n",
    "            input tensor of the softmax operation\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        Tensor\n",
    "            probability distribution with (possible) threshold\n",
    "        '''\n",
    "        prob = F.softmax(features, dim=1)\n",
    "        return self.add_threshold(prob)\n",
    "    \n",
    "    @abstractmethod\n",
    "    def loss(self, prob: Tensor):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EntropyMinimizationLoss(_Loss):\n",
    "    \n",
    "    def __init__(self, n_classes: int):\n",
    "        super(EntropyMinimizationLoss, self).__init__()\n",
    "        self.n_classes = n_classes\n",
    "    \n",
    "    @overrides\n",
    "    def loss(self, prob: Tensor):\n",
    "        prob_source = prob[:, :self.n_classes]\n",
    "        prob_target = prob[:, self.n_classes:]\n",
    "        prob_sum = prob_source + prob_target\n",
    "        return -(prob_sum.log().mul(prob_sum).sum(dim=1).mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SplitLoss(_Loss):\n",
    "    \n",
    "    def __init__(self, n_classes: int, source: bool, split_first: bool):\n",
    "        super(SplitLoss, self).__init__()\n",
    "        self.n_classes = n_classes\n",
    "        self._is_source = source\n",
    "        self._split_first = split_first\n",
    "    \n",
    "    @overrides\n",
    "    def to_softmax(self, features: Tensor):\n",
    "        if self._split_first:\n",
    "            prob = self.split_vector(features)\n",
    "            prob = F.softmax(prob, dim=1)\n",
    "        else:\n",
    "            prob = F.softmax(features, dim=1)\n",
    "            prob = self.split_vector(prob)\n",
    "        return self.add_threshold(prob)\n",
    "    \n",
    "    @final\n",
    "    def split_vector(self, prob: Tensor):\n",
    "        return prob[:,:self.n_classes] if self._is_source else prob[:,self.n_classes:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SplitCrossEntropyLoss(SplitLoss):\n",
    "    \n",
    "    def _get_y_labels(self):\n",
    "        return self._y_labels\n",
    "    def _set_y_labels(self, y_labels: Variable):\n",
    "        if not all(y < self.n_classes for y in y_labels):\n",
    "            raise ValueError('Expected all y labels < n_classes')\n",
    "        self._y_labels = y_labels\n",
    "    y_labels = property(fget=_get_y_labels, fset=_set_y_labels)\n",
    "    \n",
    "    def __init__(self, n_classes: int, source: bool, split_first: bool):\n",
    "        super(SplitCrossEntropyLoss, self).__init__(n_classes, source, split_first)\n",
    "        self.cross_entropy_loss = torch.nn.CrossEntropyLoss()\n",
    "    \n",
    "    @overrides\n",
    "    def loss(self, prob: Tensor):\n",
    "        '''Computes cross-entropy loss w.r.t. ground-truth (y label)'''\n",
    "        return self.cross_entropy_loss(prob, self.y_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DomainDiscriminationLoss(SplitLoss):\n",
    "    \n",
    "    def __init__(self, n_classes: int, source: bool):\n",
    "        super(DomainDiscriminationLoss, self).__init__(n_classes, source, False)\n",
    "        \n",
    "    @overrides\n",
    "    def loss(self, prob: Tensor):\n",
    "        return -(prob.sum(dim=1).log().mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TrainingObjectives:\n",
    "    \n",
    "    @staticmethod\n",
    "    def domain_discrimination_loss(src_dom_discrim_loss, tgt_dom_discrim_loss):\n",
    "        return src_dom_discrim_loss + tgt_dom_discrim_loss\n",
    "    \n",
    "    @staticmethod\n",
    "    def category_confusion_loss(src_cat_conf_loss, tgt_cat_conf_loss):\n",
    "        return 0.5 * (src_cat_conf_loss + tgt_cat_conf_loss)\n",
    "    \n",
    "    @staticmethod\n",
    "    def domain_confusion_loss(src_dom_conf_loss, tgt_dom_conf_loss):\n",
    "        return 0.5 * (src_dom_conf_loss + tgt_dom_conf_loss)\n",
    "    \n",
    "    @staticmethod\n",
    "    def overall_classifier_loss(src_task_class_loss, tgt_task_class_loss, domain_discrim_loss):\n",
    "        return src_task_class_loss + tgt_task_class_loss + domain_discrim_loss\n",
    "    \n",
    "    @staticmethod\n",
    "    def overall_generator_loss(cat_conf_loss, dom_conf_loss, tgt_entropy_loss, curr_epoch, tot_epochs):\n",
    "        lambda_trade_off = 2 / (1 + math.exp(-1 * 10 * curr_epoch / tot_epochs)) - 1\n",
    "        return cat_conf_loss + lambda_trade_off * (dom_conf_loss + tgt_entropy_loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### feature extractor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeatureExtractor:\n",
    "    \n",
    "    def __init__(self, n_classes: int, n_layers_trained: int, model='resnet18', optimizer='rmsprop', lr=0.01, weight_decay=0):\n",
    "        # TODO: scrivi la descrizione!!!!\n",
    "        # Upload pretrained model \n",
    "        if model.lower() == 'resnet18': \n",
    "            self.model = models.resnet18(pretrained=True)\n",
    "        elif model.lower() == 'resnet50': \n",
    "            self.model = models.resnet50(pretrained=True)\n",
    "        else:\n",
    "            raise ValueError('Unknown model')\n",
    "        \n",
    "        # Modify last fully-connected layer\n",
    "        self.model.fc = torch.nn.Linear(\n",
    "            in_features = self.model.fc.in_features, \n",
    "            out_features = n_classes * 2\n",
    "        )\n",
    "        \n",
    "        # Freeze pretrained layers\n",
    "        \n",
    "        count = 0 \n",
    "        n_params = len(list(self.model.parameters()))\n",
    "        for param in self.model.parameters():\n",
    "            n_layers_frozen = n_params - count - 1\n",
    "            param.requires_grad = (n_layers_frozen < n_layers_trained)\n",
    "            count += 1 \n",
    "        \n",
    "        #params = list(self.model.parameters())\n",
    "        #for i in range(len(params)):\n",
    "        #    n_layers_frozen = len(params) - i - 1\n",
    "        #    params[i].requires_grad = (n_layers_frozen < n_layers_trained)\n",
    "        \n",
    "        params_to_train = filter(lambda p: p.requires_grad, self.model.parameters())\n",
    "        \n",
    "        # Initialize optimizer\n",
    "        if optimizer.lower() == 'rmsprop':\n",
    "            self.optim = torch.optim.RMSprop(\n",
    "                params = params_to_train,\n",
    "                lr = lr,\n",
    "                weight_decay = weight_decay\n",
    "            )\n",
    "        elif optimizer.lower() == 'adadelta':\n",
    "            self.optim = torch.optim.Adadelta(\n",
    "                params = params_to_train,\n",
    "                lr = lr,\n",
    "                weight_decay = weight_decay\n",
    "            )\n",
    "        elif optimizer.lower() == 'sgd':\n",
    "            self.optim = torch.optim.SGD(\n",
    "                params = params_to_train,\n",
    "                lr = lr,\n",
    "                weight_decay = weight_decay,\n",
    "                nesterov = True\n",
    "            )\n",
    "        else:\n",
    "            raise ValueError('Unknown optimizer')\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ModelTrainer:\n",
    "    \n",
    "    def __init__(self, model: FeatureExtractor, n_classes: int, epochs: int):\n",
    "        self.model = model \n",
    "        self.curr_epoch = 0\n",
    "        self.tot_epochs = epochs\n",
    "        self.n_classes = n_classes\n",
    "        # Task classifier losses\n",
    "        self.src_task_class_loss = SplitCrossEntropyLoss(n_classes=n_classes, source=True, split_first=True).cuda()\n",
    "        self.tgt_task_class_loss = SplitCrossEntropyLoss(n_classes=n_classes, source=False, split_first=True).cuda()\n",
    "        # Domain discrimination losses\n",
    "        self.src_dom_discrim_loss = DomainDiscriminationLoss(n_classes=n_classes, source=True).cuda()\n",
    "        self.tgt_dom_discrim_loss = DomainDiscriminationLoss(n_classes=n_classes, source=False).cuda()\n",
    "        # Category-level confusion losses\n",
    "        self.src_cat_conf_loss = SplitCrossEntropyLoss(n_classes=n_classes, source=True, split_first=False).cuda()\n",
    "        self.tgt_cat_conf_loss = SplitCrossEntropyLoss(n_classes=n_classes, source=False, split_first=False).cuda()\n",
    "        # Domain-level confusion losses\n",
    "        self.src_dom_conf_loss = DomainDiscriminationLoss(n_classes=n_classes, source=True).cuda()\n",
    "        self.tgt_dom_conf_loss = DomainDiscriminationLoss(n_classes=n_classes, source=False).cuda()\n",
    "        # Entropy minimization loss\n",
    "        self.tgt_entropy_loss = EntropyMinimizationLoss(n_classes=n_classes).cuda()\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "    def train_one_epoch(self, source_dataloader, target_dataloader):\n",
    "        self.curr_epoch += 1\n",
    "        end_of_epoch = False\n",
    "        source_batch_loader = enumerate(source_dataloader)\n",
    "        target_batch_loader = enumerate(target_dataloader)\n",
    "        \n",
    "        torch.autograd.set_detect_anomaly(True)\n",
    "        \n",
    "        # Train for current epoch\n",
    "        while not end_of_epoch:\n",
    "            try:\n",
    "                # Get next batch for both source and target\n",
    "                (X_source, y_source) = source_batch_loader.__next__()[1]\n",
    "                (X_target, _) = target_batch_loader.__next__()[1]\n",
    "            except StopIteration:\n",
    "                end_of_epoch = True\n",
    "                continue\n",
    "            \n",
    "            # Tell model go training mode\n",
    "            self.model.model.train()\n",
    "            \n",
    "            # Convert to torch.autograd variables\n",
    "            X_source_var = Variable(X_source) \n",
    "            y_source_var = Variable(y_source)\n",
    "            X_target_var = Variable(X_target)\n",
    "            \n",
    "            # Compute features for both inputs\n",
    "            X_source_features = self.model.model(X_source_var)\n",
    "            X_target_features = self.model.model(X_target_var)\n",
    "            \n",
    "            # Compute overall training objective losses\n",
    "            classifier_loss, generator_loss = self.overall_losses(\n",
    "                X_source_features, \n",
    "                X_target_features, \n",
    "                y_source_var\n",
    "            )\n",
    "            \n",
    "            self.model.optim.zero_grad()\n",
    "            classifier_loss.backward(retain_graph=True) \n",
    "            grad_classifier_tmp = []\n",
    "            for p in self.model.model.parameters():\n",
    "                grad_classifier_tmp.append(p.grad.data.clone() if p.requires_grad else p.data.clone().zero_())\n",
    "                \n",
    "            self.model.optim.zero_grad()\n",
    "            generator_loss.backward()\n",
    "            grad_generator_tmp = []\n",
    "            for p in self.model.model.parameters():\n",
    "                grad_generator_tmp.append(p.grad.data.clone() if p.requires_grad else p.data.clone().zero_())\n",
    "            \n",
    "            count = 0 \n",
    "            for p in self.model.model.parameters():\n",
    "                grad_tmp = p.data.clone().zero_() \n",
    "                if count < 159: # FIXME: capire perché 159\n",
    "                    grad_tmp += grad_generator_tmp[count]\n",
    "                else: \n",
    "                    grad_tmp += grad_classifier_tmp[count]\n",
    "                p.grad = grad_tmp \n",
    "                count += 1 \n",
    "            self.model.optim.step()\n",
    "            \n",
    "            \n",
    "\n",
    "    def overall_losses(self, X_source_features, X_target_features, y_source_var) -> tuple[Tensor, Tensor]:\n",
    "        # Source task classifier loss\n",
    "        self.src_task_class_loss.y_labels = y_source_var\n",
    "        _src_task_class_loss = self.src_task_class_loss(X_source_features)\n",
    "        \n",
    "        # (Cross-domain) Target task classifier loss\n",
    "        self.tgt_task_class_loss.y_labels = y_source_var\n",
    "        _tgt_task_class_loss = self.tgt_task_class_loss(X_source_features)\n",
    "        \n",
    "        # Domain discrimination loss\n",
    "        _src_dom_discrim_loss = self.src_dom_discrim_loss(X_source_features)\n",
    "        _tgt_dom_discrim_loss = self.tgt_dom_discrim_loss(X_target_features)\n",
    "        _domain_discrim_loss = TrainingObjectives.domain_discrimination_loss(\n",
    "            _src_dom_discrim_loss, \n",
    "            _tgt_dom_discrim_loss\n",
    "        )\n",
    "        \n",
    "        # Category-level confusion loss\n",
    "        self.src_cat_conf_loss.y_labels = y_source_var\n",
    "        self.tgt_cat_conf_loss.y_labels = y_source_var\n",
    "        _src_cat_conf_loss = self.src_cat_conf_loss(X_source_features)\n",
    "        _tgt_cat_conf_loss = self.tgt_cat_conf_loss(X_source_features)\n",
    "        _category_conf_loss = TrainingObjectives.category_confusion_loss(\n",
    "            _src_cat_conf_loss, \n",
    "            _tgt_cat_conf_loss\n",
    "        )\n",
    "        \n",
    "        # Domain-level confusion loss\n",
    "        _src_dom_conf_loss = self.src_cat_conf_loss(X_target_features)\n",
    "        _tgt_dom_conf_loss = self.tgt_cat_conf_loss(X_target_features)\n",
    "        _domain_conf_loss = TrainingObjectives.domain_confusion_loss(\n",
    "            _src_dom_conf_loss, \n",
    "            _tgt_dom_conf_loss\n",
    "        )\n",
    "\n",
    "        # Entropy minimization loss\n",
    "        _tgt_entropy_loss = self.tgt_entropy_loss(X_target_features)\n",
    "        \n",
    "        # Overall classifier loss\n",
    "        _overall_classifier_loss = TrainingObjectives.overall_classifier_loss(\n",
    "            _src_task_class_loss, \n",
    "            _tgt_task_class_loss, \n",
    "            _domain_discrim_loss\n",
    "        )\n",
    "\n",
    "        # Overall feature extractor loss\n",
    "        _overall_generator_loss = TrainingObjectives.overall_generator_loss(\n",
    "            _category_conf_loss, \n",
    "            _domain_conf_loss, \n",
    "            _tgt_entropy_loss, \n",
    "            self.curr_epoch, \n",
    "            self.tot_epochs\n",
    "        )\n",
    "        \n",
    "        return _overall_classifier_loss, _overall_generator_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "source_train_loader, source_val_loader\n",
    "\n",
    "\n",
    "target_train_loader, target_val_loader "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/luciahrovatin/Desktop/deep-learning-proj/venv/lib/python3.9/site-packages/torch/autograd/__init__.py:173: UserWarning: Error detected in SoftmaxBackward0. Traceback of forward call that caused the error:\n",
      "  File \"/opt/homebrew/Cellar/python@3.9/3.9.13_1/Frameworks/Python.framework/Versions/3.9/lib/python3.9/runpy.py\", line 197, in _run_module_as_main\n",
      "    return _run_code(code, main_globals, None,\n",
      "  File \"/opt/homebrew/Cellar/python@3.9/3.9.13_1/Frameworks/Python.framework/Versions/3.9/lib/python3.9/runpy.py\", line 87, in _run_code\n",
      "    exec(code, run_globals)\n",
      "  File \"/Users/luciahrovatin/Desktop/deep-learning-proj/venv/lib/python3.9/site-packages/ipykernel_launcher.py\", line 17, in <module>\n",
      "    app.launch_new_instance()\n",
      "  File \"/Users/luciahrovatin/Desktop/deep-learning-proj/venv/lib/python3.9/site-packages/traitlets/config/application.py\", line 976, in launch_instance\n",
      "    app.start()\n",
      "  File \"/Users/luciahrovatin/Desktop/deep-learning-proj/venv/lib/python3.9/site-packages/ipykernel/kernelapp.py\", line 712, in start\n",
      "    self.io_loop.start()\n",
      "  File \"/Users/luciahrovatin/Desktop/deep-learning-proj/venv/lib/python3.9/site-packages/tornado/platform/asyncio.py\", line 199, in start\n",
      "    self.asyncio_loop.run_forever()\n",
      "  File \"/opt/homebrew/Cellar/python@3.9/3.9.13_1/Frameworks/Python.framework/Versions/3.9/lib/python3.9/asyncio/base_events.py\", line 601, in run_forever\n",
      "    self._run_once()\n",
      "  File \"/opt/homebrew/Cellar/python@3.9/3.9.13_1/Frameworks/Python.framework/Versions/3.9/lib/python3.9/asyncio/base_events.py\", line 1905, in _run_once\n",
      "    handle._run()\n",
      "  File \"/opt/homebrew/Cellar/python@3.9/3.9.13_1/Frameworks/Python.framework/Versions/3.9/lib/python3.9/asyncio/events.py\", line 80, in _run\n",
      "    self._context.run(self._callback, *self._args)\n",
      "  File \"/Users/luciahrovatin/Desktop/deep-learning-proj/venv/lib/python3.9/site-packages/ipykernel/kernelbase.py\", line 508, in dispatch_queue\n",
      "    await self.process_one()\n",
      "  File \"/Users/luciahrovatin/Desktop/deep-learning-proj/venv/lib/python3.9/site-packages/ipykernel/kernelbase.py\", line 497, in process_one\n",
      "    await dispatch(*args)\n",
      "  File \"/Users/luciahrovatin/Desktop/deep-learning-proj/venv/lib/python3.9/site-packages/ipykernel/kernelbase.py\", line 404, in dispatch_shell\n",
      "    await result\n",
      "  File \"/Users/luciahrovatin/Desktop/deep-learning-proj/venv/lib/python3.9/site-packages/ipykernel/kernelbase.py\", line 728, in execute_request\n",
      "    reply_content = await reply_content\n",
      "  File \"/Users/luciahrovatin/Desktop/deep-learning-proj/venv/lib/python3.9/site-packages/ipykernel/ipkernel.py\", line 383, in do_execute\n",
      "    res = shell.run_cell(\n",
      "  File \"/Users/luciahrovatin/Desktop/deep-learning-proj/venv/lib/python3.9/site-packages/ipykernel/zmqshell.py\", line 528, in run_cell\n",
      "    return super().run_cell(*args, **kwargs)\n",
      "  File \"/Users/luciahrovatin/Desktop/deep-learning-proj/venv/lib/python3.9/site-packages/IPython/core/interactiveshell.py\", line 2881, in run_cell\n",
      "    result = self._run_cell(\n",
      "  File \"/Users/luciahrovatin/Desktop/deep-learning-proj/venv/lib/python3.9/site-packages/IPython/core/interactiveshell.py\", line 2936, in _run_cell\n",
      "    return runner(coro)\n",
      "  File \"/Users/luciahrovatin/Desktop/deep-learning-proj/venv/lib/python3.9/site-packages/IPython/core/async_helpers.py\", line 129, in _pseudo_sync_runner\n",
      "    coro.send(None)\n",
      "  File \"/Users/luciahrovatin/Desktop/deep-learning-proj/venv/lib/python3.9/site-packages/IPython/core/interactiveshell.py\", line 3135, in run_cell_async\n",
      "    has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n",
      "  File \"/Users/luciahrovatin/Desktop/deep-learning-proj/venv/lib/python3.9/site-packages/IPython/core/interactiveshell.py\", line 3338, in run_ast_nodes\n",
      "    if await self.run_code(code, result, async_=asy):\n",
      "  File \"/Users/luciahrovatin/Desktop/deep-learning-proj/venv/lib/python3.9/site-packages/IPython/core/interactiveshell.py\", line 3398, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"/var/folders/_y/f4qxkhkd5lg_m6nl4sh3n4hh0000gn/T/ipykernel_19524/4029257795.py\", line 6, in <cell line: 6>\n",
      "    symnet.train_one_epoch(source_train_loader, target_train_loader)\n",
      "  File \"/var/folders/_y/f4qxkhkd5lg_m6nl4sh3n4hh0000gn/T/ipykernel_19524/1706884269.py\", line 57, in train_one_epoch\n",
      "    classifier_loss, generator_loss = self.overall_losses(\n",
      "  File \"/var/folders/_y/f4qxkhkd5lg_m6nl4sh3n4hh0000gn/T/ipykernel_19524/1706884269.py\", line 99, in overall_losses\n",
      "    _tgt_dom_discrim_loss = self.tgt_dom_discrim_loss(X_target_features)\n",
      "  File \"/Users/luciahrovatin/Desktop/deep-learning-proj/venv/lib/python3.9/site-packages/torch/nn/modules/module.py\", line 1110, in _call_impl\n",
      "    return forward_call(*input, **kwargs)\n",
      "  File \"/var/folders/_y/f4qxkhkd5lg_m6nl4sh3n4hh0000gn/T/ipykernel_19524/1599381704.py\", line 9, in forward\n",
      "    prob = self.to_softmax(input)\n",
      "  File \"/var/folders/_y/f4qxkhkd5lg_m6nl4sh3n4hh0000gn/T/ipykernel_19524/3980118938.py\", line 15, in to_softmax\n",
      "    prob = F.softmax(features, dim=1)\n",
      "  File \"/Users/luciahrovatin/Desktop/deep-learning-proj/venv/lib/python3.9/site-packages/torch/nn/functional.py\", line 1818, in softmax\n",
      "    ret = input.softmax(dim)\n",
      " (Triggered internally at  /Users/distiller/project/pytorch/torch/csrc/autograd/python_anomaly_mode.cpp:104.)\n",
      "  Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "one of the variables needed for gradient computation has been modified by an inplace operation: [torch.FloatTensor [32, 40]], which is output 0 of SoftmaxBackward0, is at version 1; expected version 0 instead. Hint: the backtrace further above shows the operation that failed to compute its gradient. The variable in question was changed in there or anywhere later. Good luck!",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m/Users/luciahrovatin/Desktop/deep-learning-proj/symNets.ipynb Cell 21'\u001b[0m in \u001b[0;36m<cell line: 6>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/luciahrovatin/Desktop/deep-learning-proj/symNets.ipynb#ch0000020?line=2'>3</a>\u001b[0m symnet \u001b[39m=\u001b[39m ModelTrainer(generator, \u001b[39m20\u001b[39m, \u001b[39m100\u001b[39m)\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/luciahrovatin/Desktop/deep-learning-proj/symNets.ipynb#ch0000020?line=4'>5</a>\u001b[0m symnet\u001b[39m.\u001b[39mtrain_one_epoch(source_train_loader, target_train_loader)\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/luciahrovatin/Desktop/deep-learning-proj/symNets.ipynb#ch0000020?line=5'>6</a>\u001b[0m symnet\u001b[39m.\u001b[39;49mtrain_one_epoch(source_train_loader, target_train_loader)\n",
      "\u001b[1;32m/Users/luciahrovatin/Desktop/deep-learning-proj/symNets.ipynb Cell 19'\u001b[0m in \u001b[0;36mModelTrainer.train_one_epoch\u001b[0;34m(self, source_dataloader, target_dataloader)\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/luciahrovatin/Desktop/deep-learning-proj/symNets.ipynb#ch0000018?line=56'>57</a>\u001b[0m classifier_loss, generator_loss \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39moverall_losses(\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/luciahrovatin/Desktop/deep-learning-proj/symNets.ipynb#ch0000018?line=57'>58</a>\u001b[0m     X_source_features, \n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/luciahrovatin/Desktop/deep-learning-proj/symNets.ipynb#ch0000018?line=58'>59</a>\u001b[0m     X_target_features, \n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/luciahrovatin/Desktop/deep-learning-proj/symNets.ipynb#ch0000018?line=59'>60</a>\u001b[0m     y_source_var\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/luciahrovatin/Desktop/deep-learning-proj/symNets.ipynb#ch0000018?line=60'>61</a>\u001b[0m )\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/luciahrovatin/Desktop/deep-learning-proj/symNets.ipynb#ch0000018?line=62'>63</a>\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodel\u001b[39m.\u001b[39moptim\u001b[39m.\u001b[39mzero_grad()\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/luciahrovatin/Desktop/deep-learning-proj/symNets.ipynb#ch0000018?line=63'>64</a>\u001b[0m classifier_loss\u001b[39m.\u001b[39;49mbackward(retain_graph\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m) \n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/luciahrovatin/Desktop/deep-learning-proj/symNets.ipynb#ch0000018?line=64'>65</a>\u001b[0m grad_classifier_tmp \u001b[39m=\u001b[39m []\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/luciahrovatin/Desktop/deep-learning-proj/symNets.ipynb#ch0000018?line=65'>66</a>\u001b[0m \u001b[39mfor\u001b[39;00m p \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodel\u001b[39m.\u001b[39mmodel\u001b[39m.\u001b[39mparameters():\n",
      "File \u001b[0;32m~/Desktop/deep-learning-proj/venv/lib/python3.9/site-packages/torch/_tensor.py:363\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    354\u001b[0m \u001b[39mif\u001b[39;00m has_torch_function_unary(\u001b[39mself\u001b[39m):\n\u001b[1;32m    355\u001b[0m     \u001b[39mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    356\u001b[0m         Tensor\u001b[39m.\u001b[39mbackward,\n\u001b[1;32m    357\u001b[0m         (\u001b[39mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    361\u001b[0m         create_graph\u001b[39m=\u001b[39mcreate_graph,\n\u001b[1;32m    362\u001b[0m         inputs\u001b[39m=\u001b[39minputs)\n\u001b[0;32m--> 363\u001b[0m torch\u001b[39m.\u001b[39;49mautograd\u001b[39m.\u001b[39;49mbackward(\u001b[39mself\u001b[39;49m, gradient, retain_graph, create_graph, inputs\u001b[39m=\u001b[39;49minputs)\n",
      "File \u001b[0;32m~/Desktop/deep-learning-proj/venv/lib/python3.9/site-packages/torch/autograd/__init__.py:173\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    168\u001b[0m     retain_graph \u001b[39m=\u001b[39m create_graph\n\u001b[1;32m    170\u001b[0m \u001b[39m# The reason we repeat same the comment below is that\u001b[39;00m\n\u001b[1;32m    171\u001b[0m \u001b[39m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    172\u001b[0m \u001b[39m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 173\u001b[0m Variable\u001b[39m.\u001b[39;49m_execution_engine\u001b[39m.\u001b[39;49mrun_backward(  \u001b[39m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    174\u001b[0m     tensors, grad_tensors_, retain_graph, create_graph, inputs,\n\u001b[1;32m    175\u001b[0m     allow_unreachable\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m, accumulate_grad\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: one of the variables needed for gradient computation has been modified by an inplace operation: [torch.FloatTensor [32, 40]], which is output 0 of SoftmaxBackward0, is at version 1; expected version 0 instead. Hint: the backtrace further above shows the operation that failed to compute its gradient. The variable in question was changed in there or anywhere later. Good luck!"
     ]
    }
   ],
   "source": [
    "generator = FeatureExtractor(20, 3, 'resnet50')\n",
    "\n",
    "symnet = ModelTrainer(generator, 20, 100)\n",
    "\n",
    "symnet.train_one_epoch(source_train_loader, target_train_loader)\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 ('venv': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "6f0d290f0742685d306541f8dcebbe79a177e37269f78587a0fc5052fa8d446c"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
