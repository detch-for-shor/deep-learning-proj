{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import  Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/luciahrovatin/Desktop/deep-learning-proj/venv/lib/python3.9/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os \n",
    "import imagesize\n",
    "import zipfile \n",
    "import statistics \n",
    "import math\n",
    "import torch\n",
    "import torchvision\n",
    "import wandb\n",
    "\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import torch.cuda as cuda\n",
    "import torchvision.transforms as T\n",
    "import torch.nn.functional as F\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "from tqdm import tqdm\n",
    "from statistics import mean\n",
    "from pathlib import Path\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch import Tensor\n",
    "from torchvision import models\n",
    "from torch.autograd import Variable\n",
    "from torch.utils.data.dataloader import DataLoader\n",
    "from torch.utils.data import Subset\n",
    "from torch.nn import CrossEntropyLoss\n",
    "from torch.optim import RMSprop, Adagrad\n",
    "from overrides import overrides, final\n",
    "from abc import abstractmethod\n",
    "#from google.colab import drive\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocess Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#wandb.init(project=\"uda project\",\n",
    "#           entity=\"dll_2022\")\n",
    "\n",
    "classes = [\"backpack\", \"bookcase\", \"car jack\", \"comb\", \"crown\", \"file cabinet\", \"flat iron\", \"game controller\", \"glasses\",\n",
    "           \"helicopter\", \"ice skates\", \"letter tray\", \"monitor\", \"mug\", \"network switch\", \"over-ear headphones\", \"pen\",\n",
    "           \"purse\", \"stand mixer\", \"stroller\"]\n",
    "\n",
    "domains = [\"product_images\", \"real_life\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_transformation(resize_dim = 256, crop_dim = 224, grayscale = True, crop_center = True):\n",
    "    \n",
    "    transform_lst = []\n",
    "    transform_lst.append(T.Resize((resize_dim)))                                                          \n",
    "    \n",
    "    if grayscale:\n",
    "        transform_lst.append(T.Grayscale(num_output_channels=3))                        \n",
    "    \n",
    "    if crop_center:\n",
    "        transform_lst.append(T.CenterCrop((crop_dim)))\n",
    "    else:\n",
    "        transform_lst.append(T.RandomCrop((crop_dim)))\n",
    "    \n",
    "    transform_lst.append(T.RandomHorizontalFlip(p=0.5))                                  \n",
    "    transform_lst.append(T.ToTensor())                                             \n",
    "        \n",
    "    return T.Compose(transform_lst)  \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalization(dataset):\n",
    "    ds_length = len(dataset)\n",
    "    for i in tqdm(range(ds_length)):\n",
    "        r_mean, g_mean, b_mean = torch.mean(dataset[i][0], dim = [1,2])\n",
    "        r_std, g_std, b_std = torch.std(dataset[i][0], dim = [1,2])\n",
    "        T.functional.normalize(\n",
    "            tensor = dataset[i][0], \n",
    "            mean = [r_mean, g_mean, b_mean],\n",
    "            std = [r_std, g_std, b_std],\n",
    "            inplace=True\n",
    "            )\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2000/2000 [01:09<00:00, 28.81it/s]\n",
      "100%|██████████| 2000/2000 [01:59<00:00, 16.79it/s]\n"
     ]
    }
   ],
   "source": [
    "source = \"product_images\"\n",
    "target = \"real_life\"\n",
    "resize_dim = 256\n",
    "crop_dim = 224\n",
    "grayscale = False\n",
    "crop_center = True \n",
    "\n",
    "\n",
    "source_ds = torchvision.datasets.ImageFolder(\n",
    "    root = f\"data/Adaptiope/{source}\",\n",
    "    transform = data_transformation(resize_dim, crop_dim, grayscale, crop_center)\n",
    "    )\n",
    "\n",
    "target_ds = torchvision.datasets.ImageFolder(\n",
    "    root = f\"data/Adaptiope/{target}\",\n",
    "    transform = data_transformation(resize_dim, crop_dim, grayscale, crop_center)\n",
    "    ) \n",
    "\n",
    "if not grayscale:\n",
    "    normalization(source_ds)\n",
    "    normalization(target_ds)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data(dataset, test_split=0.2, batch_size=32):\n",
    "    \n",
    "    train_indices, val_indices = train_test_split(\n",
    "        list(range(len(dataset.targets))),\n",
    "        test_size = test_split,\n",
    "        stratify = dataset.targets, \n",
    "        random_state = 42\n",
    "        )\n",
    "    \n",
    "    train_dataset = Subset(dataset, train_indices)\n",
    "    val_dataset = Subset(dataset, val_indices)\n",
    "\n",
    "    train_data_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "    val_data_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "    \n",
    "    return train_data_loader, val_data_loader\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "test_split = 0.2\n",
    "\n",
    "source_train_loader, source_val_loader = get_data(source_ds, test_split, batch_size)\n",
    "target_train_loader, target_val_loader = get_data(target_ds, test_split, batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class _Loss(nn.Module):\n",
    "    \n",
    "    _THRESHOLD = 1e-20\n",
    "    \n",
    "    def __init__(self):\n",
    "        super(_Loss, self).__init__()\n",
    "        \n",
    "    def forward(self, input: Tensor):\n",
    "        prob = self.to_softmax(input)\n",
    "        return self.loss(prob)\n",
    "        \n",
    "    @final\n",
    "    def add_threshold(self, prob: Tensor):\n",
    "        '''\n",
    "        Check whether the probability distribution after the softmax \n",
    "        is equal to 0 in any cell. If this holds, a standard threshold\n",
    "        is added in order to avoid log(0) case. \n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        prob: Tensor\n",
    "            output tensor of the softmax operation\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        Tensor\n",
    "            updated tensor (in case the condition above holds)\n",
    "        '''\n",
    "        zeros = (prob == 0)\n",
    "        if torch.any(zeros):\n",
    "            thre_tensor = torch.zeros(zeros.shape)\n",
    "            thre_tensor[zeros] = self._THRESHOLD\n",
    "            prob = prob + thre_tensor\n",
    "        return prob\n",
    "    \n",
    "    def to_softmax(self, features: Tensor):\n",
    "        '''\n",
    "        Apply the softmax operation on the features tensor, \n",
    "        being the output of a feature extractor. \n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        features: Tensor\n",
    "            input tensor of the softmax operation\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        Tensor\n",
    "            probability distribution with (possible) threshold\n",
    "        '''\n",
    "        prob = F.softmax(features, dim=1)\n",
    "        return self.add_threshold(prob)\n",
    "    \n",
    "    @abstractmethod\n",
    "    def loss(self, prob: Tensor):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EntropyMinimizationLoss(_Loss):\n",
    "    \n",
    "    def __init__(self, n_classes: int):\n",
    "        super(EntropyMinimizationLoss, self).__init__()\n",
    "        self.n_classes = n_classes\n",
    "    \n",
    "    @overrides\n",
    "    def loss(self, prob: Tensor):\n",
    "        prob_source = prob[:, :self.n_classes]\n",
    "        prob_target = prob[:, self.n_classes:]\n",
    "        prob_sum = prob_source + prob_target\n",
    "        return -(prob_sum.log().mul(prob_sum).sum(dim=1).mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SplitLoss(_Loss):\n",
    "    \n",
    "    def __init__(self, n_classes: int, source: bool, split_first: bool):\n",
    "        super(SplitLoss, self).__init__()\n",
    "        self.n_classes = n_classes\n",
    "        self._is_source = source\n",
    "        self._split_first = split_first\n",
    "    \n",
    "    @overrides\n",
    "    def to_softmax(self, features: Tensor):\n",
    "        if self._split_first:\n",
    "            prob = self.split_vector(features)\n",
    "            prob = F.softmax(prob, dim=1)\n",
    "        else:\n",
    "            prob = F.softmax(features, dim=1)\n",
    "            prob = self.split_vector(prob)\n",
    "        return self.add_threshold(prob)\n",
    "    \n",
    "    @final\n",
    "    def split_vector(self, prob: Tensor):\n",
    "        return prob[:,:self.n_classes] if self._is_source else prob[:,self.n_classes:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SplitCrossEntropyLoss(SplitLoss):\n",
    "    \n",
    "    def _get_y_labels(self):\n",
    "        return self._y_labels\n",
    "    def _set_y_labels(self, y_labels: Variable):\n",
    "        if not all(y < self.n_classes for y in y_labels):\n",
    "            raise ValueError('Expected all y labels < n_classes')\n",
    "        self._y_labels = y_labels\n",
    "    y_labels = property(fget=_get_y_labels, fset=_set_y_labels)\n",
    "    \n",
    "    def __init__(self, n_classes: int, source: bool, split_first: bool):\n",
    "        super(SplitCrossEntropyLoss, self).__init__(n_classes, source, split_first)\n",
    "        self.cross_entropy_loss = CrossEntropyLoss()\n",
    "    \n",
    "    @overrides\n",
    "    def loss(self, prob: Tensor):\n",
    "        '''Computes cross-entropy loss w.r.t. ground-truth (y label)'''\n",
    "        return self.cross_entropy_loss(prob, self.y_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DomainDiscriminationLoss(SplitLoss):\n",
    "    \n",
    "    def __init__(self, n_classes: int, source: bool):\n",
    "        super(DomainDiscriminationLoss, self).__init__(n_classes, source, False)\n",
    "        \n",
    "    @overrides\n",
    "    def loss(self, prob: Tensor):\n",
    "        return -(prob.sum(dim=1).log().mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TrainingObjectives:\n",
    "    \n",
    "    @staticmethod\n",
    "    def domain_discrimination_loss(src_dom_discrim_loss, tgt_dom_discrim_loss):\n",
    "        return src_dom_discrim_loss + tgt_dom_discrim_loss\n",
    "    \n",
    "    @staticmethod\n",
    "    def category_confusion_loss(src_cat_conf_loss, tgt_cat_conf_loss):\n",
    "        return 0.5 * (src_cat_conf_loss + tgt_cat_conf_loss)\n",
    "    \n",
    "    @staticmethod\n",
    "    def domain_confusion_loss(src_dom_conf_loss, tgt_dom_conf_loss):\n",
    "        return 0.5 * (src_dom_conf_loss + tgt_dom_conf_loss)\n",
    "    \n",
    "    @staticmethod\n",
    "    def overall_classifier_loss(src_task_class_loss, tgt_task_class_loss, domain_discrim_loss):\n",
    "        return src_task_class_loss + tgt_task_class_loss + domain_discrim_loss\n",
    "    \n",
    "    @staticmethod\n",
    "    def overall_generator_loss(cat_conf_loss, dom_conf_loss, tgt_entropy_loss, curr_epoch, tot_epochs):\n",
    "        lambda_trade_off = 2 / (1 + math.exp(-1 * 10 * curr_epoch / tot_epochs)) - 1\n",
    "        return cat_conf_loss + lambda_trade_off * (dom_conf_loss + tgt_entropy_loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Extractor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class FeatureExtractor:\n",
    "    \n",
    "    def __init__(self, source_only: bool, n_classes: int, freeze = True, n_params_trained = None, model='resnet50', optimizer='rmsprop', lr=0.01, weight_decay=0):\n",
    "        \"\"\"_summary_\n",
    "\n",
    "        Args:\n",
    "            n_classes (int): number of classes present in the dataset\n",
    "            n_params_trained (_type_, optional): Number of parameters (i.e., layers to be trained). Defaults to None.\n",
    "            model (str, optional): Pretrained model to import as feature extractor. Defaults to 'resnet18'.\n",
    "            optimizer (str, optional): Optimizer for the model. Defaults to 'rmsprop'.\n",
    "            lr (float, optional): Initial learning rate. Defaults to 0.01.\n",
    "            weight_decay (int, optional): Initial weight decay. Defaults to 0.\n",
    "        \"\"\"\n",
    "        self.lr = lr \n",
    "        \n",
    "        # Upload pretrained model \n",
    "        if model.lower() == 'resnet18': \n",
    "            self.model = models.resnet18(pretrained=True)\n",
    "        elif model.lower() == 'resnet50': \n",
    "            self.model = models.resnet50(pretrained=True)\n",
    "        else:\n",
    "            raise ValueError('Unknown model')\n",
    "        \n",
    "        # Modify last fully-connected layer\n",
    "        if not source_only: \n",
    "            self.model.fc = nn.Linear(\n",
    "                in_features = self.model.fc.in_features, \n",
    "                out_features = n_classes * 2\n",
    "            )\n",
    "        else:\n",
    "            self.model.fc = nn.Linear(\n",
    "                in_features = self.model.fc.in_features, \n",
    "                out_features = n_classes\n",
    "            ) \n",
    "        \n",
    "        n_params = len(list(self.model.parameters()))\n",
    "        if n_params_trained is None:\n",
    "            n_params_trained = n_params\n",
    "        \n",
    "        count = 0 \n",
    "        first_param_trained = n_params - n_params_trained\n",
    "        \n",
    "        if freeze:\n",
    "            for param in self.model.parameters():\n",
    "                param.requires_grad = (count >= first_param_trained)\n",
    "                count = count + 1 \n",
    "\n",
    "            params_to_train = filter(lambda p: p.requires_grad, self.model.parameters())\n",
    "        \n",
    "        else:\n",
    "            \n",
    "            # Layer-wise Learning Rate Decay (metà gruppi di layers = metà valore del lr)\n",
    "            \n",
    "            params_to_train = []\n",
    "            name_prev_group = None\n",
    "            \n",
    "            groups = set([name.split('.')[0] for name, _ in self.model.named_parameters()])\n",
    "            i = -1 \n",
    "            for name, param in self.model.named_parameters():\n",
    "                name_cur_group = name.split('.')[0]\n",
    "                if name_cur_group != name_prev_group or name_prev_group is None:\n",
    "                    i = i + 1\n",
    "                    lr_group = self.decay(i, len(groups)-1)\n",
    "                name_prev_group = name_cur_group\n",
    "                params_to_train.append({'params': param, 'lr': lr_group})\n",
    "            \n",
    "        # Initialize optimizer\n",
    "        if optimizer.lower() == 'rmsprop':\n",
    "            self.optim = torch.optim.RMSprop(\n",
    "                params = params_to_train,\n",
    "                lr = lr,\n",
    "                weight_decay = weight_decay\n",
    "            )\n",
    "        elif optimizer.lower() == 'adadelta':\n",
    "            self.optim = torch.optim.Adadelta(\n",
    "                params = params_to_train,\n",
    "                lr = lr,\n",
    "                weight_decay = weight_decay\n",
    "            )\n",
    "        elif optimizer.lower() == 'sgd':\n",
    "            self.optim = torch.optim.SGD(\n",
    "                params = params_to_train,\n",
    "                lr = lr,\n",
    "                weight_decay = weight_decay,\n",
    "                nesterov = True\n",
    "            )\n",
    "        else:\n",
    "            raise ValueError('Unknown optimizer')\n",
    "    \n",
    "    def decay(self, index: int, n_groups: int):\n",
    "        sigmoid = lambda x: 1/(1 + np.exp(-x)) \n",
    "        return self.lr * sigmoid(10/n_groups * (index - (n_groups/2))) \n",
    "            \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ModelTrainer:\n",
    "    _INF = 1e20\n",
    "    \n",
    "    def __init__(self, model: FeatureExtractor, n_classes: int, epochs: int):\n",
    "        \"\"\"Initialize the SymsNet model\n",
    "        Args:\n",
    "            model (FeatureExtractor): _description_\n",
    "            n_classes (int): _description_\n",
    "            epochs (int): _description_\n",
    "        \"\"\"\n",
    "       \n",
    "        self.curr_epoch = 0\n",
    "        self.tot_epochs = epochs\n",
    "        self.n_classes = n_classes\n",
    "        self.model = model \n",
    "        self.patience = 10 \n",
    "        \n",
    "        if cuda.is_available():\n",
    "            self.model = self.model.cuda()\n",
    "            # Task classifier losses\n",
    "            self.src_task_class_loss = SplitCrossEntropyLoss(n_classes=n_classes, source=True, split_first=True).cuda()\n",
    "            self.tgt_task_class_loss = SplitCrossEntropyLoss(n_classes=n_classes, source=False, split_first=True).cuda()\n",
    "            # Domain discrimination losses\n",
    "            self.src_dom_discrim_loss = DomainDiscriminationLoss(n_classes=n_classes, source=True).cuda()\n",
    "            self.tgt_dom_discrim_loss = DomainDiscriminationLoss(n_classes=n_classes, source=False).cuda()\n",
    "            # Category-level confusion losses\n",
    "            self.src_cat_conf_loss = SplitCrossEntropyLoss(n_classes=n_classes, source=True, split_first=False).cuda()\n",
    "            self.tgt_cat_conf_loss = SplitCrossEntropyLoss(n_classes=n_classes, source=False, split_first=False).cuda()\n",
    "            # Domain-level confusion losses\n",
    "            self.src_dom_conf_loss = DomainDiscriminationLoss(n_classes=n_classes, source=True).cuda()\n",
    "            self.tgt_dom_conf_loss = DomainDiscriminationLoss(n_classes=n_classes, source=False).cuda()\n",
    "            # Entropy minimization loss\n",
    "            self.tgt_entropy_loss = EntropyMinimizationLoss(n_classes=n_classes).cuda()    \n",
    "        \n",
    "        else: \n",
    "            self.src_task_class_loss = SplitCrossEntropyLoss(n_classes=n_classes, source=True, split_first=True)\n",
    "            self.tgt_task_class_loss = SplitCrossEntropyLoss(n_classes=n_classes, source=False, split_first=True)\n",
    "            # Domain discrimination losses\n",
    "            self.src_dom_discrim_loss = DomainDiscriminationLoss(n_classes=n_classes, source=True)\n",
    "            self.tgt_dom_discrim_loss = DomainDiscriminationLoss(n_classes=n_classes, source=False)\n",
    "            # Category-level confusion losses\n",
    "            self.src_cat_conf_loss = SplitCrossEntropyLoss(n_classes=n_classes, source=True, split_first=False)\n",
    "            self.tgt_cat_conf_loss = SplitCrossEntropyLoss(n_classes=n_classes, source=False, split_first=False)\n",
    "            # Domain-level confusion losses\n",
    "            self.src_dom_conf_loss = DomainDiscriminationLoss(n_classes=n_classes, source=True)\n",
    "            self.tgt_dom_conf_loss = DomainDiscriminationLoss(n_classes=n_classes, source=False)\n",
    "            # Entropy minimization loss\n",
    "            self.tgt_entropy_loss = EntropyMinimizationLoss(n_classes=n_classes)   \n",
    "\n",
    "            \n",
    "    def train_step(self, X_source: Tensor, y_source: Tensor, X_target: Tensor):\n",
    "        \n",
    "        # Tell model go training mode\n",
    "        self.model.model.train()\n",
    "        \n",
    "        # Compute features for both inputs\n",
    "        X_source_features = self.model.model(X_source)\n",
    "        X_target_features = self.model.model(X_target)\n",
    "        \n",
    "        # Compute overall training objective losses\n",
    "        classifier_loss, generator_loss = self.overall_losses(X_source_features, X_target_features, y_source)\n",
    "\n",
    "        # Compute gradients w.r.t. classifier loss\n",
    "        self.model.optim.zero_grad()\n",
    "        classifier_loss.backward(retain_graph=True)\n",
    "        grad_classifier_tmp = []\n",
    "        for p in self.model.model.parameters():\n",
    "            if p.grad is not None:\n",
    "                grad_classifier_tmp.append(p.grad.data.clone())\n",
    "        \n",
    "            \n",
    "        # Compute gradients w.r.t. generator loss\n",
    "        self.model.optim.zero_grad()\n",
    "        generator_loss.backward()\n",
    "        grad_generator_tmp = []\n",
    "        for p in self.model.model.parameters():\n",
    "            if p.grad is not None:\n",
    "                grad_generator_tmp.append(p.grad.data.clone())\n",
    "    \n",
    "        count = 0 \n",
    "        appended = 0 \n",
    "        n_classification_params = 2 \n",
    "        n_params = len(list(self.model.model.parameters()))\n",
    "        for p in self.model.model.parameters():\n",
    "            if p.grad is not None:\n",
    "                grad_tmp = p.grad.data.clone()\n",
    "                grad_tmp.zero_() \n",
    "                \n",
    "                # Whereas the last 2 parameters are trained with classifier loss,\n",
    "                # the others are trained with the generator loss  \n",
    "                if count < (n_params - n_classification_params): \n",
    "                    grad_tmp = grad_tmp + grad_generator_tmp[appended]\n",
    "                else: \n",
    "                    grad_tmp = grad_tmp + grad_classifier_tmp[appended]\n",
    "                appended = appended + 1 \n",
    "                p.grad.data = grad_tmp\n",
    "            count = count + 1 \n",
    "        \n",
    "        # Perform optimizer step    \n",
    "        self.model.optim.step()\n",
    "        target = y_source.clone().tolist()\n",
    "        preds = X_source_features.clone().tolist()\n",
    "        return classifier_loss, generator_loss, sum(torch.argmax(preds, dim=1) == target)/len(target)\n",
    "    \n",
    "    def train_epoch(self, source_dataloader: DataLoader, target_dataloader: DataLoader):\n",
    "        end_of_epoch = False\n",
    "        source_batch_loader = enumerate(source_dataloader)\n",
    "        target_batch_loader = enumerate(target_dataloader)\n",
    "        gen_loss = []\n",
    "        cl_loss = []\n",
    "        acc_source = []\n",
    "        \n",
    "        # Train for current epoch\n",
    "        while not end_of_epoch:\n",
    "            try:\n",
    "                # Get next batch for both source and target\n",
    "                (X_source, y_source) = next(source_batch_loader)[1]\n",
    "                (X_target, _) = next(target_batch_loader)[1]\n",
    "                c_loss, g_loss, acc_so = self.train_step(X_source, y_source, X_target)\n",
    "                gen_loss.append(g_loss), cl_loss.append(c_loss), acc_source.append(acc_so)\n",
    "                \n",
    "            # next(iter, default) if default is omitted and iter is empty, a StopIteration exception is yielded\n",
    "            except StopIteration: \n",
    "                end_of_epoch = True\n",
    "                return mean(cl_loss), mean(gen_loss), max(acc_source) \n",
    "            \n",
    "    def train(self, source_dataloader: DataLoader, target_dataloader: DataLoader):\n",
    "        \n",
    "        prev_classifier_loss = self._INF\n",
    "        prev_generator_loss = self._INF\n",
    "        prev_acc_source = 0.0\n",
    "        patience = self.patience \n",
    "        \n",
    "        epoch_iter = tqdm(range(self.tot_epochs), \n",
    "                          unit = \"epoch\",\n",
    "                          desc = \"Training\") \n",
    "        \n",
    "        for i in epoch_iter:\n",
    "        #while self.curr_epoch < self.tot_epochs:\n",
    "            self.curr_epoch = i\n",
    "            classifier_loss, generator_loss, acc_source = self.train_epoch(source_dataloader, target_dataloader)\n",
    "            logs = {\n",
    "                \"classifier_loss\": classifier_loss, \n",
    "                \"generator_loss\": generator_loss,\n",
    "                \"accuracy source\": acc_source \n",
    "                }\n",
    "            epoch_iter.set_postfix(logs)\n",
    "            #wandb.watch(self.model)\n",
    "            # wandb.log(logs)\n",
    "            \n",
    "            if (classifier_loss > prev_classifier_loss or \n",
    "                generator_loss > prev_generator_loss or \n",
    "                acc_source < prev_acc_source):\n",
    "                patience = patience - 1\n",
    "                if patience == 0:\n",
    "                    print('EARLY STOPPING') \n",
    "                    break\n",
    "\n",
    "    \n",
    "   \n",
    "    def overall_losses(self, X_source_features, X_target_features, y_source_var) -> tuple[Tensor, Tensor]:\n",
    "        # Source task classifier loss\n",
    "        self.src_task_class_loss.y_labels = y_source_var\n",
    "        _src_task_class_loss = self.src_task_class_loss(X_source_features)\n",
    "        \n",
    "        # (Cross-domain) Target task classifier loss\n",
    "        self.tgt_task_class_loss.y_labels = y_source_var\n",
    "        _tgt_task_class_loss = self.tgt_task_class_loss(X_source_features)\n",
    "        \n",
    "        # Domain discrimination loss\n",
    "        _src_dom_discrim_loss = self.src_dom_discrim_loss(X_source_features)\n",
    "        _tgt_dom_discrim_loss = self.tgt_dom_discrim_loss(X_target_features)\n",
    "        _domain_discrim_loss = TrainingObjectives.domain_discrimination_loss(\n",
    "            _src_dom_discrim_loss, \n",
    "            _tgt_dom_discrim_loss\n",
    "        )\n",
    "        \n",
    "        # Category-level confusion loss\n",
    "        self.src_cat_conf_loss.y_labels = y_source_var\n",
    "        self.tgt_cat_conf_loss.y_labels = y_source_var\n",
    "        _src_cat_conf_loss = self.src_cat_conf_loss(X_source_features)\n",
    "        _tgt_cat_conf_loss = self.tgt_cat_conf_loss(X_source_features)\n",
    "        _category_conf_loss = TrainingObjectives.category_confusion_loss(\n",
    "            _src_cat_conf_loss, \n",
    "            _tgt_cat_conf_loss\n",
    "        )\n",
    "        \n",
    "        # Domain-level confusion loss\n",
    "        _src_dom_conf_loss = self.src_cat_conf_loss(X_target_features)\n",
    "        _tgt_dom_conf_loss = self.tgt_cat_conf_loss(X_target_features)\n",
    "        _domain_conf_loss = TrainingObjectives.domain_confusion_loss(\n",
    "            _src_dom_conf_loss, \n",
    "            _tgt_dom_conf_loss\n",
    "        )\n",
    "\n",
    "        # Entropy minimization loss\n",
    "        _tgt_entropy_loss = self.tgt_entropy_loss(X_target_features)\n",
    "        \n",
    "        # Overall classifier loss\n",
    "        _overall_classifier_loss = TrainingObjectives.overall_classifier_loss(\n",
    "            _src_task_class_loss, \n",
    "            _tgt_task_class_loss, \n",
    "            _domain_discrim_loss\n",
    "        )\n",
    "\n",
    "        # Overall feature extractor loss\n",
    "        _overall_generator_loss = TrainingObjectives.overall_generator_loss(\n",
    "            _category_conf_loss, \n",
    "            _domain_conf_loss, \n",
    "            _tgt_entropy_loss, \n",
    "            self.curr_epoch, \n",
    "            self.tot_epochs\n",
    "        )\n",
    "        \n",
    "        return _overall_classifier_loss, _overall_generator_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generator = FeatureExtractor(source_only = False, n_classes=20, model='resnet50', freeze = False)\n",
    "symnet = ModelTrainer(model=generator, n_classes=20, epochs=2)\n",
    "symnet.train(source_train_loader, target_train_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# FROM THE ASSIGNMENT DESCRIPTION \n",
    "\n",
    "Suppose you’re working on the direction product → real world. Then the first thing you will do is train your model on $P_{train}$. Since this is your <i>source domain </i>, you are allowed to use label information (e.g. use a cross entropy loss in your training step). In your test step, you are going to evaluate the model on $RW_{test}$. This will achieve a certain accuracy; since we only trained on the source domain, and not on the target domain, this accuracy refers to the source only scenario. We call it $acc_{so}$. Now you want to evaluate your UDA component which, differently from the former case, implies training on the target domain. Since you are not allowed to use labels there, here you will use any UDA device of your choice. So, in this case, in your training step, you will train supervisedly on $P_{train}$ (like you did before) and simultaneously train unsupervisedly on $RW_train$. In your test step, once again, you want to evaluate on $RW_{test}$. This will achieve a new accuracy $acc_{uda}$, which hopefully will be higher than $acc_{so}$ since this time you also trained on the target domain, even if without label information. At this point you can compute your gain G:\n",
    "$$G = accu_{da} − acc_{so}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SourceModelTrainer:\n",
    "    \n",
    "    def __init__(self, model: FeatureExtractor, n_classes: int, epochs: int):\n",
    "        \"\"\"Initialize the SymsNet model\n",
    "        Args:\n",
    "            model (FeatureExtractor): _description_\n",
    "            n_classes (int): _description_\n",
    "            epochs (int): _description_\n",
    "        \"\"\"\n",
    "        \n",
    "        self.curr_epoch = 0\n",
    "        self.tot_epochs = epochs\n",
    "        self.n_classes = n_classes\n",
    "        self.model = model \n",
    "        \n",
    "        if cuda.is_available():\n",
    "            self.model = self.model.cuda()\n",
    "        \n",
    "        # Cross entropy Loss\n",
    "        self.loss = CrossEntropyLoss()\n",
    "            \n",
    "    def train_step(self, X_source: Tensor, y_source: Tensor):\n",
    "        \n",
    "        # Tell model go training mode\n",
    "        self.model.model.train()\n",
    "        \n",
    "        # Compute features for both inputs\n",
    "        X_source_features = self.model.model(X_source)\n",
    "        \n",
    "        # Compute overall training objective losses\n",
    "        general_loss = self.loss(X_source_features, y_source)\n",
    "\n",
    "        # Compute gradients w.r.t. classifier loss\n",
    "        self.model.optim.zero_grad()\n",
    "        general_loss.backward()\n",
    "        \n",
    "        # Perform optimizer step    \n",
    "        self.model.optim.step()\n",
    "    \n",
    "    def train_epoch(self, source_dataloader: DataLoader):\n",
    "        end_of_epoch = False\n",
    "        source_batch_loader = enumerate(source_dataloader)\n",
    "\n",
    "        # Train for current epoch\n",
    "        while not end_of_epoch:\n",
    "            try:\n",
    "                # Get next batch for both source and target\n",
    "                (X_source, y_source) = next(source_batch_loader)[1]\n",
    "                self.train_step(X_source, y_source)\n",
    "            # next(iter, default) if default is omitted and iter is empty, a StopIteration exception is yielded\n",
    "            except StopIteration: \n",
    "                end_of_epoch = True\n",
    "                return\n",
    "            \n",
    "    def train(self, source_dataloader: DataLoader):\n",
    "        prev_general_loss = self._INF\n",
    "        prev_acc_source = 0.0\n",
    "        patience = self.patience \n",
    "        \n",
    "        epoch_iter = tqdm(range(self.tot_epochs), \n",
    "                          unit = \"epoch\",\n",
    "                          desc = \"Training source only model\") \n",
    "        \n",
    "        for i in epoch_iter:\n",
    "            general_loss, acc_source = self.train_epoch(source_dataloader)\n",
    "            logs = {\n",
    "                \"general_loss\": general_loss, \n",
    "                \"accuracy\": acc_source \n",
    "                }\n",
    "            epoch_iter.set_postfix(logs)\n",
    "            \n",
    "            if (general_loss > prev_general_loss or \n",
    "                acc_source < prev_acc_source):\n",
    "                patience = patience - 1\n",
    "                if patience == 0:\n",
    "                    print('EARLY STOPPING') \n",
    "                    break\n",
    "\n",
    "    \n",
    "   \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "source_only_generator = FeatureExtractor(source_only = True, n_classes=20, model='resnet50')\n",
    "source_only_classifier = SourceModelTrainer(model=generator, n_classes=20, epochs=1)\n",
    "source_only_classifier.train(source_train_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ModelValidator: \n",
    "    \n",
    "    def __init__(self, model: FeatureExtractor, n_classes: int, source_only: bool):\n",
    "        \"\"\"Initialize the SymsNet model\n",
    "        Args:\n",
    "            model (FeatureExtractor): _description_\n",
    "            n_classes (int): _description_\n",
    "            epochs (int): _description_\n",
    "        \"\"\"\n",
    "        self.model = model \n",
    "        self.n_classes = n_classes\n",
    "        self.loss_general = CrossEntropyLoss()\n",
    "        self.loss_source = SplitCrossEntropyLoss(n_classes=n_classes, source=True, split_first=True).cuda()\n",
    "        self.loss_target = SplitCrossEntropyLoss(n_classes=n_classes, source=False, split_first=True).cuda()\n",
    "        self.source_only = source_only\n",
    "        self.val_loss_source = []\n",
    "        self.val_loss_target = []\n",
    "        self.acc_uda = []\n",
    "        \n",
    "        self.val_loss_general = []\n",
    "        self.acc_so = []\n",
    "        \n",
    "    def validation_step(self, input, target):\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            preds = self.model.model(input)\n",
    "            \n",
    "        if self.source_only: \n",
    "            loss = self.loss_general(preds, target) \n",
    "            acc_so = sum(torch.argmax(preds, dim=1) == target)/len(target)\n",
    "            return loss, acc_so\n",
    "        \n",
    "        else: \n",
    "            self.loss_source.y_labels = target\n",
    "            self.loss_target.y_labels = target\n",
    "            loss_source = self.loss_source(preds)\n",
    "            loss_target = self.loss_target(preds)\n",
    "            acc_uda = sum(torch.argmax(preds, dim=1) == target)/len(target)\n",
    "            \n",
    "            return (loss_source, loss_target), acc_uda\n",
    "        \n",
    "    \n",
    "    def validate(self, val_loader: DataLoader):\n",
    "        \n",
    "        self.model.model.eval()\n",
    "        validator = enumerate(val_loader)\n",
    "        \n",
    "        for _ , (input, target) in validator: \n",
    "            if cuda.is_available():\n",
    "                input, target = input.cuda(), target.cuda()\n",
    "            if self.source_only:\n",
    "                loss, acc  = self.validation_step(input, target)\n",
    "                self.val_loss_general.append(loss)\n",
    "                self.acc_so.append(acc)\n",
    "                 \n",
    "            else: \n",
    "                (ls_source, ls_target), acc = self.validation_step(input, target)\n",
    "                self.val_loss_source.append(ls_source)\n",
    "                self.val_loss_target.append(ls_target)\n",
    "                self.acc_uda.append(acc)\n",
    "        \n",
    "        if self.source_only: \n",
    "            return min(self.val_loss_general), max(self.acc_so)\n",
    "        \n",
    "        return (min(self.val_loss_source), min(self.val_loss_target)), max(self.acc_uda)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_so = ModelValidator(model=source_only_generator, n_classes=20, source_only = True)\n",
    "val_loss, acc_so = eval_so.validate(target_val_loader)\n",
    "\n",
    "eval_uda = ModelValidator(model=generator, n_classes=20, source_only = False)\n",
    "(val_loss_source, val_loss_target), acc_uda =  eval_uda.validate(target_val_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Overall GAIN with source only model and UDA model\n",
    "def overall_gain(acc_so, acc_uda):\n",
    "    return (acc_uda - acc_so)*100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(-3.1250)\n",
      "tensor(0.1562)\n",
      "tensor(0.1250)\n"
     ]
    }
   ],
   "source": [
    "print(overall_gain(acc_so, acc_uda))\n",
    "print(acc_so)\n",
    "print(acc_uda)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 ('venv': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "6f0d290f0742685d306541f8dcebbe79a177e37269f78587a0fc5052fa8d446c"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
