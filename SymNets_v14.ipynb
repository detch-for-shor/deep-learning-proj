{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gZK38mjSsvE6"
      },
      "source": [
        "<center>\n",
        "\n",
        "# SymNet for Adversarial Domain Adaptation\n",
        "## Deep Learning Assignment\n",
        "### A.Y. 2021/22 - University of Trento \n",
        "#### Hrovatin Lucia e Miotto Luca\n",
        "\n",
        "</center>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GBGRLj3wsvE9"
      },
      "source": [
        "# Abstract"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RgoLbA5ksvE-"
      },
      "source": [
        "\n",
        "In this notebook, we tackle the Unsupervised Domain Adaptation (UDA) task on the reduced version of Adaptiope dataset ([Ringwald & Stiefelhagen](https://paperswithcode.com/dataset/adaptiope), 2021). This version considers the <code>real world</code> (<i>RW</i>) and <code>product</code> (<i>P</i>) domains and narrow down the classes to $20$.\n",
        "\n",
        "Our solution is inspired by the <i>Domain-Symmetric Networks</i> (<i>SymNets</i>), proposed by [Zhang](https://arxiv.org/abs/1904.04663) <i>et al.</i> (2019), and suggests additional architectural refinements to overcome model overfitting and overconfidence.\n",
        "\n",
        "The deployed version of the architecture outperforms both the external baseline (i.e., from the assignment description) and the internal one, namely the source-only model. However, it is worth noting how the <i>gain</i>'s magnitude varies depending on <code>source</code> and <code>target</code> domain definition, meaning the <i>direction</i> of the adaptation.  \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kjIgn4KUsvE-"
      },
      "source": [
        "# Environment Preparation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dOxPcEXesvE-"
      },
      "source": [
        "## Python Version"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "i6s1U3SG0cGq"
      },
      "outputs": [],
      "source": [
        "%%capture \n",
        "\n",
        "!sudo apt-get update -y\n",
        "!sudo apt-get install python3.10\n",
        "!sudo update-alternatives --install /usr/bin/python3 python3 /usr/bin/python3.7 1\n",
        "!sudo update-alternatives --install /usr/bin/python3 python3 /usr/bin/python3.10 2\n",
        "!python --version"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "veDaOcVb0G9h"
      },
      "source": [
        "## Import  Libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "6Iz5l41k0G9j"
      },
      "outputs": [],
      "source": [
        "import os \n",
        "import json\n",
        "import math\n",
        "import torch\n",
        "import torchvision\n",
        "import shutil\n",
        "import imagesize\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "import numpy as np\n",
        "import torch.nn as nn\n",
        "import torch.cuda as cuda\n",
        "import torchvision.transforms as T\n",
        "import torch.nn.functional as F\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from abc import abstractmethod\n",
        "from tqdm import tqdm\n",
        "from statistics import mean, stdev\n",
        "from pathlib import Path\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score \n",
        "from torch import Tensor\n",
        "from torchvision import models\n",
        "from torchvision.datasets import ImageFolder\n",
        "from torch.utils.data.dataloader import DataLoader\n",
        "from torch.utils.data import Subset\n",
        "from torch.nn import CrossEntropyLoss\n",
        "from torch.optim import RMSprop, Adadelta, SGD\n",
        "from google.colab import drive \n",
        "\n",
        "## Not supported in Colab, but explicit references among (sub)classes can be activated\n",
        "#from overrides import overrides, final "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Mc70n_wjbGgr"
      },
      "source": [
        "## HyperParameters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "2KJ4bA8ybGgt"
      },
      "outputs": [],
      "source": [
        "# Device Settings\n",
        "seed = 42 # allow reproducibility of the results\n",
        "\n",
        "# EDA\n",
        "thre = 2 # threshold to check whether images are squared or almost squared\n",
        "\n",
        "# Data Transformation\n",
        "path = \"./adaptiope_small/\"\n",
        "source = \"real_life/\"\n",
        "target = \"product_images/\" \n",
        "resize_dim = 256 \n",
        "crop_dim = 224 # size chosen due to ResNet \n",
        "grayscale = False \n",
        "crop_center = True\n",
        "\n",
        "# Data Loader\n",
        "batch_size = 30 # 15 might also be a reasonable choice given the small number of classes \n",
        "test_split = 0.2 # given by the assignment description\n",
        "\n",
        "# Feature Extractor\n",
        "n_classes = 20 # given by the assignment description\n",
        "model = 'resnet50' # resnet18 has been tested as well \n",
        "optim = 'rmsprop' \n",
        "n_params_trained = 10 # result after fine-tuning \n",
        "weight_decay = 1e-4 # result after fine-tuning \n",
        "lr = 1e-2 # result after fine-tuning \n",
        "dropout = 0.5 # result after fine-tuning \n",
        "label_smoothing = 0.2 # result after fine-tuning \n",
        "source_only = False  \n",
        "\n",
        "# Model Trainer\n",
        "n_epochs = 250 # parameter set extremely high, but never reached\n",
        "patience = 10 "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eG_KkWoF1iLA"
      },
      "source": [
        "## Device Settings"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F-Ccv0WA0wUL",
        "outputId": "ca2ed2e5-b93b-4e4d-ac69-6ad4d8c85d21"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "You're running on GPU\n"
          ]
        }
      ],
      "source": [
        "np.random.seed(seed)\n",
        "torch.manual_seed(seed)\n",
        "if cuda.is_available():\n",
        "    print('You\\'re running on GPU')\n",
        "    cuda.manual_seed(seed)\n",
        "    gpu = True\n",
        "else:\n",
        "    print('You\\'re running on CPU')\n",
        "    gpu = False"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZYpPF-EA01MY",
        "outputId": "39d3333c-6fe9-4803-8ef7-8795d1495201"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fri Aug 19 18:59:34 2022       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 460.32.03    Driver Version: 460.32.03    CUDA Version: 11.2     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla P100-PCIE...  Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   48C    P0    32W / 250W |      2MiB / 16280MiB |      0%      Default |\n",
            "|                               |                      |                  N/A |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n"
          ]
        }
      ],
      "source": [
        "gpu_info = !nvidia-smi\n",
        "gpu_info = '\\n'.join(gpu_info)\n",
        "if gpu_info.find('failed') >= 0:\n",
        "  print('Not connected to a GPU')\n",
        "else:\n",
        "  print(gpu_info)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kIlVaihW04ub",
        "outputId": "2f086dd6-80e3-46c3-bc33-7a63f8aea409"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "27.33 GB of RAM\n",
            "\n"
          ]
        }
      ],
      "source": [
        "from psutil import virtual_memory\n",
        "ram_gb = virtual_memory().total / 1e9\n",
        "ram_gb = round(ram_gb, 2)\n",
        "print(f'{ram_gb} GB of RAM\\n')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "En10ic2_0G9s"
      },
      "source": [
        "## Model Saver "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "yci2ngnwbGhp"
      },
      "outputs": [],
      "source": [
        "class ModelSaver:\n",
        "    \n",
        "    def __init__(self, base_dir: str, source_only=False):\n",
        "        \"\"\"\n",
        "        The class saves the best running model as pickle file, along with its \n",
        "        results and parameters as json. \n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        base_dir: str\n",
        "            directory to save the files \n",
        "        source_only: bool\n",
        "            if True, it saves the results of the source only model\n",
        "        \n",
        "        Methods\n",
        "        -------\n",
        "        check_base_dir:\n",
        "            Check whether the directory exists or not.  \n",
        "\n",
        "        model_save:\n",
        "            Save the best model and optimizer in a pickle file.\n",
        "        \n",
        "        save_hyperparam: \n",
        "            Save the list of hyperparameters employed \n",
        "            by the model in a json file.  \n",
        "\n",
        "        save_results:         \n",
        "            Save the model results (e.g., accuracy on train and validation)\n",
        "            in a json file.  \n",
        "        \"\"\"\n",
        "        self.base_dir = base_dir\n",
        "        self.filename = source.split(\"_\")[0] + \"2\" + target.split(\"_\")[0]\n",
        "        self.fullpath = f'{base_dir}/{self.filename}'\n",
        "        self.source_only = source_only\n",
        "    \n",
        "    def check_base_dir(self):\n",
        "        try:\n",
        "            if not os.path.exists(self.base_dir):\n",
        "                os.makedirs(self.base_dir)\n",
        "            return True\n",
        "        except OSError:\n",
        "            return False\n",
        "\n",
        "    def model_save(self, model, optimizer):\n",
        "        # Save only if the base directory exists\n",
        "        if self.check_base_dir(): \n",
        "            with open(f'{self.fullpath}.pickle', 'wb') as f:\n",
        "                torch.save([model, optimizer], f)\n",
        "        else:\n",
        "            raise Exception('Model saving failed')\n",
        "    \n",
        "    def save_hyperparam(self):\n",
        "        params_dict = {\n",
        "            'seed': seed,\n",
        "            'source': source,\n",
        "            'target': target,\n",
        "            'grayscale': grayscale,\n",
        "            'crop_center': crop_center,\n",
        "            'batch_size': batch_size,\n",
        "            'test_split': test_split,\n",
        "            'n_classes': n_classes,\n",
        "            'model': model,\n",
        "            'optimizer': optim,\n",
        "            'n_params_trained': n_params_trained,\n",
        "            'weight_decay': weight_decay,\n",
        "            'learning_rate': lr,\n",
        "            'dropout': dropout,\n",
        "            'label_smoothing': label_smoothing,\n",
        "            'n_epochs': n_epochs,\n",
        "            'patience': patience\n",
        "        }\n",
        "        if self.check_base_dir():\n",
        "            with open(f'{self.fullpath}_params.json', 'w') as f:\n",
        "                json.dump(params_dict, f, indent=4)\n",
        "        else:\n",
        "            raise Exception('Hyperparams saving failed')\n",
        "            \n",
        "    def save_results(self, results: dict):\n",
        "        if self.check_base_dir():\n",
        "            if self.source_only:\n",
        "                self.fullpath = self.fullpath + \"_so\"\n",
        "            with open(f'{self.fullpath}_results.json', 'w') as f:\n",
        "                json.dump(results, f, indent=4)\n",
        "        else:\n",
        "            raise Exception('Results saving failed')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qB-fqOSgsvFD"
      },
      "source": [
        "# Data Collection"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "_byHbGvzCpOS"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "\n",
        "drive.mount('/content/drive', force_remount=True)\n",
        "\n",
        "if not os.path.exists('/content/adaptiope_small'):\n",
        "  !mkdir \"/content/dataset\"\n",
        "  \n",
        "  try:\n",
        "    !cp \"/content/drive/My Drive/DLL_project/Adaptiope.zip\" \"/content/dataset/\"\n",
        "  except: \n",
        "    raise FileExistsError(\"Could not find this directory, add a shortcut to your drive\")\n",
        "  \n",
        "  !ls \"/content/dataset\"\n",
        "  !unzip \"/content/dataset/Adaptiope.zip\"\n",
        "  !rm -rf \"/content/adaptiope_small\"\n",
        "  !mkdir \"/content/adaptiope_small\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wHM96q2tDMXd",
        "outputId": "c493ba2e-546a-414f-ba99-b17f112e326e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 20/20 [00:02<00:00,  8.65it/s]\n",
            "100%|██████████| 20/20 [00:00<00:00, 21.67it/s]\n"
          ]
        }
      ],
      "source": [
        "# Filter Adaptiope dataset to extract the classes of interest \n",
        "\n",
        "classes = [\"backpack\", \"bookcase\", \"car jack\", \"comb\", \"crown\", \"file cabinet\", \"flat iron\", \"game controller\", \"glasses\",\n",
        "           \"helicopter\", \"ice skates\", \"letter tray\", \"monitor\", \"mug\", \"network switch\", \"over-ear headphones\", \"pen\",\n",
        "           \"purse\", \"stand mixer\", \"stroller\"]\n",
        "\n",
        "for d, td in zip([\"Adaptiope/product_images\", \"Adaptiope/real_life\"], [\"adaptiope_small/product_images\", \"adaptiope_small/real_life\"]):\n",
        "  if not os.path.exists(td):\n",
        "    os.makedirs(td)\n",
        "    for c in tqdm(classes):\n",
        "      c_path = os.path.join(d, c)\n",
        "      c_target = os.path.join(td, c)\n",
        "      shutil.copytree(c_path, c_target)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ehyhsDQ2svFD"
      },
      "source": [
        "# Data Preparation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8HWKS2D8svFD"
      },
      "source": [
        "## Exploratory Data Analysis (EDA)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M5RHK3H-svFD"
      },
      "source": [
        "An Exploratory Data Analysis has been performed to learn about the data we are working with. Specifically, the metadata of each image has been extracted and saved into a dictionary(**metadata_img**). "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "4UolhksQsvFE"
      },
      "outputs": [],
      "source": [
        "metadata_img = {}\n",
        "domains = [\"product_images\", \"real_life\"]\n",
        "\n",
        "for domain in domains:\n",
        "  subclass = os.listdir(f\"{path}{domain}\")\n",
        "  for cls_name in classes:\n",
        "    imgs = [img.name for img in Path(f\"{path}{domain}/{cls_name}/\").iterdir() if img.suffix == \".jpg\"]\n",
        "    for img in imgs: \n",
        "      metadata_img[str(img + domain)] = imagesize.get(f\"{path}{domain}/{cls_name}/\"+img)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nsXBWIKVsvFE",
        "outputId": "cb8ac1b7-118a-40ce-fc67-00d1ea199c3a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Height: avg = 1122 ± sd = 415.62\n",
            "Width: avg = 1160 ± sd = 415.42\n"
          ]
        }
      ],
      "source": [
        "img_df = pd.DataFrame.from_dict([metadata_img]).T.reset_index().set_axis(['file_name', 'size'], axis='columns', inplace=False)\n",
        "\n",
        "# store width and height in a Pandas data frame\n",
        "img_df[[\"width\", \"height\"]] = pd.DataFrame(img_df[\"size\"].tolist(), index=img_df.index)\n",
        "\n",
        "# define the domain of each image\n",
        "img_df[\"domain\"] = pd.DataFrame(img_df[\"file_name\"].apply(lambda x: x[1:].split('.jpg')[-1]))\n",
        "\n",
        "avg_width = round(img_df[\"width\"].sum() / len(img_df))\n",
        "avg_height = round(img_df[\"height\"].sum() / len(img_df))\n",
        "sd_width = round(stdev(img_df[\"width\"]), 2)\n",
        "sd_height = round(stdev(img_df[\"height\"]), 2)\n",
        "\n",
        "print(f'Height: avg = {avg_height} ± sd = {sd_height}')\n",
        "print(f'Width: avg = {avg_width} ± sd = {sd_width}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cRJMdTK8svFE"
      },
      "source": [
        "<center>\n",
        "\n",
        "| Dimension| Average | Standard deviation | Minimum | Maximum |\n",
        "|  :----: | :-----------: |:----------: |  :----: |  :----: | \n",
        "| Height | 1122 | 415.62| 75 | 2560|\n",
        "| Width | 1160 | 415.42| 120| 2870|\n",
        "\n",
        "</center>\n",
        "\n",
        "The high standard deviations and discrepancies between extremes (i.e., <i>min</i> and <i>max</i>) stress the need for further analyses. Thus, several insights can be gained by displaying the underlying distributions of <i>height</i> and <i>width</i>. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 493
        },
        "id": "5bn_CHeBsvFE",
        "outputId": "d1a2bc04-5d05-4534-b7fe-c257eaa8c040"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1080x504 with 2 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ],
      "source": [
        "fig, (ax1, ax2) = plt.subplots(1,2, figsize=(15, 7))\n",
        "fig.suptitle(\"Distribution of height and width in the reduced Adaptiope\", \n",
        "             fontsize=18)\n",
        "\n",
        "sns.histplot(\n",
        "    data = img_df[\"height\"],\n",
        "    kde = True, \n",
        "    ax = ax1, \n",
        "    stat = \"density\", \n",
        "    color = \"lightgreen\")\n",
        "\n",
        "sns.histplot(\n",
        "    data = img_df[\"width\"],\n",
        "    kde = True,\n",
        "    ax = ax2, \n",
        "    stat = \"density\")\n",
        "\n",
        "ax1.set(xlabel='Height (pixel)', ylabel='Density')\n",
        "ax2.set(xlabel='Width (pixel)', ylabel='Density')\n",
        "\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PZ65ZhrKsvFE"
      },
      "source": [
        "Three considerations follow: \n",
        "\n",
        "1. as aforementioned, the extremes cover a broad range. Indeed, the distributions start from minima below 100 (pixel) to maxima around 2500 (pixel).\n",
        "\n",
        "2. basic statistics (e.g., mean) did not detect the presence of the three peaks, which are symmetrically set at 700, 1200 and 1500 pixels\n",
        "\n",
        "3. the previous consideration about the peaks' symmetry might suggest a moderate presence of squared images. \n",
        "\n",
        "The last statement has been checked by computing the percentage of images having an (almost) squared shape. A threshold of $t = 2$ (pixels) has been added to detect images with similar dimensions, namely slightly rectangular shapes. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MjBYPSS5svFE",
        "outputId": "c26ebde7-6331-41da-f75f-bae0045423f6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Squared images in the dataset: 33.0 %\n"
          ]
        }
      ],
      "source": [
        "cond = np.logical_and(img_df['height'] - thre < img_df['width'], img_df['width'] < img_df['height'] + thre)\n",
        "sq_df = img_df[cond]\n",
        "print('Squared images in the dataset: ' + str(round(len(sq_df.index)/len(img_df.index), 2)*100) + \" %\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GCXYh1O3svFE"
      },
      "source": [
        "Even though the threshold is posed, only $\\frac{1}{3}$ of the images are (almost) squared. This result is relevant in evaluating the data transformation step (see next Section), as the resizing and cropping procedure will determine a consistent lack of information and increase the model generalizability. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QhWEa-5C0G9l"
      },
      "source": [
        "## Data Transformation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "KZSDTkjY0G9m"
      },
      "outputs": [],
      "source": [
        "def data_transformation(resize_dim=256, crop_dim=224, grayscale=False, crop_center=True):\n",
        "    \"\"\"Transform the data by resizing, cropping, recoloring and flipping it. \n",
        "\n",
        "    Args:\n",
        "        resize_dim (int): Defaults set to 256.\n",
        "        crop_dim (int): Set to 224 as the ResNet requires these dimensions \n",
        "        grayscale (bool): If True, the images are converted to grayscale\n",
        "        crop_center (bool): If True images are cropped in the center, otherwise the crop is random\n",
        "    \"\"\"\n",
        "    transform_lst = []\n",
        "    \n",
        "    # Resizes the images \n",
        "    transform_lst.append(T.Resize((resize_dim)))                                                          \n",
        "    \n",
        "    if grayscale:\n",
        "        # Converts images to grayscale and returned images are 3 channels (with r == g == b)\n",
        "        transform_lst.append(T.Grayscale(num_output_channels=3))                        \n",
        "    \n",
        "    if crop_center:\n",
        "        # Crops the image in the center\n",
        "        transform_lst.append(T.CenterCrop((crop_dim)))\n",
        "    else:\n",
        "        # Performs a random crop \n",
        "        transform_lst.append(T.RandomCrop((crop_dim)))\n",
        "    \n",
        "    # Flips the image randomly with the given probability\n",
        "    transform_lst.append(T.RandomHorizontalFlip(p=0.5))\n",
        "    \n",
        "    # Convert the images to tensors                                  \n",
        "    transform_lst.append(T.ToTensor())                                             \n",
        "    \n",
        "    # Composes several transforms together and returns them\n",
        "    return T.Compose(transform_lst)  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "BtPLbGgh0G9m"
      },
      "outputs": [],
      "source": [
        "def image_normalization(dataset: ImageFolder) -> ImageFolder:\n",
        "    \"\"\" \n",
        "    Normalize images by changing the range of pixel intensity values.\n",
        "    The normalization employs mean (mean[1],...,mean[n]) and standard deviation (std[1],..,std[n])\n",
        "    for n, number of channels (i.e., 3). Each channel of the input tensor will be \n",
        "    normalized as follows: output[channel] = (input[channel] - mean[channel]) / std[channel]\n",
        "    \"\"\"\n",
        "    ds_length = len(dataset)\n",
        "    for i in tqdm(range(ds_length)):\n",
        "        r_mean, g_mean, b_mean = torch.mean(dataset[i][0], dim = [1,2])\n",
        "        r_std, g_std, b_std = torch.std(dataset[i][0], dim = [1,2])\n",
        "        T.functional.normalize(\n",
        "            tensor = dataset[i][0], \n",
        "            mean = [r_mean, g_mean, b_mean],\n",
        "            std = [r_std, g_std, b_std],\n",
        "            inplace=True)\n",
        "    return dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M6dBd8Zh0G9n",
        "outputId": "712b1b80-8f4e-432f-f4ad-b47d4e6f92e3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 2000/2000 [03:39<00:00,  9.11it/s]\n",
            "100%|██████████| 2000/2000 [02:05<00:00, 15.94it/s]\n"
          ]
        }
      ],
      "source": [
        "# Source Domain\n",
        "source_ds = ImageFolder(\n",
        "    root = path + source,\n",
        "    transform = data_transformation(\n",
        "        resize_dim, \n",
        "        crop_dim, \n",
        "        grayscale, \n",
        "        crop_center))\n",
        "\n",
        "# Target Domain\n",
        "target_ds = ImageFolder(\n",
        "    root = path + target, \n",
        "    transform = data_transformation(\n",
        "        resize_dim, \n",
        "        crop_dim, \n",
        "        grayscale, \n",
        "        crop_center))\n",
        " \n",
        "# Apply normalization\n",
        "if not grayscale:\n",
        "    image_normalization(source_ds)\n",
        "    image_normalization(target_ds)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ya6nhiHiDhkM"
      },
      "source": [
        "# Data Splitting"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "nrDImKxB0G9o"
      },
      "outputs": [],
      "source": [
        "def get_data(dataset: ImageFolder, test_split=0.2, batch_size=30) -> (DataLoader, DataLoader):\n",
        "    \"\"\"\n",
        "    Transformed data are splitted into training (0.8) and validation (0.2) sets and \n",
        "    data loaders with a predefined size are returned. \n",
        "\n",
        "    Args:\n",
        "        test_split (float): Fixed train/test split 80/20. \n",
        "        batch_size (int): Batch size of the loaders, the default is set to 30.\n",
        "    \"\"\"\n",
        "    \n",
        "    train_indices, val_indices = train_test_split(\n",
        "        list(range(len(dataset.targets))),\n",
        "        test_size = test_split,\n",
        "        stratify = dataset.targets, \n",
        "        random_state = 42)\n",
        "    \n",
        "    train_dataset = Subset(dataset, train_indices)\n",
        "    val_dataset = Subset(dataset, val_indices)\n",
        "\n",
        "    train_data_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "    val_data_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
        "    \n",
        "    return train_data_loader, val_data_loader"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "uWXxiULT0G9o"
      },
      "outputs": [],
      "source": [
        "# Source domain \n",
        "source_train_loader, source_val_loader = get_data(source_ds, test_split, batch_size)\n",
        "\n",
        "# Target domain\n",
        "target_train_loader, target_val_loader = get_data(target_ds, test_split, batch_size)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T9uO6jyHEeKs"
      },
      "source": [
        "# UDA Architecture "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f7hvzUo8svFF"
      },
      "source": [
        "As aforementioned, the UDA architecture here presented\n",
        "seeks to address the <i>domain shift</i> problem and strives for accurate predictions on the target domain even in the absence of target labels.\n",
        "\n",
        "Our solution is build around the structure presented by [Zhang](https://arxiv.org/abs/1904.04663) <i>et al.</i> and reported in the figure below. Whereas the symmetric design of source and target task classifiers does not introduce any novelty in the UDA field, the adversarial learning function system results innovative. Thus, SymNet is inspired by the domain discriminator of Generative Adversarial Networks (GAN) training procedure, but it shifts its role onto the training loss function. Hence, it comprises a two-level domain confusion scheme, entailing a domain discrimination loss and a domain confusion loss. Moreover, the category-level confusion loss improves over the domain level by stabilizing the intermediate network features independently from the categories. Both domain discrimination and domain confusion losses are implemented on an additional classifier. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yHJCx7Or3eef"
      },
      "source": [
        "![SymNet](https://drive.google.com/uc?id=1E_bMJdr4kZR_5XpZiBYBXqQ8-P_UU94S)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lS8yTMu10G9r"
      },
      "source": [
        "## Feature Extractor"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HSf0_h16svFG"
      },
      "source": [
        "The first element of the architecture is the feature extractor ($G$) that computes the images' latent representation. In computer vision <code>ResNet</code> is a quite popular choice and, accordingly to the original implementation, [ResNet50](https://pytorch.org/hub/nvidia_deeplearningexamples_resnet50/). It is worth noting that the code allows a certain degree of flexibility by proposing ResNet18 as a less computationally expensive backbone. In both cases, the linear output layer has been modified to allow for <b>Unsupervised Domain Adaptation</b> and, specifically, it is doubled. The results of the two domain-specific classification tasks (i.e., on the source and target) can be represented as the length of each half corresponding to the number of classes ($n=20$ for this specific application).\n",
        "\n",
        "* <b>Dropout layer </b><br>\n",
        "\n",
        "A <b>dropout layer</b> is applied to the latent representations before the linear output layer. This refinement derives from empirical and architectural reasons. After a few attempts, we noticed that the model was mapping several different input values to a small number of output points, and, in extreme cases, it collapsed into a unique latent representation for images belonging to different categories. This behaviour suggested the <b>mode collapse</b> phenomenon, which is typical of adversarial-based architectures ([Goodfellow](https://arxiv.org/abs/1701.00160), 2016). To mitigate this phenomenon, we introduced a dropout layer that, by simply dropping out the output of some randomly chosen neurons, should promote generalization in the network ([Mordido <i>et al.</i>](https://arxiv.org/pdf/1807.11346.pdf), 2020)\n",
        "\n",
        "* <b>Freezing layers </b><br>\n",
        "\n",
        "Given that the ResNet is already pretrained and has useful weights, we introduced two alternatives to freeze the layers and speed up the training. Whereas the first method directly freezes an <i>a-priori</i> number of layers (i.e., <code> n_params - n_params_trained </code>), the second introduces a <b>layer-wise learning rate decay</b> ()[]. The latter decays the learning rate layer by layer (here, group of parameters) instead of the whole model accordingly to the following sigmoid function:\n",
        "\n",
        "$$ \\displaystyle \\eta(x) = \\eta_{max} \\cdot \\sigma\\bigg(\\frac{k}{n-1} \\cdot x - \\frac{k}{2}\\bigg) = \\frac{\\eta_{max}}{1+\\mathcal{e}^{-(\\frac{k}{n-1} \\cdot x - \\frac{k}{2})}} $$\n",
        "\n",
        "Where $n$ is the number of parameters (here, groups of parameters), $\\eta_{max}$ is the maximum possible learning rate assigned, and $k=10$ is a fixed coefficient chosen to guarantee that the outer layers have $\\approx 100\\%$ the maximum learning rate applied. Therefore, teh function applies a higher learning rate to the outer layers and then decreases it for the inner ones. Moreover, the function shows a central symmetry and, independently from $k$, $x=\\frac{n-1}{2}$ implies exactly $\\eta(x)=\\frac{\\eta_{max}}{2}$. <br>\n",
        "\n",
        "* <b>Optimizer </b><br>\n",
        "\n",
        "Whereas the original implementation employs <b>Stochastic Gradient Descent</b> ([<code>SGD</code>](https://pytorch.org/docs/stable/generated/torch.optim.SGD.html)) to update the parameters, our implementation is based on the unpublished <b>Root Mean Square Propagation</b>, namely [<code>RMSProp</code>](https://pytorch.org/docs/stable/generated/torch.optim.RMSprop.html) ([Hinton](https://www.cs.toronto.edu/~tijmen/csc321/slides/lecture_slides_lec6.pdf), 2018). RMSProp belongs to the realm of adaptive learning rate methods and extends the SGD algorithm and Momentum method, representing the [<code>Adam</code>](https://pytorch.org/docs/stable/generated/torch.optim.Adam.html) algorithm's foundation ([Kingma & Ba](https://arxiv.org/pdf/1412.6980.pdf), 2014). Even though Adam algorithm is the default choice of many neural network implementations and the best among the adaptive optimizers in most of the cases ([Ruder](https://ruder.io/optimizing-gradient-descent/index.html#adam), 2016), RMSProp was developed as a stochastic technique for mini-batch learning ([Kingma & Ba]((https://arxiv.org/pdf/1412.6980.pdf), 2014; [Guide to Adam and Rmsprop](https://medium.com/analytics-vidhya/a-complete-guide-to-adam-and-rmsprop-optimizer-75f4502d83be)) and seemed a reasonable choice considering our batches. Unlike the original paper (<code>batch_size=120</code>), we considered batches of 30 images due to the small number of classes. While the reduced Adaptiope dataset maps to $20$ classes, the original included $123$ ([Ringwald & Stiefelhagen](https://paperswithcode.com/dataset/adaptiope), 2021) and [Office-31](https://paperswithcode.com/dataset/office-31) dataset $31$. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "AfuKcRs50G9r"
      },
      "outputs": [],
      "source": [
        "class FeatureExtractor:\n",
        "    \n",
        "    def __init__(self, n_classes: int, model='resnet50', optimizer='rmsprop', n_params_trained=None, lr=0.01, weight_decay=0.0, dropout=0.0, source_only=False):\n",
        "        '''\n",
        "        This class implements the internal model (a.k.a. feature extractor).\n",
        "        For image classification tasks, ResNet is a popular choice.\n",
        "        This class slightly differs from pretrained model due to a modified\n",
        "        output layer, which allows for unsupervised domain adaptation (UDA).\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        n_classes: int\n",
        "            number of classes in the dataset\n",
        "        model: str, optional\n",
        "            pretrained model to import as feature extractor [default='resnet50']\n",
        "        optimizer: str, optional\n",
        "            optimizer to apply on the model [default='rmsprop']\n",
        "        n_params_trained: int, optional\n",
        "            number of parameters (i.e., layers to be trained) [default=None]\n",
        "        lr: float, optional\n",
        "            learning rate [default=0.01]\n",
        "        weight_decay: float, optional\n",
        "            weight decay [default=0.0]\n",
        "        dropout: float, optional\n",
        "            dropout to apply to the latent representation \n",
        "            before linear output is applied [default=0.0]\n",
        "        source_only: boolean, optional\n",
        "            source-only training mode requires output \n",
        "            as long as n_classes [default=False]\n",
        "        '''\n",
        "        self.learning_rate = lr\n",
        "        self.weight_decay = weight_decay\n",
        "        self.dropout = dropout\n",
        "        \n",
        "        # Upload pretrained model \n",
        "        if model.lower() == 'resnet18': \n",
        "            self.model = models.resnet18(pretrained=True)\n",
        "        elif model.lower() == 'resnet50': \n",
        "            self.model = models.resnet50(pretrained=True)\n",
        "        else:\n",
        "            raise ValueError('Unknown model')\n",
        "        \n",
        "        # Modify last layer (classification)\n",
        "        n_feat_in = self.model.fc.in_features\n",
        "        n_feat_out = 2*n_classes if not source_only else n_classes\n",
        "        \n",
        "        self.model.fc = nn.Sequential(\n",
        "            nn.Dropout(p=self.dropout, inplace=True),\n",
        "            nn.Linear(n_feat_in, n_feat_out))\n",
        "        \n",
        "        if cuda.is_available():\n",
        "            self.model = self.model.cuda()\n",
        "\n",
        "        count = 0 \n",
        "        n_params = len(list(self.model.parameters()))\n",
        "        first_param_trained = n_params - n_params_trained if n_params_trained else 0\n",
        "        if n_params_trained is not None:\n",
        "            # Freeze first layers\n",
        "            for param in self.model.parameters():\n",
        "                param.requires_grad = (count >= first_param_trained)\n",
        "                count = count + 1 \n",
        "            params_to_train = filter(lambda p: p.requires_grad, self.model.parameters())\n",
        "        else:\n",
        "            # Layer-wise Learning Rate Decay\n",
        "            i = -1 \n",
        "            params_to_train = []\n",
        "            name_prev_group = None\n",
        "            groups = set([name.split('.')[0] for name, _ in self.model.named_parameters()])\n",
        "            for name, param in self.model.named_parameters():\n",
        "                name_cur_group = name.split('.')[0]\n",
        "                if name_cur_group != name_prev_group or name_prev_group is None:\n",
        "                    i = i + 1\n",
        "                    lr_group = self.lr_annealing(i, len(groups)-1, self.learning_rate)\n",
        "                name_prev_group = name_cur_group\n",
        "                params_to_train.append({'params': param, 'lr': lr_group})\n",
        "            \n",
        "        # Initialize optimizer\n",
        "        if optimizer.lower() == 'rmsprop':\n",
        "            self.optim = RMSprop(\n",
        "                params = params_to_train,\n",
        "                lr = self.learning_rate,\n",
        "                weight_decay = self.weight_decay)\n",
        "        else:\n",
        "            raise ValueError('Unknown optimizer')\n",
        "    \n",
        "    def lr_annealing(self, index: int, n_groups: int, lr: float):\n",
        "        lr_steep = 10\n",
        "        sigmoid = lambda x: 1/(1 + np.exp(-x)) \n",
        "        return lr * sigmoid(lr_steep/n_groups * (index - (n_groups/2))) "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PzHiEzWR0G9p"
      },
      "source": [
        "## Losses"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vAwRudiUsvFG"
      },
      "source": [
        "The SymNet architecture presents an overall training function based on compositionality and introducing a novel adversarial learning approach. It embeds two main modules: the first respondible for the updating the classifiers and the second that applies on the feature extractor ([Zhang <i>et al.</i>](https://arxiv.org/abs/1904.04663), see § 3.3). \n",
        "\n",
        "$$ \\displaystyle\\min_{C^s, C^t, C^{st}} \\large \\mathcal{E}_{task}^{(s)}(G, C^{(s)}) +  \\large\\mathcal{E}_{task}^{(t)}(G, C^{(t)}) + \\large\\mathcal{E}_{domain}^{(st)}(G, C^{(st)})$$    \n",
        "$$\\displaystyle\\min_{G} \\large \\mathcal{F}_{category}^{(st)}(G, C^{(st)}) + \\lambda [\\large \\mathcal{F}_{domain}^{(st)}(G, C^{(st)}) + \\large \\mathcal{M}^{(st)}(G, C^{(st)})]$$\n",
        "\n",
        "The multiple elements (i.e., losses) have been organized in a tree-wise structure to avoid redundancy in the code and provide a user-friendly object-oriented framework.\n",
        "\n",
        "The root is the class <code>_Loss</code>, which inherits the methods from <code>[torch.nn.Module](https://pytorch.org/docs/stable/generated/torch.nn.Module.html)</code>. It is then specified by the children: <code>EntropyMinimizationLoss</code> and <code>SplitLoss</code>. The former is used to implement the <i>Entropy Minimization Principle</i> ([Zhang <i>et al.</i>](https://arxiv.org/abs/1904.04663), § 3.2.1), while the latter specifies a series of submodules and brings together the functionalities of different losses. Specifically, the losses are characterized by two couples of mutually exclusive features: <code>source/target domain</code> and <code>split_first/split_after</code> (see § Cross-Entropy Based Losses for futher details).\n",
        "\n",
        "Therefore, the hierarchy is structured as follows: \n",
        "```\n",
        "Loss\n",
        "│\n",
        "├── EntropyMinimizationLoss\n",
        "│ \n",
        "└── SplitLoss\n",
        "    │   \n",
        "    └── SplitCrossEntropyLoss\n",
        "    |\n",
        "    └── DomainDiscriminationLoss\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "66HGz_c30G9p"
      },
      "outputs": [],
      "source": [
        "class _Loss(nn.Module):\n",
        "    '''\n",
        "    This is a base abstract class representing a generic loss.\n",
        "    A few methods of common use are implemented for the benefit\n",
        "    of the child classes.\n",
        "    \n",
        "    Methods\n",
        "    -------\n",
        "    add_threshold:\n",
        "        Adds threshold to avoid log(0) cases.\n",
        "    to_softmax:\n",
        "        Computes post-softmax tensor on a \n",
        "        given output (features) tensor.\n",
        "    '''\n",
        "    \n",
        "    _THRESHOLD = 1e-20\n",
        "    \n",
        "    def __init__(self):\n",
        "        super(_Loss, self).__init__()\n",
        "        \n",
        "    def forward(self, input: Tensor):\n",
        "        prob = self.to_softmax(input)\n",
        "        loss = self.compute_loss(prob)\n",
        "        if cuda.is_available():\n",
        "            loss = loss.cuda()\n",
        "        return loss\n",
        "        \n",
        "    ### @final\n",
        "    def add_threshold(self, prob: Tensor):\n",
        "        '''\n",
        "        Check whether the probability distribution after the softmax \n",
        "        is equal to 0 in any cell. If this holds, a standard threshold\n",
        "        is added in order to avoid log(0) case. \n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        prob: Tensor\n",
        "            output tensor of the softmax operation\n",
        "\n",
        "        Returns\n",
        "        -------\n",
        "        Tensor\n",
        "            updated tensor (in case the condition above holds)\n",
        "        '''\n",
        "        zeros = (prob == 0)\n",
        "        if torch.any(zeros):\n",
        "            thre_tensor = torch.zeros(zeros.shape)\n",
        "            if cuda.is_available():\n",
        "                thre_tensor = thre_tensor.cuda()\n",
        "            thre_tensor[zeros] = self._THRESHOLD\n",
        "            prob = prob + thre_tensor\n",
        "        if cuda.is_available():\n",
        "            prob = prob.cuda()\n",
        "        return prob\n",
        "    \n",
        "    def to_softmax(self, features: Tensor):\n",
        "        '''\n",
        "        Apply the softmax operation on the features tensor \n",
        "        (i.e., the output of a feature extractor).\n",
        "        \n",
        "        Parameters\n",
        "        ----------\n",
        "        features: Tensor\n",
        "            output of a feature extractor (assuming that dim=1\n",
        "            is as long as the number of classes in your task)\n",
        "\n",
        "        Returns\n",
        "        -------\n",
        "        Tensor\n",
        "            probability distribution with (possible) threshold\n",
        "        '''\n",
        "        prob = F.softmax(features, dim=1)\n",
        "        prob = self.add_threshold(prob)\n",
        "        if cuda.is_available():\n",
        "            prob = prob.cuda()\n",
        "        return prob\n",
        "    \n",
        "    @abstractmethod\n",
        "    def compute_loss(self, prob: Tensor):\n",
        "        pass"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zh0hPnIcsvFG"
      },
      "source": [
        "### Entropy Minimization Principle"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3nwrX9DssvFG"
      },
      "source": [
        "The <b>Entropy Minimization</b> objective is here adopted as regularizer ([Zhang <i>et al.</i>](https://arxiv.org/abs/1904.04663), § 3.2.1) . Specifically, it updates the feature extractor (<i>G</i>) and enhances the discrimination among task categories. This enhancement is achieved by minimizing the entropy of the total (sum) probabiility of a category: \n",
        "\n",
        "$$\\displaystyle\\min_{G}\\mathcal{M}^{(st)}(G, C^{(t)}) = -\\frac{1}{n_t}\\sum_{j=1}^{n_t}\\sum_{k=1}^{K}q_k^{(st)}(x_j^{(t)})log\\bigg(q_k^{(st)}(x_j^{(t)})\\bigg)$$\n",
        "\n",
        "$$where: \\quad \\displaystyle q_k^{(st)}(x_j^{(t)}) = p_k^{(st)}(x_j^{(t)}) + p_{k+K}^{(st)}(x_j^{(t)}) \\quad k\\in{1,...,K}$$\n",
        "\n",
        "It can be argued that this regularization approach seems to enforce a <i>balanced</i> training behaviour between source and target as minimizing the entropy of the total probability (seen as <i>confidence</i>) over the source and target classifiers corresponds to minimizing the <i>surprise</i> over the source and target outcomes for each task category $k$. Thus, if the model tends to be <i>overconfident</i> about a domain (e.g., source with $p_k^{(st)}(\\underline{x}^{(t)}) \\approx 1.0$) and strongly <i>underconfident</i> about the other (e.g., target with $p_{k+K}^{(st)}(\\underline{x}^{(t)}) \\approx 0.0$), then $\\mathcal{M}^{(st)}(G, C^{(t)})$ will return zero at best. However, this value is very far from its minimum: a negative value. Reasonably, the regularizer discourages any unbalanced training behaviour and, in this case, it only updates $G$ to reduce the side effects caused by a large domain shift ([Zhang <i>et al.</i>](https://openaccess.thecvf.com/content_cvpr_2018/papers/Zhang_Importance_Weighted_Adversarial_CVPR_2018_paper.pdf), 2018). "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "EhXN9HF-0G9q"
      },
      "outputs": [],
      "source": [
        "class EntropyMinimizationLoss(_Loss):\n",
        "    '''\n",
        "    This class implements the loss for \n",
        "    the entropy minimization principle\n",
        "    '''\n",
        "    \n",
        "    def __init__(self, n_classes: int):\n",
        "        super(EntropyMinimizationLoss, self).__init__()\n",
        "        self.n_classes = n_classes\n",
        "    \n",
        "    ### @overrides\n",
        "    def compute_loss(self, prob: Tensor):\n",
        "        prob_source = prob[:, :self.n_classes]\n",
        "        prob_target = prob[:, self.n_classes:]\n",
        "        prob_sum = prob_source + prob_target\n",
        "        # applies the formula above \n",
        "        loss = -(prob_sum.log().mul(prob_sum).sum(dim=1).mean())\n",
        "        if cuda.is_available():\n",
        "            loss = loss.cuda()\n",
        "        return loss"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kWjl5wJ5svFH"
      },
      "source": [
        "### Cross-Entropy based losses \n",
        "The losses in the training objective described forehead are implemented starting from the class <code>SplitLoss</code> and are either standard cross-entropy losses or a combination of two losses (i.e., <i>two-way cross-entropy losses</i>). "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "br0WNphcsvFH"
      },
      "source": [
        "#### Standard cross-entropy losses\n",
        "\n",
        "* <b>Loss of source task classifier </b> <br>\n",
        "  \n",
        "Given that two classifiers have been implemented to solve the classification task on the <i>source</i> ($C^{(s)}$) and <i>target</i> ($C^{(t)}$) domains respectively. The <b>task classifier</b> $C^{(s)}$ is trained using the following cross-entropy loss over the <i>labeled</i> source samples:\n",
        "\n",
        "$$ \\displaystyle\\min_{C^{(s)}}\\mathcal{E}_{task}^{(s)}(G, C^{(s)})=-\\frac{1}{n_{s}}\\sum_{i=1}^{n_{s}}log\\bigg(p_{y_{i}^{(s)}}^{(s)}({x}_{i}^{(s)})\\bigg) $$\n",
        "\n",
        "In the formula above, $G$ represents the <b>feature extractor</b>, ${x}_{i}^{(s)}$ is a source sample, and $p_{y_{i}^{(s)}}^{(s)}({x}_{i}^{(s)}) \\in [0,1]^{K}$ the distribution of probability after the <code>[softmax](https://pytorch.org/docs/stable/generated/torch.nn.functional.softmax.html)</code> operation. \n",
        "\n",
        "* <b>Cross-Domain Loss of Target Task Classifier</b> <br>\n",
        "  \n",
        "Since target samples are <i>unlabeled</i>, there exists no direct\n",
        "supervision signals to learn a task classifier $C^{(t)}$. Therefore, [Zhang <i>et al.</i>](https://arxiv.org/abs/1904.04663) leverage the <i>labeled</i> source samples to train $C^{(t)}$ for patterns that can be used on the taregt domain as well. This can be achieved by using the following cross-entropy loss:\n",
        "\n",
        "$$ \\displaystyle\\min_{C^{(t)}}\\mathcal{E}_{task}^{(t)}(G, C^{(t)})=-\\frac{1}{n_{s}}\\sum_{i=1}^{n_{s}}log\\bigg(p_{y_{i}^{(s)}}^{(t)}({x}_{i}^{(s)})\\bigg) $$\n",
        "\n",
        "It is worth noticing that $C^{(t)}$ will be distinguishable from $C^{(s)}$ through the domain discrimination training of the classifier $C^{(st)}$, reported below. \n",
        "\n",
        "Moreover, to reduce code redundancy, the two losses have been merged into the class <code>SplitCrossEntropyLoss</code>. They are distinguishable through a proper parameter choice, meaning <code>split_first=<font color=\"green\">True</font></code> for both and setting <code>source=<font color=\"red\">False</font></code> for the target task classifier or, otherwise, for the source task classifier.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w-d_MZ7bl0b6"
      },
      "source": [
        "#### Two-way cross-entropy losses\n",
        "\n",
        "* <b>Domain Discrimination Loss </b> <br>\n",
        "  \n",
        "The difference between $C^{(t)}$ and $C^{(s)}$ is stressed by the constructed classifier $C^{(st)}$ that leverages the probabilities of classifying an input sample $x$ as the source and target domains:  \n",
        "\n",
        "$$ \\displaystyle\\min_{C^{(st)}}\\mathcal{E}_{domain}^{(st)}(G, C^{(t)})=-\\frac{1}{n_{t}}\\sum_{j=1}^{n_t}log\\bigg(\\sum_{k=1}^{K} p_{k+K}^{(st)}({x}_{j}^{(t)})\\bigg)-\\frac{1}{n_s}\\sum_{i=1}^{n_s}log\\bigg(\\sum_{k=1}^{K}p_{k}^{(st)}({x}_{i}^{(s)})\\bigg) $$\n",
        "\n",
        "* <b>Domain Discrimination Loss</b> <br>\n",
        "\n",
        "Furthermore, a two-level confusion loss is applied and aims at maximising the confusion between the domains. Whereas a first loss is category-level and relies on the source labels, the second loss is domain-level and focuses on the target. Thus, the joint distributions of features and categories across domains tend to align ([Zhang et al.](https://arxiv.org/abs/1904.04663), see  § 3.2). \n",
        "\n",
        "$$ \\displaystyle\\min_{G}\\mathcal{F}_{category}^{(st)}(G, C^{(t)})=-\\frac{1}{2n_{s}}\\sum_{i=1}^{n_s}log(p_{y_i^s+K}^{(st)}({x}_{i}^{(s)}))-\\frac{1}{2n_s}\\sum_{i=1}^{n_s}log(p_{y_i^s}^{(st)}({x}_{i}^{(s)})) $$\n",
        "\n",
        "$$ \\displaystyle\\min_{G}\\mathcal{F}_{domain}^{(st)}(G, C^{(t)})=-\\frac{1}{2n_{t}}\\sum_{j=1}^{n_t}log\\bigg(\\sum_{k=1}^{K} p_{k+K}^{(st)}({x}_{j}^{(t)})\\bigg)-\\frac{1}{2n_t}\\sum_{j=1}^{n_t}log\\bigg(\\sum_{k=1}^{K}p_{k}^{(st)}({x}_{j}^{(t)})\\bigg) $$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BC8UPB2Z0-44"
      },
      "source": [
        "### Label Smoothing \n",
        "\n",
        "After a few attempts, we noticed how the model was overconfident as its predicted probabilities of outcomes did not reflect the accuracy. To mitigate this risk, the <b>label smoothing</b> regularization method has been applied on all the losses based on <code>SplitCrossEntropyLoss</code>. Practically, label smoothing replaces the one-hot encoded label vector $y$ with a mixture of $y$ and the uniform distribution:\n",
        "$$y_{ls} = (1 - \\alpha) \\cdot y + \\frac{\\alpha}{n}$$\n",
        "where $n$ is the number of categories, and $\\alpha$ is the hyperparameter determining the amount of smoothing. Therefore, $\\alpha = 0.0$ retrieves the original $y$ and $\\alpha= 1.0$ returns the uniform distribution [Müller et al., 2019](https://papers.nips.cc/paper/2019/hash/f1748d6b0fd9d439f71450117eba2725-Abstract.html)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "C5zkjQbG0G9q"
      },
      "outputs": [],
      "source": [
        "class SplitLoss(_Loss):\n",
        "    '''\n",
        "    This abstract class represents a generic loss\n",
        "    such that the features array needs to be splitted\n",
        "    either before or after the softmax function is applied\n",
        "    \n",
        "    Methods\n",
        "    -------\n",
        "    split_vector:\n",
        "        Return the proper half of the input array \n",
        "        according to initialization settings\n",
        "    '''\n",
        "    \n",
        "    def __init__(self, n_classes: int, source: bool, split_first: bool, label_smoothing: float):\n",
        "        super(SplitLoss, self).__init__()\n",
        "        self.n_classes = n_classes\n",
        "        self._is_source = source\n",
        "        self._split_first = split_first\n",
        "        self.label_smoothing = label_smoothing\n",
        "    \n",
        "    ### @overrides\n",
        "    def to_softmax(self, features: Tensor):\n",
        "        if self._split_first:\n",
        "            prob = self.split_vector(features)\n",
        "            prob = F.softmax(prob, dim=1)\n",
        "        else:\n",
        "            prob = F.softmax(features, dim=1)\n",
        "            prob = self.split_vector(prob)\n",
        "        prob = self.add_threshold(prob)\n",
        "        if cuda.is_available():\n",
        "            prob = prob.cuda()\n",
        "        return prob\n",
        "    \n",
        "    ### @final\n",
        "    def split_vector(self, prob: Tensor):\n",
        "        return prob[:,:self.n_classes] if self._is_source else prob[:,self.n_classes:]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "aDGpTlic0G9q"
      },
      "outputs": [],
      "source": [
        "class SplitCrossEntropyLoss(SplitLoss):\n",
        "    '''\n",
        "    This class realizes the abstract class SplitLoss\n",
        "    for the case in which a cross-entropy loss should be\n",
        "    applied between the softmax and the ground truth\n",
        "    \n",
        "    Properties\n",
        "    ----------\n",
        "    y_labels:\n",
        "        ground truth is transformed properly \n",
        "        so to ease the accuracy computation\n",
        "    '''\n",
        "    \n",
        "    def _get_y_labels(self) -> Tensor:\n",
        "        return self._y_labels\n",
        "    def _set_y_labels(self, y_labels: list):\n",
        "        # Set the proper device \n",
        "        dev = 'cuda:0' if cuda.is_available() else 'cpu:0'\n",
        "        # Transform the target labels into a long tensor object\n",
        "        y_labels_tns = torch.tensor(y_labels, dtype=torch.long, device=dev)\n",
        "        # One-hot encode the target labels \n",
        "        self._y_labels = F.one_hot(y_labels_tns, num_classes=self.n_classes)\n",
        "        # Cast the one-hot-encoded long tensor object into a float tensor object\n",
        "        # to ensure type compatibility with the probabilities tensor object (i.e., prob)\n",
        "        self._y_labels = self._y_labels.type(torch.float)\n",
        "    y_labels = property(fget=_get_y_labels, fset=_set_y_labels)\n",
        "    \n",
        "    def __init__(self, n_classes: int, source: bool, split_first: bool, label_smoothing: float):\n",
        "        super(SplitCrossEntropyLoss, self).__init__(n_classes, source, split_first, label_smoothing)\n",
        "        # apply label smoothing to mitigate overconfidence phenomenon\n",
        "        self.cross_entropy_loss = CrossEntropyLoss(label_smoothing=self.label_smoothing)\n",
        "        if cuda.is_available():\n",
        "            self.cross_entropy_loss = self.cross_entropy_loss.cuda()\n",
        "    \n",
        "    ### @overrides\n",
        "    def compute_loss(self, prob: Tensor):\n",
        "        loss = self.cross_entropy_loss(prob, self.y_labels)\n",
        "        if cuda.is_available():\n",
        "            loss = loss.cuda()\n",
        "        return loss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "9WJRrEK20G9r"
      },
      "outputs": [],
      "source": [
        "class DomainDiscriminationLoss(SplitLoss):\n",
        "    '''\n",
        "    This class realizes the abstract class SplitLoss\n",
        "    for the case in which a cross-entropy loss should be\n",
        "    applied on the total probability for one of the two\n",
        "    domains (hence, now the input of the cross-entropy is \n",
        "    actually the probability of classifying a sample as \n",
        "    belonging to that domain)\n",
        "    '''\n",
        "    \n",
        "    def __init__(self, n_classes: int, source: bool):\n",
        "        super(DomainDiscriminationLoss, self).__init__(n_classes, source, False, 0.0)\n",
        "        \n",
        "    ### @overrides\n",
        "    def compute_loss(self, prob: Tensor):\n",
        "        loss = -(prob.sum(dim=1).log().mean())\n",
        "        if cuda.is_available():\n",
        "            loss = loss.cuda()\n",
        "        return loss"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "um5fczHysvFI"
      },
      "source": [
        "Accordingly to the definition of the overall model's objective ([Zhang <i>et al.</i>](https://arxiv.org/abs/1904.04663), § 3.3), the class <code>TrainingObjectives</code> recalls the aforementioned losses. \n",
        "\n",
        "Moreover, the $\\lambda$ trade-off parameter is introduced to suppress noisy signals for domain confusion loss and entropy. $\\lambda$ value depends on the number of epochs, as it is iteratively computed through the following formula:\n",
        "\n",
        "$$\\lambda = \\frac{2}{1 + e^{(- \\gamma \\cdot \\frac{ep}{n_{ep}})}} - 1$$\n",
        "\n",
        "where $\\gamma$ is usually set to $10$ (see § 4.1) and $\\frac{ep}{n_{ep}}$ is iteratively updated at each epoch, as it represents the current epoch over the total. Therefore, $\\lambda$ parameter will start from $0$ and gradually increase ($ep = n_{ep} → \\lambda \\approx 1$). \n",
        "Thus, the penalty provided by $\\lambda$ on domain confusion loss and entropy decreases over time, coherently with the fact that the latter might be pretty noisy in the first few epochs, and thus they shouldn't be much relevant on the overall generator loss. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "rSp7_hu20G9r"
      },
      "outputs": [],
      "source": [
        "class TrainingObjectives:\n",
        "    \"\"\"\n",
        "    This class combines the final elements and \n",
        "    structures the final training objective. \n",
        "    \"\"\"\n",
        "    \n",
        "    @staticmethod\n",
        "    def domain_discrimination_loss(src_dom_discrim_loss, tgt_dom_discrim_loss):\n",
        "        return src_dom_discrim_loss + tgt_dom_discrim_loss\n",
        "    \n",
        "    @staticmethod\n",
        "    def category_confusion_loss(src_cat_conf_loss, tgt_cat_conf_loss):\n",
        "        return 0.5 * (src_cat_conf_loss + tgt_cat_conf_loss)\n",
        "    \n",
        "    @staticmethod\n",
        "    def domain_confusion_loss(src_dom_conf_loss, tgt_dom_conf_loss):\n",
        "        return 0.5 * (src_dom_conf_loss + tgt_dom_conf_loss)\n",
        "    \n",
        "    @staticmethod\n",
        "    def overall_classifier_loss(src_task_class_loss, tgt_task_class_loss, domain_discrim_loss):\n",
        "        return src_task_class_loss + tgt_task_class_loss + domain_discrim_loss\n",
        "    \n",
        "    @staticmethod\n",
        "    def overall_generator_loss(cat_conf_loss, dom_conf_loss, tgt_entropy_loss, curr_epoch, tot_epochs):\n",
        "        # trade off parameter as formula above \n",
        "        lambda_trade_off = 2 / (1 + math.exp(-1 * 10 * curr_epoch / tot_epochs)) - 1\n",
        "        return cat_conf_loss + lambda_trade_off * (dom_conf_loss + tgt_entropy_loss)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wfsk22OulkK0"
      },
      "source": [
        "## SymNet"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "xNNN7Mwo0G9s"
      },
      "outputs": [],
      "source": [
        "class SymNet:\n",
        "    \n",
        "    def __init__(self, model: FeatureExtractor, loader: ModelSaver, n_classes: int, epochs: int, patience: int, label_smoothing: float):\n",
        "        \"\"\"Initialize and run the SymNet model \n",
        "        \n",
        "        Parameters\n",
        "        ----------\n",
        "        model: FeatureExtractor\n",
        "            ResNet backbone model \n",
        "        loader: ModelSaver\n",
        "            loader to store the partial and final results of the training\n",
        "        n_classes: int\n",
        "            number of classes (i.e., here 20)\n",
        "        epochs: int\n",
        "            number of epochs the model is trained if not previously stopped\n",
        "        patience: int\n",
        "            number of epochs the model waits before (early) stopping \n",
        "        label_smoothing: float \n",
        "            amount of smoothing when computing the loss, where 0.0 means no smoothing\n",
        "        \n",
        "        Methods\n",
        "        ----------\n",
        "        train_step: \n",
        "            Perform the train step for each batch \n",
        "        train_epoch: \n",
        "            Perform the train for each epoch\n",
        "        val_step: \n",
        "            Perform the validation for each batch \n",
        "        val_epoch: \n",
        "            Perform the validation for each epoch\n",
        "        train_validate: \n",
        "            Iterate the process of train/validation for n epochs (or till early-stopping)\n",
        "        overall_losses: \n",
        "            Structure the different losses in a readable format\n",
        "        \"\"\"\n",
        "        self.curr_epoch = 0\n",
        "        self.tot_epochs = epochs\n",
        "        self.n_classes = n_classes\n",
        "        self.model = model\n",
        "        self.loader = loader\n",
        "        self.patience = patience \n",
        "        self.label_smoothing = label_smoothing\n",
        "        \n",
        "        # Losses \n",
        "        self.src_task_class_loss = SplitCrossEntropyLoss(n_classes=n_classes, source=True, split_first=True, label_smoothing=label_smoothing)\n",
        "        self.tgt_task_class_loss = SplitCrossEntropyLoss(n_classes=n_classes, source=False, split_first=True, label_smoothing=label_smoothing)\n",
        "        # Domain discrimination losses\n",
        "        self.src_dom_discrim_loss = DomainDiscriminationLoss(n_classes=n_classes, source=True)\n",
        "        self.tgt_dom_discrim_loss = DomainDiscriminationLoss(n_classes=n_classes, source=False)\n",
        "        # Category-level confusion losses\n",
        "        self.src_cat_conf_loss = SplitCrossEntropyLoss(n_classes=n_classes, source=True, split_first=False, label_smoothing=label_smoothing)\n",
        "        self.tgt_cat_conf_loss = SplitCrossEntropyLoss(n_classes=n_classes, source=False, split_first=False, label_smoothing=label_smoothing)\n",
        "        # Domain-level confusion losses\n",
        "        self.src_dom_conf_loss = DomainDiscriminationLoss(n_classes=n_classes, source=True)\n",
        "        self.tgt_dom_conf_loss = DomainDiscriminationLoss(n_classes=n_classes, source=False)\n",
        "        # Entropy minimization loss\n",
        "        self.tgt_entropy_loss = EntropyMinimizationLoss(n_classes=n_classes)   \n",
        "\n",
        "        if cuda.is_available():\n",
        "            # Task classifier losses\n",
        "            self.src_task_class_loss = self.src_task_class_loss.cuda()\n",
        "            self.tgt_task_class_loss = self.tgt_task_class_loss.cuda()\n",
        "            # Domain discrimination losses\n",
        "            self.src_dom_discrim_loss = self.src_dom_discrim_loss.cuda()\n",
        "            self.tgt_dom_discrim_loss = self.tgt_dom_discrim_loss.cuda()\n",
        "            # Category-level confusion losses\n",
        "            self.src_cat_conf_loss = self.src_cat_conf_loss.cuda()\n",
        "            self.tgt_cat_conf_loss = self.tgt_cat_conf_loss.cuda()\n",
        "            # Domain-level confusion losses\n",
        "            self.src_dom_conf_loss = self.src_dom_conf_loss.cuda()\n",
        "            self.tgt_dom_conf_loss = self.tgt_dom_conf_loss.cuda()\n",
        "            # Entropy minimization loss\n",
        "            self.tgt_entropy_loss = self.tgt_entropy_loss.cuda()    \n",
        "\n",
        "\n",
        "## TRAINING ##\n",
        "    def train_step(self, X_source: Tensor, y_source: Tensor, X_target: Tensor):\n",
        "        # Tell model go training mode\n",
        "        self.model.model.train()\n",
        "        # Compute features for both inputs\n",
        "        y_source_pred = self.model.model(X_source)\n",
        "        y_target_pred = self.model.model(X_target)\n",
        "        # Compute overall training objective losses\n",
        "        classifier_loss, generator_loss = self.overall_losses(\n",
        "            y_source_pred, \n",
        "            y_target_pred, \n",
        "            y_source)\n",
        "        # Compute gradients w.r.t. classifier loss\n",
        "        self.model.optim.zero_grad()\n",
        "        classifier_loss.backward(retain_graph=True)\n",
        "        grad_classifier_tmp = []\n",
        "        for p in self.model.model.parameters():\n",
        "            if p.grad is not None:\n",
        "                grad_classifier_tmp.append(p.grad.data.clone())\n",
        "        # Compute gradients w.r.t. generator loss\n",
        "        self.model.optim.zero_grad()\n",
        "        generator_loss.backward()\n",
        "        grad_generator_tmp = []\n",
        "        for p in self.model.model.parameters():\n",
        "            if p.grad is not None:\n",
        "                grad_generator_tmp.append(p.grad.data.clone())\n",
        "        # Update gradient data for each parameter \n",
        "        count = 0 \n",
        "        appended = 0 \n",
        "        n_classification_params = 2 \n",
        "        n_params = len(list(self.model.model.parameters()))\n",
        "        for p in self.model.model.parameters():\n",
        "            if p.grad is not None:\n",
        "                grad_tmp = p.grad.data.clone()\n",
        "                grad_tmp.zero_() \n",
        "                if count < (n_params - n_classification_params): \n",
        "                    grad_tmp = grad_tmp + grad_generator_tmp[appended]\n",
        "                else: \n",
        "                    grad_tmp = grad_tmp + grad_classifier_tmp[appended]\n",
        "                appended = appended + 1 \n",
        "                p.grad.data = grad_tmp\n",
        "            count = count + 1\n",
        "        # Perform optimizer step    \n",
        "        self.model.optim.step()\n",
        "        \n",
        "        # Calculate accuracies\n",
        "        # Ground truth \n",
        "        y_source_true = y_source.clone().tolist()\n",
        "        # Predicted labels\n",
        "        y_source_pred = y_source_pred.clone()\n",
        "        \n",
        "        # Predictions on the first half (i.e., source)\n",
        "        y_source_pred1 = torch.argmax(y_source_pred[:,:self.n_classes], dim=1).tolist()\n",
        "        acc_on_source_half = accuracy_score(y_source_true, y_source_pred1)\n",
        "        \n",
        "        # Predictions on the second half (i.e., target)\n",
        "        y_source_pred2 = torch.argmax(y_source_pred[:,self.n_classes:], dim=1).tolist()\n",
        "        acc_on_target_half = accuracy_score(y_source_true, y_source_pred2)\n",
        "\n",
        "        # Return losses and accuracies\n",
        "        return classifier_loss.item(), generator_loss.item(), acc_on_source_half, acc_on_target_half\n",
        "\n",
        "    def train_epoch(self, source_train: DataLoader, target_train: DataLoader):\n",
        "        end_of_epoch = False\n",
        "        source_batch_loader = enumerate(source_train)\n",
        "        target_batch_loader = enumerate(target_train)\n",
        "        \n",
        "        # Store partial results of the training\n",
        "        gen_losses = []\n",
        "        cl_losses = []\n",
        "        accuracies_src = []\n",
        "        accuracies_tgt = []\n",
        "        \n",
        "        # Train current epoch\n",
        "        while not end_of_epoch:\n",
        "            try:\n",
        "                # Get next batch for both source and target\n",
        "                (X_source, y_source) = next(source_batch_loader)[1]\n",
        "                (X_target, _) = next(target_batch_loader)[1]\n",
        "                if cuda.is_available():\n",
        "                    X_source = X_source.cuda()\n",
        "                    y_source = y_source.cuda()\n",
        "                    X_target = X_target.cuda()\n",
        "                # Apply training step\n",
        "                cl_loss, gen_loss, acc_src, acc_tgt = self.train_step(X_source, y_source, X_target)\n",
        "                # Append losses and accuracies\n",
        "                cl_losses.append(cl_loss)\n",
        "                gen_losses.append(gen_loss)\n",
        "                accuracies_src.append(acc_src)\n",
        "                accuracies_tgt.append(acc_tgt)\n",
        "            except StopIteration: \n",
        "                end_of_epoch = True\n",
        "        # Return average training losses and accuracies for this epoch\n",
        "        return mean(cl_losses), mean(gen_losses), mean(accuracies_src), mean(accuracies_tgt)\n",
        "\n",
        "\n",
        "## VALIDATION ## \n",
        "    def val_step(self, X_target: Tensor, y_target: Tensor) -> (float, float):\n",
        "        # Switch to validation mode\n",
        "        self.model.model.eval()\n",
        "        # Get outputs for target domain\n",
        "        with torch.no_grad():\n",
        "            y_target_pred = self.model.model(X_target)\n",
        "        \n",
        "        # Calculate accuracy\n",
        "        # Ground truth \n",
        "        y_target_true = y_target.clone().tolist()\n",
        "        \n",
        "        # Predicted labels \n",
        "        y_target_pred = y_target_pred.clone()\n",
        "        \n",
        "        # Prediction on the first half \n",
        "        y_target_pred1 = torch.argmax(y_target_pred[:,:self.n_classes], dim=1).tolist()\n",
        "        acc_on_source_half = accuracy_score(y_target_true, y_target_pred1)\n",
        "        \n",
        "        # Prediction on the second half \n",
        "        y_target_pred2 = torch.argmax(y_target_pred[:,self.n_classes:], dim=1).tolist()\n",
        "        acc_on_target_half = accuracy_score(y_target_true, y_target_pred2)\n",
        "\n",
        "        # Return accuracies\n",
        "        return acc_on_source_half, acc_on_target_half\n",
        "    \n",
        "    def val_epoch(self, target_val: DataLoader) -> (float, float):\n",
        "        end_of_epoch = False\n",
        "        target_batch_loader = enumerate(target_val)\n",
        "        \n",
        "        # Store the partial results of the validation\n",
        "        accuracies_src = []\n",
        "        accuracies_tgt = []\n",
        "        \n",
        "        # Validate current epoch\n",
        "        while not end_of_epoch:\n",
        "            try:\n",
        "                # Get next batch for both source and target\n",
        "                (X_target, y_target) = next(target_batch_loader)[1]\n",
        "                if cuda.is_available():\n",
        "                    X_target = X_target.cuda()\n",
        "                    y_target = y_target.cuda()\n",
        "                # Apply validation step\n",
        "                acc_src, acc_tgt = self.val_step(X_target, y_target)\n",
        "                # Append accuracies\n",
        "                accuracies_src.append(acc_src)\n",
        "                accuracies_tgt.append(acc_tgt)\n",
        "            except StopIteration: \n",
        "                end_of_epoch = True\n",
        "        # Return average validation accuracies \n",
        "        return mean(accuracies_src), mean(accuracies_tgt)\n",
        "    \n",
        "    def train_validate(self, source_train: DataLoader, target_train: DataLoader, target_val: DataLoader) -> float:\n",
        "        \n",
        "        # Keep track of the results for train and validation per epoch\n",
        "        tr_cl_losses = []\n",
        "        tr_gen_losses = []\n",
        "        tr_src_accs = []\n",
        "        tr_tgt_accs = []\n",
        "        val_src_accs = []\n",
        "        val_tgt_accs = []\n",
        "        best_acc_tgt = 0.0\n",
        "        patience = self.patience\n",
        "        epochs_iter = tqdm(\n",
        "            range(self.tot_epochs), \n",
        "            unit = \"epoch\",\n",
        "            desc = \"TRAINING\")\n",
        "        \n",
        "        # Train and validation step for each epoch\n",
        "        for epoch in epochs_iter:\n",
        "            self.curr_epoch = epoch\n",
        "            # Train current epoch\n",
        "            cl_loss, gen_loss, tr_acc_src, tr_acc_tgt = self.train_epoch(source_train, target_train)\n",
        "            # Store training results\n",
        "            tr_cl_losses.append(cl_loss)\n",
        "            tr_gen_losses.append(gen_loss)\n",
        "            tr_src_accs.append(tr_acc_src)\n",
        "            tr_tgt_accs.append(tr_acc_tgt)\n",
        "            # Show training results\n",
        "            epochs_iter.set_postfix({\n",
        "                \"tr_cl_loss\": round(cl_loss, 3), \n",
        "                \"tr_gen_loss\": round(gen_loss, 3),\n",
        "                \"tr_acc_on_src\": round(tr_acc_src, 3),\n",
        "                \"tr_acc_on_tgt\": round(tr_acc_tgt, 3)\n",
        "            })\n",
        "            \n",
        "            # Validate current epoch\n",
        "            val_acc_src, val_acc_tgt = self.val_epoch(target_val)\n",
        "            # Store validation results\n",
        "            val_src_accs.append(val_acc_src)\n",
        "            val_tgt_accs.append(val_acc_tgt)\n",
        "            # Show validation results\n",
        "            epochs_iter.set_postfix({\n",
        "                \"tr_cl_loss\": round(cl_loss, 3), \n",
        "                \"tr_gen_loss\": round(gen_loss, 3),\n",
        "                \"tr_acc_on_src\": round(tr_acc_src, 3),\n",
        "                \"tr_acc_on_tgt\": round(tr_acc_tgt, 3),\n",
        "                \"val_acc_on_src\": round(val_acc_src, 3),\n",
        "                \"val_acc_on_tgt\": round(val_acc_tgt, 3)\n",
        "            })\n",
        "            # Manage patience for early-stopping\n",
        "            if epoch > self.patience and val_acc_tgt < max(val_tgt_accs[-self.patience:]):\n",
        "                # Decrease current patience\n",
        "                patience = patience - 1\n",
        "                print(f'\\n--- PATIENCE={patience} ---') \n",
        "                if patience == 0:\n",
        "                    print('\\n--- EARLY STOPPING ---')\n",
        "                    break # Interrupt iteration\n",
        "            else:\n",
        "                # Reset current patience\n",
        "                patience = self.patience\n",
        "                # Evaluate model performance and save the best model \n",
        "                if val_acc_tgt > best_acc_tgt:\n",
        "                    best_acc_tgt = val_acc_tgt\n",
        "                    self.loader.model_save(\n",
        "                        model = self.model.model,\n",
        "                        optimizer = self.model.optim)\n",
        "                    print('\\n--- SAVED NEW BEST MODEL ---')\n",
        "        # Save hyperparameters  \n",
        "        self.loader.save_hyperparam()\n",
        "        # Save final results\n",
        "        self.loader.save_results({\n",
        "            'tr_cl_losses': tr_cl_losses,\n",
        "            'tr_gen_losses': tr_gen_losses,\n",
        "            'tr_src_accs': tr_src_accs,\n",
        "            'tr_tgt_accs': tr_tgt_accs,\n",
        "            'val_src_accs': val_src_accs,\n",
        "            'val_tgt_accs': val_tgt_accs\n",
        "        })\n",
        "\n",
        "        # Return best avg accuracy on target  \n",
        "        return max(val_tgt_accs)\n",
        "\n",
        "    def overall_losses(self, y_source_pred: Tensor, y_target_pred: Tensor, y_source_true: Tensor) -> (Tensor, Tensor):\n",
        "        # Source task classifier loss\n",
        "        self.src_task_class_loss.y_labels = y_source_true\n",
        "        _src_task_class_loss = self.src_task_class_loss(y_source_pred)\n",
        "        # (Cross-domain) Target task classifier loss\n",
        "        self.tgt_task_class_loss.y_labels = y_source_true\n",
        "        _tgt_task_class_loss = self.tgt_task_class_loss(y_source_pred)\n",
        "        # Domain discrimination loss\n",
        "        _src_dom_discrim_loss = self.src_dom_discrim_loss(y_source_pred)\n",
        "        _tgt_dom_discrim_loss = self.tgt_dom_discrim_loss(y_target_pred)\n",
        "        _domain_discrim_loss = TrainingObjectives.domain_discrimination_loss(\n",
        "            _src_dom_discrim_loss, \n",
        "            _tgt_dom_discrim_loss)\n",
        "        # Category-level confusion loss\n",
        "        self.src_cat_conf_loss.y_labels = y_source_true\n",
        "        self.tgt_cat_conf_loss.y_labels = y_source_true\n",
        "        _src_cat_conf_loss = self.src_cat_conf_loss(y_source_pred)\n",
        "        _tgt_cat_conf_loss = self.tgt_cat_conf_loss(y_source_pred)\n",
        "        _category_conf_loss = TrainingObjectives.category_confusion_loss(\n",
        "            _src_cat_conf_loss, \n",
        "            _tgt_cat_conf_loss)\n",
        "        # Domain-level confusion loss\n",
        "        _src_dom_conf_loss = self.src_cat_conf_loss(y_target_pred)\n",
        "        _tgt_dom_conf_loss = self.tgt_cat_conf_loss(y_target_pred)\n",
        "        _domain_conf_loss = TrainingObjectives.domain_confusion_loss(\n",
        "            _src_dom_conf_loss, \n",
        "            _tgt_dom_conf_loss)\n",
        "        # Entropy minimization loss\n",
        "        _tgt_entropy_loss = self.tgt_entropy_loss(y_target_pred)\n",
        "        # Overall classifier loss\n",
        "        _overall_classifier_loss = TrainingObjectives.overall_classifier_loss(\n",
        "            _src_task_class_loss, \n",
        "            _tgt_task_class_loss, \n",
        "            _domain_discrim_loss)\n",
        "        # Overall feature extractor loss\n",
        "        _overall_generator_loss = TrainingObjectives.overall_generator_loss(\n",
        "            _category_conf_loss, \n",
        "            _domain_conf_loss, \n",
        "            _tgt_entropy_loss, \n",
        "            self.curr_epoch, \n",
        "            self.tot_epochs)\n",
        "        # Return obtained overall losses\n",
        "        return _overall_classifier_loss, _overall_generator_loss"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Jwa_O2xT0G9u"
      },
      "source": [
        "# Source-Only Architecture"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZtZKQbfAgV6Z"
      },
      "source": [
        "Along with external baseline results, an internal criterion is required to evaluate the goodness of the UDA component. Therefore, a source-only architecture has been deployed. It follows a similar architecture to the SymNet, employing the Feature Extractor (i.e., [<code>ResNet</code>](https://pytorch.org/hub/pytorch_vision_resnet/)), but without the adversarial training objective. Thus, the model is trained simpoly usinga [<code>nn.CrossEntropyLoss </code>](https://pytorch.org/docs/stable/generated/torch.nn.CrossEntropyLoss.html)(with <code>label_smoothing=0.0</code>). \n",
        "\n",
        "In this case, the model is supervisedly trained on the <i>source domain </i> and then directly evaluated on the  <i>target domain</i>. Therefore, the source-only scenario accuracy ($acc_{so}$) is expected to be lower than the accuracy of the UDA architecture ($acc_{uda}$). To validate this assumption and assess the goodness of the UDA architecture, the <i>gain</i> measure can be computed as follows:\n",
        "$$G = acc_{uda} − acc_{so}$$"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "id": "s7bKkf9g0G9u"
      },
      "outputs": [],
      "source": [
        "class SourceOnly:\n",
        "    \n",
        "    def __init__(self, model: FeatureExtractor, so_loader: ModelSaver, n_classes: int, epochs: int, patience: int):\n",
        "        \"\"\"Initialize the Source-Only model \n",
        "        \n",
        "        Parameters\n",
        "        ----------\n",
        "        model: FeatureExtractor\n",
        "            ResNet backbone model (with source_only=True)\n",
        "        loader: ModelSaver\n",
        "            loader to store the final results (with source_only=True)\n",
        "        n_classes: int\n",
        "            number of classes (i.e., here 20)\n",
        "        epochs: int\n",
        "            number of epochs the model is trained if not previously stopped\n",
        "        patience: int\n",
        "            number of epochs the model waits before (early) stopping \n",
        "        \n",
        "        Methods\n",
        "        ----------\n",
        "        train_step: \n",
        "            Perform the train step for each batch \n",
        "        train_epoch: \n",
        "            Perform the train for each epoch\n",
        "        val_step: \n",
        "            Perform the validation for each batch \n",
        "        val_epoch: \n",
        "            Perform the validation for each epoch\n",
        "        train_validate: \n",
        "            Iterate the process of train/validation for n epochs (or till early-stopping)\n",
        "        \"\"\"\n",
        "\n",
        "        self.patience = patience\n",
        "        self.tot_epochs = epochs\n",
        "        self.n_classes = n_classes\n",
        "        self.so_loader = so_loader\n",
        "        # same backbone of the UDA architecture but only n_classes as output  \n",
        "        self.model = model \n",
        "        # chosen loss \n",
        "        self.loss = CrossEntropyLoss()\n",
        "        if cuda.is_available(): \n",
        "            self.loss = self.loss.cuda()\n",
        "    \n",
        "    ## TRAINING ##\n",
        "    def train_step(self, X_source: Tensor, y_source: Tensor) -> float:\n",
        "        # Tell model go training mode\n",
        "        self.model.model.train()\n",
        "        # Compute features for both inputs\n",
        "        y_source_pred = self.model.model(X_source)\n",
        "        # Compute overall training objective losses\n",
        "        y_src = y_source.clone()\n",
        "        general_loss = self.loss(y_source_pred, y_src)\n",
        "        # Compute gradients w.r.t. classifier loss\n",
        "        self.model.optim.zero_grad()\n",
        "        general_loss.backward()\n",
        "        # Perform optimizer step    \n",
        "        self.model.optim.step()\n",
        "        y_true = y_source.clone().tolist()\n",
        "        y_pred = y_source_pred.clone()\n",
        "        y_pred = torch.argmax(y_pred, dim=1).tolist()\n",
        "        # Compute and return accuracy \n",
        "        return accuracy_score(y_true, y_pred)\n",
        "    \n",
        "    def train_epoch(self, source_train: DataLoader) -> float:\n",
        "        end_of_epoch = False\n",
        "        source_batch_loader = enumerate(source_train)\n",
        "        acc = []\n",
        "        # Train for current epoch\n",
        "        while not end_of_epoch:\n",
        "            try:\n",
        "                # Get next batch for both source and target\n",
        "                (X_source, y_source) = next(source_batch_loader)[1]\n",
        "                if cuda.is_available():\n",
        "                    X_source = X_source.cuda()\n",
        "                    y_source = y_source.cuda()\n",
        "                accuracy = self.train_step(X_source, y_source)\n",
        "                acc.append(accuracy)\n",
        "            except StopIteration: \n",
        "                end_of_epoch = True\n",
        "         # Return average training accuracy for this epoch\n",
        "        return mean(acc)\n",
        "    \n",
        "    \n",
        "    ## VALIDATION ##\n",
        "    def val_step(self, X_target: Tensor, y_target: Tensor) -> float:\n",
        "        self.model.model.eval()\n",
        "        with torch.no_grad():\n",
        "            y_target_pred = self.model.model(X_target)\n",
        "        # Ground Truth\n",
        "        y_target_true = y_target.clone().tolist()\n",
        "        \n",
        "        # Predictions \n",
        "        y_target_pred = y_target_pred.clone()\n",
        "        y_target_pred = torch.argmax(y_target_pred, dim=1).tolist()\n",
        "        # Calculate accuracy and return it \n",
        "        return accuracy_score(y_target_true, y_target_pred)\n",
        "    \n",
        "    def val_epoch(self, target_val: DataLoader) -> float:\n",
        "        end_of_epoch = False\n",
        "        target_batch_loader = enumerate(target_val)\n",
        "        accuracies = []\n",
        "        # Validate current epoch\n",
        "        while not end_of_epoch:\n",
        "            try:\n",
        "                # Get next batch for target\n",
        "                (X_target, y_target) = next(target_batch_loader)[1]\n",
        "                if cuda.is_available():\n",
        "                    X_target = X_target.cuda()\n",
        "                    y_target = y_target.cuda()\n",
        "                # Apply validation step\n",
        "                acc = self.val_step(X_target, y_target)\n",
        "                # Append accuracy\n",
        "                accuracies.append(acc)\n",
        "            except StopIteration: \n",
        "                end_of_epoch = True\n",
        "        # Return average validation accuracy for this epoch\n",
        "        return mean(accuracies)\n",
        "    \n",
        "\n",
        "    def train_validate(self, source_train: DataLoader, target_val: DataLoader) -> float:\n",
        "        \n",
        "        # Keep track of the validation accuracy on the target\n",
        "        val_accuracies = []\n",
        "        epochs_iter = tqdm(\n",
        "            range(self.tot_epochs), \n",
        "            unit = \"epoch\",\n",
        "            desc = \"SOURCE-ONLY TRAINING\")\n",
        "        for e in epochs_iter:\n",
        "            # Train current epoch\n",
        "            self.train_epoch(source_train)\n",
        "            # Validate current epoch\n",
        "            acc_val = self.val_epoch(target_val)\n",
        "            # Store validation result \n",
        "            val_accuracies.append(acc_val)\n",
        "            # Show validation results\n",
        "            epochs_iter.set_postfix({\n",
        "                \"val_accuracy_domain_shift\": round(acc_val, 3)\n",
        "            })\n",
        "\n",
        "            if e > self.patience and acc_val < max(val_accuracies[-self.patience:]):\n",
        "                # Decrease current patience\n",
        "                patience = patience - 1\n",
        "                print(f'\\n--- PATIENCE={patience} ---') \n",
        "                if patience == 0:\n",
        "                    print('\\n--- EARLY STOPPING ---') \n",
        "                    break # Interrupt iteration\n",
        "            else: \n",
        "                patience = self.patience\n",
        "        \n",
        "        self.so_loader.save_results({\n",
        "            'val_accuracies': val_accuracies\n",
        "            })\n",
        "        # Return best accuracy on target  \n",
        "        return max(val_accuracies)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qvrTPgYTbGh2"
      },
      "source": [
        "# Training "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qqcLMwbhsvFJ"
      },
      "source": [
        "Both directions $P \\rightarrow RW$ and $RW \\rightarrow P$ will be perfomed aiming at assessing higher than the external baseline: \n",
        "<center>\n",
        "\n",
        "| Version| $P \\rightarrow R$ |  $R \\rightarrow P$ | \n",
        "|  :----: | :-----------: |:----------: | \n",
        "| Source-only | $76 \\%$ |$90 \\%$|\n",
        "| SymNet | $80 \\%$ | $93 \\%$| \n",
        "\n",
        "</center>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5FAwH4nZsvFJ"
      },
      "source": [
        "## Direction R$→$P"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 140,
          "referenced_widgets": [
            "1b1c0854f78146e1bd7c78bba1740d3a",
            "1916883c18e548cc892072e9e9272dc7",
            "2d1b1519b84a45d9a0e55482772f2aee",
            "0ba30ecf34244f4cb3aeba6b618a66ee",
            "149df33c2c8c4807a74aa05cde2f3b03",
            "d2f00ffe7e7d4bfeb82c1e66a2414410",
            "c5087c481bd64697a0da3a8acbfc4fff",
            "198b6d8958e84679affe6dbb699f2706",
            "c200b2818ea942e2bd05fa18ec22ac23",
            "d97bcf74d4104b53928dbb646f350e41",
            "2e6603df4a784115aacca03e47cc43b9"
          ]
        },
        "id": "Lpm0K41_0G9t",
        "outputId": "84396a16-9d9a-4e17-866d-c3c813c87689"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Downloading: \"https://download.pytorch.org/models/resnet50-0676ba61.pth\" to /root/.cache/torch/hub/checkpoints/resnet50-0676ba61.pth\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "  0%|          | 0.00/97.8M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "1b1c0854f78146e1bd7c78bba1740d3a"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "TRAINING:   0%|          | 1/250 [01:51<7:43:39, 111.72s/epoch, tr_cl_loss=29.7, tr_gen_loss=3.01, tr_acc_on_src=0.046, tr_acc_on_tgt=0.048, val_acc_on_src=0.052, val_acc_on_tgt=0.057]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- SAVED NEW BEST MODEL ---\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "TRAINING:  39%|███▉      | 97/250 [2:47:09<4:21:56, 102.72s/epoch, tr_cl_loss=17.1, tr_gen_loss=5.9, tr_acc_on_src=0.052, tr_acc_on_tgt=0.052, val_acc_on_src=0.052, val_acc_on_tgt=0.057]"
          ]
        }
      ],
      "source": [
        "loader = ModelSaver('/content')\n",
        "\n",
        "uda_generator = FeatureExtractor(n_classes, model=model, optimizer=optim, \n",
        "                                n_params_trained=n_params_trained, \n",
        "                                lr=lr, weight_decay=weight_decay, \n",
        "                                dropout=dropout, source_only=False)\n",
        "\n",
        "symnet = SymNet(uda_generator, loader, n_classes, n_epochs, patience, label_smoothing)\n",
        "\n",
        "# Train on both source and target, validate only on target \n",
        "acc_uda_r2p = symnet.train_validate(source_train_loader, target_train_loader, target_val_loader)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "u4P3VCrH0G9u"
      },
      "outputs": [],
      "source": [
        "so_loader = ModelSaver('/content', source_only = True)\n",
        "\n",
        "so_generator = FeatureExtractor(n_classes, model=model, optimizer=optim, \n",
        "                                n_params_trained=n_params_trained, \n",
        "                                lr=lr, weight_decay=weight_decay, \n",
        "                                dropout=0.0, source_only=True)\n",
        "\n",
        "source_only = SourceOnly(so_generator, so_loader, n_classes, n_epochs, patience)\n",
        "\n",
        "# Train only on source, validate only on target \n",
        "acc_so_r2p = source_only.train_validate(source_train_loader, target_val_loader)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zeibzHYibGh7"
      },
      "outputs": [],
      "source": [
        "# Evaluate the performance gain between the two architectures \n",
        "overall_gain_rel_r2p = lambda acc_uda, acc_so: (abs(acc_uda - acc_so)/acc_so)*100\n",
        "overall_gain_abs_r2p = lambda acc_uda, acc_so: (acc_uda - acc_so)\n",
        "\n",
        "print(f'Relative gain = {round(overall_gain_rel_r2p(acc_uda_r2p, acc_so_r2p), 2)} %')\n",
        "print(f'Absolute gain = {round(overall_gain_abs_r2p(acc_uda_r2p, acc_so_r2p), 2)}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zo6Zj95-svFK"
      },
      "source": [
        "## Direction P$→$R"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "l0EeYQPLbZB8"
      },
      "outputs": [],
      "source": [
        "tmp_source = source\n",
        "source = target \n",
        "target = tmp_source \n",
        "\n",
        "tmp_train, tmp_val = source_train_loader, source_val_loader \n",
        "# Source domain \n",
        "source_train_loader, source_val_loader = target_train_loader, target_val_loader \n",
        "\n",
        "# Target domain\n",
        "target_train_loader, target_val_loader = tmp_train, tmp_val"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "001rkr8DsvFK"
      },
      "outputs": [],
      "source": [
        "loader = ModelSaver('/content')\n",
        "\n",
        "uda_generator = FeatureExtractor(n_classes, model=model, optimizer=optim, \n",
        "                                n_params_trained=n_params_trained, \n",
        "                                lr=lr, weight_decay=weight_decay, \n",
        "                                dropout=dropout, source_only=False)\n",
        "\n",
        "symnet = SymNet(uda_generator, loader, n_classes, n_epochs, patience, label_smoothing)\n",
        "\n",
        "# Train on both source and target, validate only on target \n",
        "acc_uda_p2r = symnet.train_validate(source_train_loader, target_train_loader, target_val_loader)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zvx3DgedsvFK"
      },
      "outputs": [],
      "source": [
        "so_loader = ModelSaver('/content', source_only = True)\n",
        "\n",
        "so_generator = FeatureExtractor(n_classes, model=model, optimizer=optim, \n",
        "                                n_params_trained=n_params_trained, \n",
        "                                lr=lr, weight_decay=weight_decay, \n",
        "                                source_only=True)\n",
        "\n",
        "source_only = SourceOnly(so_generator, so_loader, n_classes, n_epochs, patience)\n",
        "\n",
        "# Train only on source, validate only on target \n",
        "acc_so_p2r = source_only.train_validate(source_train_loader, target_val_loader)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dmlW3Z5RsvFK"
      },
      "outputs": [],
      "source": [
        "# Evaluate the performance gain between the two architectures \n",
        "overall_gain_rel_p2r = lambda acc_uda, acc_so: (abs(acc_uda - acc_so)/acc_so)*100\n",
        "overall_gain_abs_p2r = lambda acc_uda, acc_so: (acc_uda - acc_so)\n",
        "\n",
        "print(f'Relative gain = {round(overall_gain_rel_p2r(acc_uda_p2r, acc_so_p2r), 2)} %')\n",
        "print(f'Absolute gain = {round(overall_gain_abs_p2r(acc_uda_p2r, acc_so_p2r), 2)}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0VN5V1hhsvFK"
      },
      "source": [
        "# Results"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3feORWgK65YF"
      },
      "source": [
        "<center>\n",
        "\n",
        "| Version| $P \\rightarrow R$ |  $R \\rightarrow P$ | \n",
        "|  :----: | :-----------: |:----------: | \n",
        "| Source-only | $82.4 \\%$ |$94.7 \\%$|\n",
        "| SymNet | $93.3 \\%$ | $97.4 \\%$| \n",
        "\n",
        "</center>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1qEeEYiS69aK"
      },
      "source": [
        "The top-notch results achieved by the UDA full architecture are displayed in the Table above. They confirm the effectiveness of the architecture in solving the unsupervised domain adaptation task, as the validation accuracy values underline the goodness in aligning feature and category joint distributions across domains. Moreover, the performance on difficult adaptation tasks (i.e., <code>P</code> $\\rightarrow$ <code>RW</code>) greatly differ from the external baseline one. Similarly to Adaptiope's original paper, the outcomes emphasise a difference in the magnitude of the gain measures. This tendency of having lower accuracy scores due to a strong domain gap was already known and appeared independently from the backbone architecture ([Ringwald & Stiefelhagen](https://paperswithcode.com/dataset/adaptiope), see § 5.2). \n",
        "\n",
        "It is worth noticing that these results are the outcome of a fine-tuning process that interested most of the hyperparameters. For a fair comparison, the different trials are displayed below and accessible in the following [folder](https://drive.google.com/drive/folders/1LKLpTckmcrQ6w7mGFp55r1JWSv_5A3d1?usp=sharing). "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_cEcqldVsvFK"
      },
      "outputs": [],
      "source": [
        "if not os.path.exists(\"/content/drive/My Drive/DLL_project/results/\"):\n",
        "  raise FileExistsError(\"Could not find this directory, add a shortcut to your drive\")\n",
        "else: \n",
        "  path = \"/content/drive/My Drive/DLL_project/results/\"\n",
        "\n",
        "# names of the files in the Folder \n",
        "filenames = [\n",
        "      'all',\n",
        "      'no_ls_dp',\n",
        "      'lr_annealing',\n",
        "      'no_dropout_layer',\n",
        "      'resnet18',\n",
        "      '15batch', \n",
        "      'lr_0.001', \n",
        "      'grayscale', \n",
        "      'dropout0.5', \n",
        "      'resnet18_dropout'\n",
        "  ]\n",
        "\n",
        "# legend nomenclature  \n",
        "to_label = {\n",
        "      'all': 'full architecture',\n",
        "      'dropout0.5': 'dropout=0.5',\n",
        "      'no_ls_dp': 'no label_smoothing, no droupout',\n",
        "      'lr_annealing': 'lr decay',\n",
        "      'no_dropout_layer': 'dropout=0.0',\n",
        "      'resnet18': 'resnet18', \n",
        "      '15batch': 'batch=15', \n",
        "      'lr_0.001': 'lr=0.001', \n",
        "      'grayscale': 'grayscale=True', \n",
        "      'resnet18_dropout': 'model=resnet18, dropout=0.5'\n",
        "  }\n",
        "\n",
        "# line colors \n",
        "to_color = {\n",
        "        'all': 'xkcd:bright red',\n",
        "        'dropout0.5': 'xkcd:dirty yellow',\n",
        "        'no_ls_dp': 'xkcd:blue',\n",
        "        'lr_annealing': 'xkcd:purple',\n",
        "        'no_dropout_layer': 'xkcd:lime green',\n",
        "        'resnet18': 'xkcd:grass green', \n",
        "        '15batch': 'xkcd:cyan',\n",
        "        'lr_0.001': 'xkcd:olive', \n",
        "        'grayscale': 'xkcd:orange',\n",
        "        'resnet18_dropout': 'xkcd:pink'\n",
        "    }\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JHAOOX5wvAzJ"
      },
      "outputs": [],
      "source": [
        "def set_domains(source: str, target: str, filenames: list, path:str):\n",
        "  '''\n",
        "  From each folder extract the results json file containing \n",
        "  the accuracies and losses of the model training and validation. \n",
        "  From the best model folder (all) extract the source-only \n",
        "  results. \n",
        "\n",
        "  Parameters\n",
        "  ----------\n",
        "  source: str\n",
        "      source domain \n",
        "  target: str\n",
        "      target domain \n",
        "  filenames: list\n",
        "      list of the folder names\n",
        "  path: str \n",
        "      directory to the proper results folder  \n",
        "\n",
        "  Returns\n",
        "  -------\n",
        "  results \n",
        "      dictionary having as key the filename and as values the model's results\n",
        "  source_only \n",
        "      list with the source-only results \n",
        "  baseline\n",
        "      integer value set depending on the source domain\n",
        "  '''\n",
        "  results = {}\n",
        "  source_only = []\n",
        "\n",
        "  filename = source.split(\"_\")[0] + \"2\" + target.split(\"_\")[0]\n",
        "  foldername = source.split(\"_\")[0][0] + \"2\" + target.split(\"_\")[0][0]  \n",
        "\n",
        "  for fn in filenames:\n",
        "      with open(f'{path}{foldername}_{fn}/{filename}_results.json', 'r') as f:\n",
        "          results[fn] = json.load(f)\n",
        "\n",
        "  with open(f'{path}{foldername}_{fn}/{filename}_so_results.json', 'r') as f:\n",
        "    source_only = json.load(f)\n",
        "\n",
        "  if source.startswith(\"real\"): \n",
        "    baseline = 93 # given by the assignment\n",
        "  else: \n",
        "    baseline = 80 # given by the assignment \n",
        "  \n",
        "  return results, source_only, baseline\n",
        "\n",
        "\n",
        "def show_results(source: str, target: str, epochs: int, filenames: list, path: str, y_axis=0):\n",
        "  '''\n",
        "  Show the line chart of the target validation accuracy \n",
        "  of each model.\n",
        "\n",
        "  Parameters\n",
        "  ----------\n",
        "  source: str\n",
        "      source domain \n",
        "  target: str\n",
        "      target domain \n",
        "  epochs: int \n",
        "      number of epochs to show as x-axis \n",
        "  filenames: list\n",
        "      list of the folder names\n",
        "  path: str \n",
        "      directory to the proper results folder  \n",
        "  y_axis: int\n",
        "      origin of the y-axis \n",
        "  '''\n",
        "  results, source_only, baseline = set_domains(source, target, filenames, path)\n",
        "\n",
        "  sns.set_theme(style=\"whitegrid\", palette=\"pastel\")\n",
        "  fig, ax = plt.subplots()\n",
        "  \n",
        "  # Display the target validation accuracy for each model \n",
        "  for k in filenames:\n",
        "    if k in results.keys(): \n",
        "        val_ppls = results[k][\"val_tgt_accs\"]\n",
        "        val_ppls = [x*100 for x in val_ppls]\n",
        "        e = range(len(val_ppls))\n",
        "        ax.plot(e, val_ppls, label=to_label[k], color=to_color[k])\n",
        "    else: \n",
        "      raise ValueError(f\"The results of {k} have not been stored.\")\n",
        "  \n",
        "  # Add the external baseline \n",
        "  ax.plot(range(epochs-10), [baseline]*(epochs-10), '--', label='baseline', color='xkcd:grey', linewidth=3)\n",
        "  \n",
        "  # Add the internal baseline, meaning the source-only model \n",
        "  ax.plot(range(len(source_only['val_accuracies'])), [x*100 for x in source_only['val_accuracies']], '--', label='source-only', color='xkcd:black')\n",
        "  \n",
        "  # Set width and height of the graph\n",
        "  fig.set_figwidth(20)\n",
        "  fig.set_figheight(10)\n",
        "  fig.set_dpi(300)\n",
        "\n",
        "  # Set size of the words \n",
        "  plt.rc('font', size=20)\n",
        "  plt.rc('axes', titlesize=20, labelsize=20)\n",
        "  plt.rc('xtick', labelsize=20)\n",
        "  plt.rc('ytick', labelsize=20)\n",
        "  plt.rc('legend', fontsize=10)\n",
        "\n",
        "  # Set extremes of x and y axes \n",
        "  plt.ylim(bottom=y_axis, top=100)\n",
        "  plt.xlim(left=-0.5, right=epochs)\n",
        "\n",
        "  # Add legend \n",
        "  plt.legend(loc='upper right')\n",
        "\n",
        "  # Add title/labels \n",
        "  plt.title(source.split('_')[0].capitalize() + r'$\\rightarrow$' + target.split('_')[0].capitalize())\n",
        "  plt.xlabel(\"Epoch\")\n",
        "  plt.ylabel(\"Validation Accuracy\")\n",
        "\n",
        "  # Show picture \n",
        "  plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7rh7nqssvGlO"
      },
      "outputs": [],
      "source": [
        "show_results(source=\"real_life\", target=\"product_images\", epochs=52, filenames=filenames, path=path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mRd2PtvOFSCM"
      },
      "outputs": [],
      "source": [
        "show_results(source=\"product\", target=\"real_life\", epochs=60, filenames=filenames, path=path)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Different experiments were conducted by tuning the hyperparameters of the full architecture, aiming to investigate the effects on the overall performance (i.e., both directions). The considerations, supported by the diagnostic plots above, follow. \n",
        "\n",
        "* <b>Learning Rate</b> \n",
        "\n",
        "Setting the correct <code>learning rate</code> might be the main challenge in training deep learning neural networks, and it may turn out to be the most important hyperparameter for the model ([Goodfellow](https://www.deeplearningbook.org), 2016). Whereas the original paper and the PyTorch [<code>RMSProp</code>](https://pytorch.org/docs/stable/generated/torch.optim.RMSprop.html) implementation set the initial learning rate value to $0.01$ ([Zhang <i>et al.</i>](https://arxiv.org/abs/1904.04663), § 4.1), our architecture did not support that choice. Moreover, even $\\eta_0 = 1e^{-3}$ led to suboptimal performance as the model collapsed after a few epochs with losses stuck in local minima. However, the reduction of $1/10$ of the learning rate (i.e., $\\eta_0 = 1e^{-4}$) stabilized the learning process, which shows a proper loss decay and a rising accuracy (till a plateau). \n",
        "\n",
        "* <b>Batch Size</b>\n",
        "\n",
        "The <code>batch size</code> is another relevant hyperparameter. Considering that RMSProp was developed as a stochastic technique for mini-batch learning ([Kingma & Ba]((https://arxiv.org/pdf/1412.6980.pdf), 2014; [Guide to Adam and Rmsprop](https://medium.com/analytics-vidhya/a-complete-guide-to-adam-and-rmsprop-optimizer-75f4502d83be)) and that the number of classes to map was low (n = 20), a small batch size seemed a reasonable choice. Although the model achieves the best outcomes with batches of size $30$, also $15$ has been considered and displayed interesting insights. Thus, the validation accuracies are similar, but a smaller batch size implies a model prone to local minima convergence (see the second graph) and with a smoother learning curve, namely slower. \n",
        "\n",
        "* <b>ResNet</b>\n",
        "\n",
        "The empirical outcomes confirm that the application of ResNet50 provides the most reliable and stable performance, but interesting cues come from the ResNet18 application. The results achieved with this shallower backbone outperform the others when the domain gap is strong ($P → R$) and the dropout rate ($p=0.2$) is low, while the model rapidly degrades when the dropout rate rises ($p=0.5$). However, this is positively interpreted and demonstrates how ResNet18 is a suitable backbone that requires little fine-tuning, avoids additional noise, and is less computationally demanding. \n",
        "\n",
        "* <b>Grayscale & Random Crop</b>\n",
        "\n",
        "The data transformation seems not to affect the overall model learning as the diagnostic plots do not register any dissimilarity between the cases with pixel intensity normalization and those with the shift to grayscale images (i.e., <code>grayscale=True</code>). Similarly, the expected impact of randomly crop the images (i.e., <code>center=False</code>) does not occur. This phenomenon might be partially explained by the Adaptiope dataset setup that minimizes the number of uninformative pixels within the images by centring and cropping them.<a name=\"cite_ref-1\"></a>[<sup>[1]</sup>](#cite_note-1)\n",
        "\n",
        "</br>\n",
        "</br>\n",
        "<a name=\"cite_note-1\">[1]</a> <font size=\"-1\">The discussion about image dimensions was part of the Exploratory Data Analysis Section, where some considerations were demarcated and further analysed.</font> \n"
      ],
      "metadata": {
        "id": "qm41QLrwU8Vk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# select only the models where dropout is modified \n",
        "filenames = [\n",
        "      'all',\n",
        "      'no_ls_dp',\n",
        "      'no_dropout_layer',\n",
        "      'dropout0.5', \n",
        "      'resnet18_dropout'\n",
        "  ]\n",
        "show_results(source=\"real_life\", target=\"product_images\", epochs=52, filenames=filenames, y_axis=60, path=path)"
      ],
      "metadata": {
        "id": "aHxtRGUJpvvw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "show_results(source=\"product_images\", target=\"real_life\", epochs=52, filenames=filenames, y_axis=50, path=path)"
      ],
      "metadata": {
        "id": "PTg7irDDqosG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "* <b>Label smoothing and Dropout</b>\n",
        "\n",
        "<code>Label smoothing</code> and <code>Dropout</code> have been introduced to stabilize the training process, avoid overfitting and improve generalization error. Their importance is reflected in the partial results (see Appendix) and can also be detected in the plots above. Specifically, the impact of a noisy training process is visible when $p$ is set at $0.5$, which is a common and optimal choice for a wide range of networks and tasks ([Srivastava](https://jmlr.org/papers/v15/srivastava14a.html), 2014). Other noise, but on the labels, is introduced by the label Smoothing regularization technique. The best model is trained with a label smoothing factor of $0.2$. As aforementioned, the performance was quantitatively similar to training with hard targets (no label smoothing), but the risk of overconfidence was reduced<a name=\"cite_ref-2\"></a>[<sup>[2]</sup>](#cite_note-2).\n",
        "\n",
        "</br>\n",
        "</br>\n",
        "<a name=\"cite_note-2\">[2]</a> <font size=\"-1\">The overconfidence of the model has been empirically detected by printing the predicted labels and compared to the ground truth. After a few epochs, specific patterns dominated the training and strongly influenced the accuracy measure.</font> "
      ],
      "metadata": {
        "id": "1F7EgZSlu7qR"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OnVZLkNwcjK6"
      },
      "outputs": [],
      "source": [
        "def show_val(source: str, target: str, epoch: int, file: str, path: str):\n",
        "  '''\n",
        "  Show the line chart of the target validation accuracy \n",
        "  of the full architecture model in both directions.\n",
        "\n",
        "  Parameters\n",
        "  ----------\n",
        "  source: str\n",
        "      source domain \n",
        "  target: str\n",
        "      target domain \n",
        "  epoch: int \n",
        "      number of epochs to show as x-axis \n",
        "  file: str\n",
        "      name of the file reporting the best model results \n",
        "  path: str \n",
        "      directory to the proper results folder  \n",
        "  '''\n",
        "  \n",
        "  sns.set_theme(style=\"whitegrid\", palette=\"pastel\")\n",
        "  fig, ax = plt.subplots()\n",
        "  \n",
        "  # source -> target \n",
        "  results, source_only, baseline = set_domains(source, target, [file], path)\n",
        "  direction = source.split('_')[0][0].capitalize() + r'$\\rightarrow$' + target.split('_')[0][0].capitalize()\n",
        "  \n",
        "  # Display the target validation accuracy curve for uda \n",
        "  val_tgt_acc = results[file][\"val_tgt_accs\"]\n",
        "  ax.plot(range(len(val_tgt_acc)), val_tgt_acc, label=f'uda val. acc. {direction}', color='xkcd:blue')\n",
        "  \n",
        "  # Display the target validation accuracy curve for source-only  \n",
        "  ax.plot(range(len(source_only['val_accuracies'])), source_only['val_accuracies'], '--', label=f'source-only val. acc. {direction}', color='xkcd:black')\n",
        "\n",
        "  # Switch domains \n",
        "  # target -> source \n",
        "  results, source_only, baseline = set_domains(target, source, [file], path)\n",
        "  direction = target.split('_')[0][0].capitalize() + r'$\\rightarrow$' + source.split('_')[0][0].capitalize()\n",
        "  val_tgt_acc = results[file][\"val_tgt_accs\"]\n",
        "  ax.plot(range(len(val_tgt_acc)), val_tgt_acc, label=f'uda val. acc. {direction}', color='xkcd:orange')\n",
        "  ax.plot(range(len(source_only['val_accuracies'])), source_only['val_accuracies'], '--', label=f'source-only val. acc. {direction}', color='xkcd:gray')\n",
        " \n",
        "  # Set width and height of the graph\n",
        "  fig.set_figwidth(7)\n",
        "  fig.set_figheight(7)\n",
        "  fig.set_dpi(300)\n",
        "\n",
        "  # Set size of the words \n",
        "  plt.rc('font', size=20)\n",
        "  plt.rc('axes', titlesize=20, labelsize=20)\n",
        "  plt.rc('xtick', labelsize=20)\n",
        "  plt.rc('ytick', labelsize=20)\n",
        "  plt.rc('legend', fontsize=5)\n",
        "\n",
        "  # Set extremes of x and y axes \n",
        "  plt.ylim(bottom=0.4, top=1)\n",
        "  plt.xlim(left=-0.5, right=epoch)\n",
        "\n",
        "  # Add legend \n",
        "  plt.legend(loc='upper right')\n",
        "\n",
        "  # Add title/labels \n",
        "  plt.title(\"Model accuracy for target domain\", fontsize=15)\n",
        "  plt.xlabel(\"Epoch\")\n",
        "  plt.ylabel(\"Accuracy\")\n",
        "\n",
        "  # Show picture \n",
        "  plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ifSOf8N6cjK6"
      },
      "outputs": [],
      "source": [
        "show_val(source=\"real_life\", target=\"product\", epoch=55, file=\"dropout0.5\", path=path)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "As previously stated, the full architecture achieves better performance in both directions. It is also worth noticing how the two learning curves progress. Whereas the source-only architecture curve shows unstable and almost random progress with several peaks and falls, the full architecture grows steadily and reaches a plateau. Using a robust backbone (ResNet50) and a relatively large batch size can justify the overall satisfactory performance of the source-only. "
      ],
      "metadata": {
        "id": "EPbSxKqR2a-L"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-NXAF82tsvFK"
      },
      "source": [
        "# References"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uAXwN159svFL"
      },
      "source": [
        "<ol>\n",
        "         <li>\n",
        "         <p> Zhang, Y. <i> et al.</i>, 2019. <cite>Domain-symmetric networks for adversarial domain adaptation.</cite> In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pp. 5031-5040.</p>\n",
        "         </li>\n",
        "         <li>\n",
        "            <p> Zhang, J. <i> et al.</i>, 2018. <cite>Importance weighted adversarial nets for partial domain adaptation.</cite> In Proceedings of the IEEE conference on computer vision and pattern recognition, pp. 8156-8164. </p>\n",
        "         </li>\n",
        "         <li>\n",
        "            <p> Goodfellow, I., 2016. <cite>Nips 2016 tutorial: Generative adversarial networks.</cite> ArXiv preprint arXiv:1701.00160</p>\n",
        "         </li>\n",
        "         <li>\n",
        "            <p> Mordido, G. <i> et al.</i>, 2018.<cite>Dropout-gan: Learning from a dynamic ensemble of discriminators.</cite> arXiv preprint arXiv:1807.11346</p>\n",
        "         </li>\n",
        "         <li>\n",
        "            <p> Müller, R.<i> et al.</i>, 2019.<cite>When does label smoothing help?.</cite> Advances in neural information processing systems, p. 32.\n",
        "            </li>\n",
        "            <li>\n",
        "            <p> Kingma, D.P., & Ba, J., 2014. <cite>Adam: A method for stochastic optimization. </cite> arXiv preprint arXiv:1412.6980.\n",
        "            </li>\n",
        "            <li>\n",
        "            <p> Tieleman, T., & Hinton, G., 2012. <cite>Lecture 6.5-rmsprop: Divide the gradient by a running average of its recent magnitude.</cite> COURSERA: Neural networks for machine learning, 4(2), pp. 26-31.\n",
        "            </li>\n",
        "            <li>\n",
        "            <p> Ringwald, T., & Stiefelhagen, R., 2021. <cite>Adaptiope: A modern benchmark for unsupervised domain adaptation.</cite> In 2021 IEEE Winter Conference on Applications of Computer Vision (WACV), pp. 101–110.\n",
        "            </li>\n",
        "            <li>\n",
        "            <p> Goodfellow, I.J., Bengio, Y., and  Courville, A., 2016. <cite>Deep Learning.</cite> MIT Press, p.429.\n",
        "            </li>\n",
        "           <li>\n",
        "            <p> \n",
        "Srivastava, N. <i>et al.</i>, 2014. <cite>Dropout: A Simple Way to Prevent Neural Networks from Overfitting.</cite> Journal of Machine Learning Research, 15(56), pp. 1929−1958.\n",
        "            </li>\n",
        "      </ol>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uA748h05svFL"
      },
      "source": [
        "# Appendix"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R9SsQkRz1zSl"
      },
      "source": [
        "## Memory Tools"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Code to clean up the RAM GPU after architecture training. It is worth noticing as it has been trained in Google Colab Pro environment due to the high computational power needed.  "
      ],
      "metadata": {
        "id": "EkrpQCVOSj1A"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QraheZZx0sre"
      },
      "outputs": [],
      "source": [
        "import gc\n",
        "\n",
        "!nvidia-smi\n",
        "\n",
        "if 'loader' in globals() or 'loader' in locals():\n",
        "    print('Deleting loader...')\n",
        "    del loader\n",
        "if 'uda_generator' in globals() or 'uda_generator' in locals():\n",
        "    print('Deleting generator...')\n",
        "    del uda_generator\n",
        "if 'symnet' in globals() or 'symnet' in locals():\n",
        "    print('Deleting symnet...')\n",
        "    del symnet\n",
        "\n",
        "gc.collect()\n",
        "torch.cuda.empty_cache()\n",
        "\n",
        "!nvidia-smi"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Fine-tuning results"
      ],
      "metadata": {
        "id": "RHpXY34XQuz5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The fine-tuning process has been tracked and the results of each model saved in ad hoc folder within the [<code>results</code>](https://drive.google.com/drive/folders/1LKLpTckmcrQ6w7mGFp55r1JWSv_5A3d1?usp=sharing) folder. Each sub-folder reflects the following structure: \n",
        "\n",
        "```\n",
        "results \n",
        "│\n",
        "├── p2r_foldername\n",
        "│ \n",
        "└── r2p_foldername\n",
        "    │   \n",
        "    └── real2product_results.json\n",
        "    |\n",
        "    └── real2product_params.json\n",
        "    |\n",
        "    └── real2product_so_results.json\n",
        "    |\n",
        "    └── real2product.pickle\n",
        "\n",
        "```\n",
        "\n",
        "The code is also available at the following [Git repository](https://github.com/detch-for-shor/deep-learning-proj)"
      ],
      "metadata": {
        "id": "2CH1G8hiQ2jv"
      }
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "uA748h05svFL",
        "RHpXY34XQuz5"
      ],
      "machine_shape": "hm",
      "name": "SymNets_final.ipynb",
      "provenance": [],
      "toc_visible": true
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3.9.13 64-bit",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.13"
    },
    "vscode": {
      "interpreter": {
        "hash": "b0fa6594d8f4cbf19f97940f81e996739fb7646882a419484c72d19e05852a7e"
      }
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "1b1c0854f78146e1bd7c78bba1740d3a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_1916883c18e548cc892072e9e9272dc7",
              "IPY_MODEL_2d1b1519b84a45d9a0e55482772f2aee",
              "IPY_MODEL_0ba30ecf34244f4cb3aeba6b618a66ee"
            ],
            "layout": "IPY_MODEL_149df33c2c8c4807a74aa05cde2f3b03"
          }
        },
        "1916883c18e548cc892072e9e9272dc7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d2f00ffe7e7d4bfeb82c1e66a2414410",
            "placeholder": "​",
            "style": "IPY_MODEL_c5087c481bd64697a0da3a8acbfc4fff",
            "value": "100%"
          }
        },
        "2d1b1519b84a45d9a0e55482772f2aee": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_198b6d8958e84679affe6dbb699f2706",
            "max": 102530333,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_c200b2818ea942e2bd05fa18ec22ac23",
            "value": 102530333
          }
        },
        "0ba30ecf34244f4cb3aeba6b618a66ee": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d97bcf74d4104b53928dbb646f350e41",
            "placeholder": "​",
            "style": "IPY_MODEL_2e6603df4a784115aacca03e47cc43b9",
            "value": " 97.8M/97.8M [00:01&lt;00:00, 91.4MB/s]"
          }
        },
        "149df33c2c8c4807a74aa05cde2f3b03": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d2f00ffe7e7d4bfeb82c1e66a2414410": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c5087c481bd64697a0da3a8acbfc4fff": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "198b6d8958e84679affe6dbb699f2706": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c200b2818ea942e2bd05fa18ec22ac23": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "d97bcf74d4104b53928dbb646f350e41": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2e6603df4a784115aacca03e47cc43b9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}