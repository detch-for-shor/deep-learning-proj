{"cells":[{"cell_type":"markdown","metadata":{"id":"W5if2JDi0ic6"},"source":["# Python Version "]},{"cell_type":"code","execution_count":null,"metadata":{"id":"i6s1U3SG0cGq"},"outputs":[],"source":["!sudo apt-get update -y\n","!sudo apt-get install python3.10\n","!sudo update-alternatives --install /usr/bin/python3 python3 /usr/bin/python3.7 1\n","!sudo update-alternatives --install /usr/bin/python3 python3 /usr/bin/python3.10 2\n","!python --version"]},{"cell_type":"markdown","metadata":{"id":"veDaOcVb0G9h"},"source":["# Import  Libraries"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"6Iz5l41k0G9j"},"outputs":[],"source":["import os \n","import json\n","import math\n","import torch\n","import torchvision\n","import shutil\n","\n","import numpy as np\n","import torch.nn as nn\n","import torch.cuda as cuda\n","import torchvision.transforms as T\n","import torch.nn.functional as F\n","import pandas as pd\n","import seaborn as sns\n","import matplotlib.pyplot as plt\n","\n","\n","from tqdm import tqdm\n","from statistics import mean\n","from pathlib import Path\n","from sklearn.model_selection import train_test_split\n","from sklearn.metrics import accuracy_score \n","from torch import Tensor\n","from torchvision import models\n","from torch.autograd import Variable\n","from torch.utils.data.dataloader import DataLoader\n","from torch.utils.data import Subset\n","from torch.nn import CrossEntropyLoss\n","from torch.optim import RMSprop, Adagrad\n","\n","try: \n","  from overrides import overrides, final\n","except: \n","  pass\n","\n","from abc import abstractmethod\n","from google.colab import drive\n","from __future__ import annotations"]},{"cell_type":"markdown","metadata":{},"source":["# HyperParameters"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Device Settings\n","seed = 42\n","\n","# Data Transformation\n","path = \"./adaptiope_small/\"\n","source = \"product_images/\"\n","target = \"real_life/\"\n","resize_dim = 256\n","crop_dim = 224\n","grayscale = False\n","crop_center = True\n","\n","# Data Loader\n","batch_size = 32\n","test_split = 0.2\n","\n","# Feature Extractor\n","n_classes = 20\n","freeze = False\n","model = 'resnet50'\n","optim = 'rmsprop'\n","\n","# Model Trainer\n","n_epochs = 250"]},{"cell_type":"markdown","metadata":{"id":"eG_KkWoF1iLA"},"source":["# Device Settings"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":7,"status":"ok","timestamp":1658951310433,"user":{"displayName":"Detch","userId":"00216123624798167471"},"user_tz":-120},"id":"F-Ccv0WA0wUL","outputId":"d3cd47cd-319c-4edf-f364-34c723c981d2"},"outputs":[{"name":"stdout","output_type":"stream","text":["You're running on GPU\n"]}],"source":["np.random.seed(seed)\n","torch.manual_seed(seed)\n","if cuda.is_available():\n","    print('You\\'re running on GPU')\n","    cuda.manual_seed(seed)\n","    gpu = True\n","else:\n","    print('You\\'re running on CPU')\n","    gpu = False"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ZYpPF-EA01MY"},"outputs":[],"source":["gpu_info = !nvidia-smi\n","gpu_info = '\\n'.join(gpu_info)\n","if gpu_info.find('failed') >= 0:\n","  print('Not connected to a GPU')\n","else:\n","  print(gpu_info)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"kIlVaihW04ub"},"outputs":[],"source":["from psutil import virtual_memory\n","ram_gb = virtual_memory().total / 1e9\n","ram_gb = round(ram_gb, 2)\n","print(f'{ram_gb} GB of RAM\\n')"]},{"cell_type":"markdown","metadata":{"id":"zzwgGaFW2VpR"},"source":["# Load Dataset"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"_byHbGvzCpOS"},"outputs":[],"source":["drive.mount('/content/drive', force_remount=True)\n","\n","if not os.path.exists('/content/adaptiope_small'):\n","  !mkdir dataset\n","  !cp \"gdrive/My Drive/DLL_project/Adaptiope.zip\" dataset/\n","  !ls dataset\n","  !unzip dataset/Adaptiope.zip\n","  !rm -rf adaptiope_small\n","  !mkdir adaptiope_small\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Ydd7Y4y40G9l"},"outputs":[],"source":["classes = [\"backpack\", \"bookcase\", \"car jack\", \"comb\", \"crown\", \"file cabinet\", \"flat iron\", \"game controller\", \"glasses\",\n","           \"helicopter\", \"ice skates\", \"letter tray\", \"monitor\", \"mug\", \"network switch\", \"over-ear headphones\", \"pen\",\n","           \"purse\", \"stand mixer\", \"stroller\"]"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"wHM96q2tDMXd"},"outputs":[],"source":["for d, td in zip([\"Adaptiope/product_images\", \"Adaptiope/real_life\"], [\"adaptiope_small/product_images\", \"adaptiope_small/real_life\"]):\n","  os.makedirs(td)\n","  for c in tqdm(classes):\n","    c_path = os.path.join(d, c)\n","    c_target = os.path.join(td, c)\n","    shutil.copytree(c_path, c_target)"]},{"cell_type":"markdown","metadata":{"id":"QhWEa-5C0G9l"},"source":["# Data Transformation"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"KZSDTkjY0G9m"},"outputs":[],"source":["def data_transformation(resize_dim = 256, crop_dim = 224, grayscale = True, crop_center = True):\n","    \n","    transform_lst = []\n","    transform_lst.append(T.Resize((resize_dim)))                                                          \n","    \n","    if grayscale:\n","        transform_lst.append(T.Grayscale(num_output_channels=3))                        \n","    \n","    if crop_center:\n","        transform_lst.append(T.CenterCrop((crop_dim)))\n","    else:\n","        transform_lst.append(T.RandomCrop((crop_dim)))\n","    \n","    transform_lst.append(T.RandomHorizontalFlip(p=0.5))                                  \n","    transform_lst.append(T.ToTensor())                                             \n","        \n","    return T.Compose(transform_lst)  \n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"BtPLbGgh0G9m"},"outputs":[],"source":["def normalization(dataset):\n","    ds_length = len(dataset)\n","    for i in tqdm(range(ds_length)):\n","        r_mean, g_mean, b_mean = torch.mean(dataset[i][0], dim = [1,2])\n","        r_std, g_std, b_std = torch.std(dataset[i][0], dim = [1,2])\n","        T.functional.normalize(\n","            tensor = dataset[i][0], \n","            mean = [r_mean, g_mean, b_mean],\n","            std = [r_std, g_std, b_std],\n","            inplace=True)\n","    return dataset"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"M6dBd8Zh0G9n","outputId":"7ca94881-926d-4778-b413-b29681f0342f"},"outputs":[{"name":"stderr","output_type":"stream","text":["100%|██████████| 2000/2000 [01:36<00:00, 20.83it/s]\n","100%|██████████| 2000/2000 [02:49<00:00, 11.81it/s]\n"]}],"source":["root = path + source\n","\n","source_ds = torchvision.datasets.ImageFolder(\n","    root = path + source,\n","    transform = data_transformation(\n","        resize_dim, \n","        crop_dim, \n","        grayscale, \n","        crop_center))\n","\n","target_ds = torchvision.datasets.ImageFolder(\n","    root = path + target, \n","    transform = data_transformation(\n","        resize_dim, \n","        crop_dim, \n","        grayscale, \n","        crop_center)) \n","\n","if not grayscale:\n","    normalization(source_ds)\n","    normalization(target_ds)"]},{"cell_type":"markdown","metadata":{"id":"ya6nhiHiDhkM"},"source":["# Data Loader "]},{"cell_type":"code","execution_count":null,"metadata":{"id":"nrDImKxB0G9o"},"outputs":[],"source":["def get_data(dataset, test_split=0.2, batch_size=32):\n","    \n","    train_indices, val_indices = train_test_split(\n","        list(range(len(dataset.targets))),\n","        test_size = test_split,\n","        stratify = dataset.targets, \n","        random_state = 42)\n","    \n","    train_dataset = Subset(dataset, train_indices)\n","    val_dataset = Subset(dataset, val_indices)\n","\n","    train_data_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n","    val_data_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n","    \n","    return train_data_loader, val_data_loader\n","    "]},{"cell_type":"code","execution_count":null,"metadata":{"id":"uWXxiULT0G9o"},"outputs":[],"source":["source_train_loader, source_val_loader = get_data(source_ds, test_split, batch_size)\n","target_train_loader, target_val_loader = get_data(target_ds, test_split, batch_size)"]},{"cell_type":"markdown","metadata":{"id":"T9uO6jyHEeKs"},"source":["# UDA Architecture "]},{"cell_type":"markdown","metadata":{"id":"PzHiEzWR0G9p"},"source":["## Losses"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"66HGz_c30G9p"},"outputs":[],"source":["class _Loss(nn.Module):\n","    \n","    _THRESHOLD = 1e-20\n","    \n","    def __init__(self):\n","        super(_Loss, self).__init__()\n","        \n","    def forward(self, input: Tensor):\n","        prob = self.to_softmax(input)\n","        return self.loss(prob)\n","        \n","    # @final\n","    def add_threshold(self, prob: Tensor):\n","        '''\n","        Check whether the probability distribution after the softmax \n","        is equal to 0 in any cell. If this holds, a standard threshold\n","        is added in order to avoid log(0) case. \n","\n","        Parameters\n","        ----------\n","        prob: Tensor\n","            output tensor of the softmax operation\n","\n","        Returns\n","        -------\n","        Tensor\n","            updated tensor (in case the condition above holds)\n","        '''\n","        zeros = (prob == 0)\n","        if torch.any(zeros):\n","            thre_tensor = torch.zeros(zeros.shape)\n","            thre_tensor[zeros] = self._THRESHOLD\n","            prob = prob + thre_tensor\n","        return prob\n","    \n","    def to_softmax(self, features: Tensor):\n","        '''\n","        Apply the softmax operation on the features tensor, \n","        being the output of a feature extractor. \n","        \n","        Parameters\n","        ----------\n","        features: Tensor\n","            input tensor of the softmax operation\n","\n","        Returns\n","        -------\n","        Tensor\n","            probability distribution with (possible) threshold\n","        '''\n","        prob = F.softmax(features, dim=1)\n","        return self.add_threshold(prob)\n","    \n","    @abstractmethod\n","    def loss(self, prob: Tensor):\n","        pass"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"EhXN9HF-0G9q"},"outputs":[],"source":["class EntropyMinimizationLoss(_Loss):\n","    \n","    def __init__(self, n_classes: int):\n","        super(EntropyMinimizationLoss, self).__init__()\n","        self.n_classes = n_classes\n","    \n","    # @overrides\n","    def loss(self, prob: Tensor):\n","        prob_source = prob[:, :self.n_classes]\n","        prob_target = prob[:, self.n_classes:]\n","        prob_sum = prob_source + prob_target\n","        return -(prob_sum.log().mul(prob_sum).sum(dim=1).mean())"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"C5zkjQbG0G9q"},"outputs":[],"source":["class SplitLoss(_Loss):\n","    \n","    def __init__(self, n_classes: int, source: bool, split_first: bool):\n","        super(SplitLoss, self).__init__()\n","        self.n_classes = n_classes\n","        self._is_source = source\n","        self._split_first = split_first\n","    \n","    # @overrides\n","    def to_softmax(self, features: Tensor):\n","        if self._split_first:\n","            prob = self.split_vector(features)\n","            prob = F.softmax(prob, dim=1)\n","        else:\n","            prob = F.softmax(features, dim=1)\n","            prob = self.split_vector(prob)\n","        return self.add_threshold(prob)\n","    \n","    # @final\n","    def split_vector(self, prob: Tensor):\n","        return prob[:,:self.n_classes] if self._is_source else prob[:,self.n_classes:]"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"aDGpTlic0G9q"},"outputs":[],"source":["class SplitCrossEntropyLoss(SplitLoss):\n","    \n","    def _get_y_labels(self):\n","        return self._y_labels\n","    def _set_y_labels(self, y_labels: Variable):\n","        if not all(y < self.n_classes for y in y_labels):\n","            raise ValueError('Expected all y labels < n_classes')\n","        self._y_labels = y_labels\n","    y_labels = property(fget=_get_y_labels, fset=_set_y_labels)\n","    \n","    def __init__(self, n_classes: int, source: bool, split_first: bool):\n","        super(SplitCrossEntropyLoss, self).__init__(n_classes, source, split_first)\n","        self.cross_entropy_loss = CrossEntropyLoss()\n","    \n","    # @overrides\n","    def loss(self, prob: Tensor):\n","        '''Computes cross-entropy loss w.r.t. ground-truth (y label)'''\n","        return self.cross_entropy_loss(prob, self.y_labels)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"9WJRrEK20G9r"},"outputs":[],"source":["class DomainDiscriminationLoss(SplitLoss):\n","    \n","    def __init__(self, n_classes: int, source: bool):\n","        super(DomainDiscriminationLoss, self).__init__(n_classes, source, False)\n","        \n","    # @overrides\n","    def loss(self, prob: Tensor):\n","        return -(prob.sum(dim=1).log().mean())"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"rSp7_hu20G9r"},"outputs":[],"source":["class TrainingObjectives:\n","    \n","    @staticmethod\n","    def domain_discrimination_loss(src_dom_discrim_loss, tgt_dom_discrim_loss):\n","        return src_dom_discrim_loss + tgt_dom_discrim_loss\n","    \n","    @staticmethod\n","    def category_confusion_loss(src_cat_conf_loss, tgt_cat_conf_loss):\n","        return 0.5 * (src_cat_conf_loss + tgt_cat_conf_loss)\n","    \n","    @staticmethod\n","    def domain_confusion_loss(src_dom_conf_loss, tgt_dom_conf_loss):\n","        return 0.5 * (src_dom_conf_loss + tgt_dom_conf_loss)\n","    \n","    @staticmethod\n","    def overall_classifier_loss(src_task_class_loss, tgt_task_class_loss, domain_discrim_loss):\n","        return src_task_class_loss + tgt_task_class_loss + domain_discrim_loss\n","    \n","    @staticmethod\n","    def overall_generator_loss(cat_conf_loss, dom_conf_loss, tgt_entropy_loss, curr_epoch, tot_epochs):\n","        lambda_trade_off = 2 / (1 + math.exp(-1 * 10 * curr_epoch / tot_epochs)) - 1\n","        return cat_conf_loss + lambda_trade_off * (dom_conf_loss + tgt_entropy_loss)"]},{"cell_type":"markdown","metadata":{"id":"lS8yTMu10G9r"},"source":["## Feature Extractor"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"AfuKcRs50G9r"},"outputs":[],"source":["\n","class FeatureExtractor:\n","    \n","    def __init__(self, n_classes: int, model='resnet50', optimizer='rmsprop', n_params_trained=None, freeze=False, lr=0.01, weight_decay=0, source_only=False):\n","        \"\"\"_summary_\n","\n","        Args:\n","            n_classes (int): number of classes present in the dataset\n","            n_params_trained (_type_, optional): Number of parameters (i.e., layers to be trained). Defaults to None.\n","            model (str, optional): Pretrained model to import as feature extractor. Defaults to 'resnet18'.\n","            optimizer (str, optional): Optimizer for the model. Defaults to 'rmsprop'.\n","            lr (float, optional): Initial learning rate. Defaults to 0.01.\n","            weight_decay (int, optional): Initial weight decay. Defaults to 0.\n","        \"\"\"\n","        self.lr = lr \n","        \n","        # Upload pretrained model \n","        if model.lower() == 'resnet18': \n","            self.model = models.resnet18(pretrained=True)\n","        elif model.lower() == 'resnet50': \n","            self.model = models.resnet50(pretrained=True)\n","        else:\n","            raise ValueError('Unknown model')\n","        \n","        # Modify last fully-connected layer\n","        if not source_only: \n","            self.model.fc = nn.Linear(\n","                in_features = self.model.fc.in_features, \n","                out_features = n_classes * 2\n","            )\n","        else:\n","            self.model.fc = nn.Linear(\n","                in_features = self.model.fc.in_features, \n","                out_features = n_classes\n","            ) \n","        \n","        n_params = len(list(self.model.parameters()))\n","        if n_params_trained is None:\n","            n_params_trained = n_params\n","        \n","        count = 0 \n","        first_param_trained = n_params - n_params_trained\n","        \n","        if freeze:\n","            for param in self.model.parameters():\n","                param.requires_grad = (count >= first_param_trained)\n","                count = count + 1 \n","\n","            params_to_train = filter(lambda p: p.requires_grad, self.model.parameters())\n","        \n","        else:\n","            \n","            # Layer-wise Learning Rate Decay (metà gruppi di layers = metà valore del lr)\n","            \n","            params_to_train = []\n","            name_prev_group = None\n","            \n","            groups = set([name.split('.')[0] for name, _ in self.model.named_parameters()])\n","            i = -1 \n","            for name, param in self.model.named_parameters():\n","                name_cur_group = name.split('.')[0]\n","                if name_cur_group != name_prev_group or name_prev_group is None:\n","                    i = i + 1\n","                    lr_group = self.decay(i, len(groups)-1)\n","                name_prev_group = name_cur_group\n","                params_to_train.append({'params': param, 'lr': lr_group})\n","            \n","        # Initialize optimizer\n","        if optimizer.lower() == 'rmsprop':\n","            self.optim = torch.optim.RMSprop(\n","                params = params_to_train,\n","                lr = lr,\n","                weight_decay = weight_decay\n","            )\n","        elif optimizer.lower() == 'adadelta':\n","            self.optim = torch.optim.Adadelta(\n","                params = params_to_train,\n","                lr = lr,\n","                weight_decay = weight_decay\n","            )\n","        elif optimizer.lower() == 'sgd':\n","            self.optim = torch.optim.SGD(\n","                params = params_to_train,\n","                lr = lr,\n","                weight_decay = weight_decay,\n","                nesterov = True\n","            )\n","        else:\n","            raise ValueError('Unknown optimizer')\n","    \n","    def decay(self, index: int, n_groups: int):\n","        sigmoid = lambda x: 1/(1 + np.exp(-x)) \n","        return self.lr * sigmoid(10/n_groups * (index - (n_groups/2))) \n","            \n","        "]},{"cell_type":"markdown","metadata":{"id":"En10ic2_0G9s"},"source":["## Training "]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["class ModelLoader:\n","    \n","    def __init__(self, base_dir: str, filename: str):\n","        self.base_dir = base_dir\n","        self.filename = filename\n","    \n","    def check_base_dir(self):\n","        try:\n","            if not os.path.exists(self.base_dir):\n","                os.makedirs(self.base_dir)\n","            return True\n","        except OSError:\n","            return False\n","\n","    def model_save(self, model, optimizer):\n","        if self.check_base_dir(): \n","            with open(self.filename, 'wb') as f:\n","                torch.save([model, optimizer], f)\n","        else:\n","            raise Exception('Model saving failed')\n","\n","    def model_load(self):\n","        if self.check_base_dir():\n","            with open(self.filename, 'rb') as f:\n","                model, optimizer = torch.load(f)\n","                return model, optimizer\n","        else:\n","            raise Exception('Model loading failed')\n","    \n","    def save_hyperparam(self, params_dict):\n","        with open(f'{self.filename}_results.json', 'w') as f:\n","            json.dump(params_dict, f, indent=4)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"xNNN7Mwo0G9s"},"outputs":[],"source":["class ModelTrainer:\n","    \n","    _INF = 1e20\n","    _LAST = 5\n","    \n","    def __init__(self, model: FeatureExtractor, loader: ModelLoader, n_classes: int, epochs: int):\n","        \"\"\"Initialize the SymsNet model\n","        Args:\n","            model (FeatureExtractor): _description_\n","            n_classes (int): _description_\n","            epochs (int): _description_\n","        \"\"\"\n","        self.curr_epoch = 0\n","        self.tot_epochs = epochs\n","        self.n_classes = n_classes\n","        self.model = model\n","        self.loader = loader\n","        self.patience = 10 \n","        self.src_task_class_loss = SplitCrossEntropyLoss(n_classes=n_classes, source=True, split_first=True)\n","        self.tgt_task_class_loss = SplitCrossEntropyLoss(n_classes=n_classes, source=False, split_first=True)\n","        # Domain discrimination losses\n","        self.src_dom_discrim_loss = DomainDiscriminationLoss(n_classes=n_classes, source=True)\n","        self.tgt_dom_discrim_loss = DomainDiscriminationLoss(n_classes=n_classes, source=False)\n","        # Category-level confusion losses\n","        self.src_cat_conf_loss = SplitCrossEntropyLoss(n_classes=n_classes, source=True, split_first=False)\n","        self.tgt_cat_conf_loss = SplitCrossEntropyLoss(n_classes=n_classes, source=False, split_first=False)\n","        # Domain-level confusion losses\n","        self.src_dom_conf_loss = DomainDiscriminationLoss(n_classes=n_classes, source=True)\n","        self.tgt_dom_conf_loss = DomainDiscriminationLoss(n_classes=n_classes, source=False)\n","        # Entropy minimization loss\n","        self.tgt_entropy_loss = EntropyMinimizationLoss(n_classes=n_classes)   \n","\n","        if cuda.is_available():\n","            # Task classifier losses\n","            self.src_task_class_loss = self.src_task_class_loss.cuda()\n","            self.tgt_task_class_loss = self.tgt_task_class_loss.cuda()\n","            # Domain discrimination losses\n","            self.src_dom_discrim_loss = self.src_dom_discrim_loss.cuda()\n","            self.tgt_dom_discrim_loss = self.tgt_dom_discrim_loss.cuda()\n","            # Category-level confusion losses\n","            self.src_cat_conf_loss = self.src_cat_conf_loss.cuda()\n","            self.tgt_cat_conf_loss = self.tgt_cat_conf_loss.cuda()\n","            # Domain-level confusion losses\n","            self.src_dom_conf_loss = self.src_dom_conf_loss.cuda()\n","            self.tgt_dom_conf_loss = self.tgt_dom_conf_loss.cuda()\n","            # Entropy minimization loss\n","            self.tgt_entropy_loss = self.tgt_entropy_loss.cuda()    \n","\n","    def train_step(self, X_source: Tensor, y_source: Tensor, X_target: Tensor):\n","        # Tell model go training mode\n","        self.model.model.train()\n","        # Compute features for both inputs\n","        y_source_pred = self.model.model(X_source)\n","        y_target_pred = self.model.model(X_target)\n","        # Compute overall training objective losses\n","        classifier_loss, generator_loss = self.overall_losses(\n","            y_source_pred, \n","            y_target_pred, \n","            y_source)\n","        # Compute gradients w.r.t. classifier loss\n","        self.model.optim.zero_grad()\n","        classifier_loss.backward(retain_graph=True)\n","        grad_classifier_tmp = []\n","        for p in self.model.model.parameters():\n","            if p.grad is not None:\n","                grad_classifier_tmp.append(p.grad.data.clone())\n","        # Compute gradients w.r.t. generator loss\n","        self.model.optim.zero_grad()\n","        generator_loss.backward()\n","        grad_generator_tmp = []\n","        for p in self.model.model.parameters():\n","            if p.grad is not None:\n","                grad_generator_tmp.append(p.grad.data.clone())\n","        # Update gradient data for each parameter \n","        count = 0 \n","        appended = 0 \n","        n_classification_params = 2 \n","        n_params = len(list(self.model.model.parameters()))\n","        for p in self.model.model.parameters():\n","            if p.grad is not None:\n","                grad_tmp = p.grad.data.clone()\n","                grad_tmp.zero_() \n","                if count < (n_params - n_classification_params): \n","                    grad_tmp = grad_tmp + grad_generator_tmp[appended]\n","                else: \n","                    grad_tmp = grad_tmp + grad_classifier_tmp[appended]\n","                appended = appended + 1 \n","                p.grad.data = grad_tmp\n","            count = count + 1\n","        # Perform optimizer step    \n","        self.model.optim.step()\n","        # Calculate accuracy\n","        y_source_true = y_source.clone().tolist()\n","        y_source_pred = y_source_pred.clone()\n","        acc_on_source_half = accuracy_score(y_source_true, torch.argmax(y_source_pred[:,:self.n_classes], dim=1))\n","        acc_on_target_half = accuracy_score(y_source_true, torch.argmax(y_source_pred[:,self.n_classes:], dim=1))\n","        # Return losses and accuracies\n","        return classifier_loss, generator_loss, acc_on_source_half, acc_on_target_half\n","       \n","    def val_step(self, X_target: Tensor, y_target: Tensor):\n","        # Tell model go validation mode\n","        self.model.model.eval()\n","        # Get outputs for both inputs\n","        with torch.no_grad():\n","            y_target_pred = self.model.model(X_target)\n","        # Calculate accuracy\n","        y_target_true = y_target.clone().tolist()\n","        y_target_pred = y_target_pred.clone()\n","        acc_on_source_half = accuracy_score(y_target_true, torch.argmax(y_target_pred[:,:self.n_classes], dim=1))\n","        acc_on_target_half = accuracy_score(y_target_true, torch.argmax(y_target_pred[:,self.n_classes:], dim=1))\n","        # Return accuracies\n","        return acc_on_source_half, acc_on_target_half\n","        \n","    def train_epoch(self, source_loader: DataLoader, target_loader: DataLoader):\n","        end_of_epoch = False\n","        source_batch_loader = enumerate(source_loader)\n","        target_batch_loader = enumerate(target_loader)\n","        gen_losses = []\n","        cl_losses = []\n","        accuracies_src = []\n","        accuracies_tgt = []\n","        # Train current epoch\n","        while not end_of_epoch:\n","            try:\n","                # Get next batch for both source and target\n","                (X_source, y_source) = next(source_batch_loader)[1]\n","                (X_target, _) = next(target_batch_loader)[1]\n","                # Apply training step\n","                cl_loss, gen_loss, acc_src, acc_tgt = self.train_step(X_source, y_source, X_target)\n","                # Append losses and accuracies\n","                cl_losses.append(cl_loss.item())\n","                gen_losses.append(gen_loss.item())\n","                accuracies_src.append(acc_src)\n","                accuracies_tgt.append(acc_tgt)\n","            except StopIteration: \n","                end_of_epoch = True\n","        # Return average training losses and accuracies for this epoch\n","        return mean(cl_loss), mean(gen_loss), mean(accuracies_src), mean(accuracies_tgt)\n","    \n","    def val_epoch(self, target_loader: DataLoader):\n","        end_of_epoch = False\n","        target_batch_loader = enumerate(target_loader)\n","        accuracies_src = []\n","        accuracies_tgt = []\n","        # Validate current epoch\n","        while not end_of_epoch:\n","            try:\n","                # Get next batch for both source and target\n","                (X_target, y_target) = next(target_batch_loader)[1]\n","                # Apply validation step\n","                acc_src, acc_tgt = self.val_step(X_target, y_target)\n","                # Append accuracies\n","                accuracies_src.append(acc_src)\n","                accuracies_tgt.append(acc_tgt)\n","            except StopIteration: \n","                end_of_epoch = True\n","        # Return average validation accuracies for this epoch\n","        return mean(accuracies_src), mean(accuracies_tgt)\n","    \n","    def train_validate(self, source_train: DataLoader, target_train: DataLoader, target_val: DataLoader):\n","        tr_cl_losses = []\n","        tr_gen_losses = []\n","        tr_src_accs = []\n","        tr_tgt_accs = []\n","        val_src_accs = []\n","        val_tgt_accs = []\n","        min_acc_tgt = self._INF\n","        patience = self.patience\n","        epochs_iter = tqdm(\n","            range(self.tot_epochs), \n","            unit = \"epoch\",\n","            desc = \"TRAINING\")\n","        # Train and validate for each epoch\n","        for epoch in epochs_iter:\n","            self.curr_epoch = epoch\n","            # Train current epoch\n","            cl_loss, gen_loss, acc_src, acc_tgt = self.train_epoch(source_train, target_train)\n","            # Store training results\n","            tr_cl_losses.append(cl_loss)\n","            tr_gen_losses.append(gen_loss)\n","            tr_src_accs.append(acc_src)\n","            tr_tgt_accs.append(acc_tgt)\n","            # Show training results\n","            epochs_iter.set_postfix({\n","                \"train_classifier_loss\": round(cl_loss, 3), \n","                \"train_generator_loss\": round(gen_loss, 3),\n","                \"train_accuracy_on_source_half\": round(acc_src, 3),\n","                \"train_accuracy_on_target_half\": round(acc_tgt, 3)\n","            })\n","            # Validate current epoch\n","            acc_src, acc_tgt = self.val_epoch(target_val)\n","            # Store validation results\n","            val_src_accs.append(acc_src)\n","            val_tgt_accs.append(acc_tgt)\n","            # Show validation results\n","            epochs_iter.set_postfix({\n","                \"val_accuracy_on_source_half\": round(acc_src, 3),\n","                \"val_accuracy_on_target_half\": round(acc_tgt, 3)\n","            })\n","            # Manage patience for early-stopping\n","            if acc_tgt > min(val_tgt_accs[-self._LAST:]):\n","                # Decrease current patience\n","                patience = patience - 1\n","                print(f'\\n--- PATIENCE={patience} ---') \n","                if patience == 0:\n","                    print('\\n--- EARLY STOPPING ---') \n","                    break # Interrupt iteration\n","            else:\n","                # Reset current patience\n","                patience = self.patience\n","                if acc_tgt < min_acc_tgt:\n","                    min_acc_tgt = acc_tgt\n","                    self.loader.model_save(\n","                        model = self.model.model,\n","                        optimizer = self.model.optim)\n","                print('--- SAVED NEW BEST MODEL ---')\n","        # TODO: Save parameters + training and validation history\n","        self.loader.save_hyperparam(params_dict) # da definire il params_dict \n","\n","    def overall_losses(self, y_source_pred, y_target_pred, y_source_true) -> tuple[Tensor, Tensor]:\n","        # Source task classifier loss\n","        self.src_task_class_loss.y_labels = y_source_true\n","        _src_task_class_loss = self.src_task_class_loss(y_source_pred)\n","        # (Cross-domain) Target task classifier loss\n","        self.tgt_task_class_loss.y_labels = y_source_true\n","        _tgt_task_class_loss = self.tgt_task_class_loss(y_source_pred)\n","        # Domain discrimination loss\n","        _src_dom_discrim_loss = self.src_dom_discrim_loss(y_source_pred)\n","        _tgt_dom_discrim_loss = self.tgt_dom_discrim_loss(y_target_pred)\n","        _domain_discrim_loss = TrainingObjectives.domain_discrimination_loss(\n","            _src_dom_discrim_loss, \n","            _tgt_dom_discrim_loss)\n","        # Category-level confusion loss\n","        self.src_cat_conf_loss.y_labels = y_source_true\n","        self.tgt_cat_conf_loss.y_labels = y_source_true\n","        _src_cat_conf_loss = self.src_cat_conf_loss(y_source_pred)\n","        _tgt_cat_conf_loss = self.tgt_cat_conf_loss(y_source_pred)\n","        _category_conf_loss = TrainingObjectives.category_confusion_loss(\n","            _src_cat_conf_loss, \n","            _tgt_cat_conf_loss)\n","        # Domain-level confusion loss\n","        _src_dom_conf_loss = self.src_cat_conf_loss(y_target_pred)\n","        _tgt_dom_conf_loss = self.tgt_cat_conf_loss(y_target_pred)\n","        _domain_conf_loss = TrainingObjectives.domain_confusion_loss(\n","            _src_dom_conf_loss, \n","            _tgt_dom_conf_loss)\n","        # Entropy minimization loss\n","        _tgt_entropy_loss = self.tgt_entropy_loss(y_target_pred)\n","        # Overall classifier loss\n","        _overall_classifier_loss = TrainingObjectives.overall_classifier_loss(\n","            _src_task_class_loss, \n","            _tgt_task_class_loss, \n","            _domain_discrim_loss)\n","        # Overall feature extractor loss\n","        _overall_generator_loss = TrainingObjectives.overall_generator_loss(\n","            _category_conf_loss, \n","            _domain_conf_loss, \n","            _tgt_entropy_loss, \n","            self.curr_epoch, \n","            self.tot_epochs)\n","        # Return obtained overall losses\n","        return _overall_classifier_loss, _overall_generator_loss"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Lpm0K41_0G9t"},"outputs":[],"source":["generator = FeatureExtractor(n_classes, model=model, optimizer=optim, freeze=False)\n","symnet = ModelTrainer(model=generator, n_classes=n_classes, epochs=n_epochs)\n","symnet.train_validate(source_train_loader, target_train_loader, target_val_loader)"]},{"cell_type":"markdown","metadata":{"id":"Jwa_O2xT0G9u"},"source":["\n","# Source-Only Architecture\n","\n","Suppose you’re working on the direction product → real world. Then the first thing you will do is train your model on $P_{train}$. Since this is your <i>source domain </i>, you are allowed to use label information (e.g. use a cross entropy loss in your training step). In your test step, you are going to evaluate the model on $RW_{test}$. This will achieve a certain accuracy; since we only trained on the source domain, and not on the target domain, this accuracy refers to the source-only scenario. We call it $acc_{so}$. Now you want to evaluate your UDA component which, differently from the former case, implies training on the target domain. Since you are not allowed to use labels there, here you will use any UDA device of your choice. So, in this case, in your training step, you will train supervisedly on $P_{train}$ (like you did before) and simultaneously train unsupervisedly on $RW_{train}$. In your test step, once again, you want to evaluate on $RW_{test}$. This will achieve a new accuracy $acc_{uda}$, which hopefully will be higher than $acc_{so}$ since this time you also trained on the target domain, even if without label information. At this point you can compute your gain $G$:\n","$$G = acc_{uda} − acc_{so}$$"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"s7bKkf9g0G9u"},"outputs":[],"source":["class SourceModelTrainer:\n","    _INF = 1e20 \n","    \n","    def __init__(self, model: FeatureExtractor, loader_so: ModelLoader, n_classes: int, epochs: int):\n","        \"\"\"Initialize the SymsNet model\n","        Args:\n","            model (FeatureExtractor): _description_\n","            n_classes (int): _description_\n","            epochs (int): _description_\n","        \"\"\"\n","        self.patience = 10 \n","        self.tot_epochs = epochs\n","        self.n_classes = n_classes\n","        self.loader_so = loader_so\n","        self.model = model \n","        self.loss = CrossEntropyLoss()\n","            \n","    def train_step(self, X_source: Tensor, y_source: Tensor):\n","        # Tell model go training mode\n","        self.model.model.train()\n","        # Compute features for both inputs\n","        X_source_features = self.model.model(X_source)\n","        # Compute overall training objective losses\n","        general_loss = self.loss(X_source_features, y_source)\n","        # Compute gradients w.r.t. classifier loss\n","        self.model.optim.zero_grad()\n","        general_loss.backward()\n","        # Perform optimizer step    \n","        self.model.optim.step()\n","        target = y_source.clone().tolist()\n","        preds = X_source_features.clone()\n","        return general_loss, accuracy_score(target, torch.argmax(preds, dim=1))\n","        \n","    \n","    def train_epoch(self, source_dataloader: DataLoader):\n","        end_of_epoch = False\n","        source_batch_loader = enumerate(source_dataloader)\n","        gen_loss = []\n","        acc = []\n","        # Train for current epoch\n","        while not end_of_epoch:\n","            try:\n","                # Get next batch for both source and target\n","                (X_source, y_source) = source_batch_loader.__next__()[1]\n","                loss, accuracy = self.train_step(X_source, y_source)\n","                gen_loss.append(loss.item())\n","                acc.append(accuracy)\n","            except StopIteration: \n","                end_of_epoch = True\n","        return mean(gen_loss), mean(acc)\n","    \n","    def val_step(self, X_target: Tensor, y_target: Tensor):\n","        # Tell model go validation mode\n","        self.model.model.eval()\n","        # Get outputs for both inputs\n","        with torch.no_grad():\n","            y_target_pred = self.model.model(X_target)\n","        # Calculate accuracy\n","        y_target_true = y_target.clone().tolist()\n","        y_target_pred = y_target_pred.clone()\n","        acc_so = accuracy_score(y_target_true, torch.argmax(y_target_pred, dim=1))\n","        return acc_so\n","    \n","    def val_epoch(self, target_dataloader: DataLoader):\n","        end_of_epoch = False\n","        target_batch_loader = enumerate(target_dataloader)\n","        accuracies = []\n","        \n","        # Validate current epoch\n","        while not end_of_epoch:\n","            try:\n","                # Get next batch for both source and target\n","                (X_target, y_target) = next(target_batch_loader)[1]\n","                # Apply validation step\n","                acc = self.val_step(X_target, y_target)\n","                # Append accuracies\n","                accuracies.append(acc)\n","                \n","            except StopIteration: \n","                end_of_epoch = True\n","        # Return average validation accuracies for this epoch\n","        return mean(accuracies)\n","        \n","            \n","    def train_validate(self, source_dataloader: DataLoader, target_dataloader: DataLoader):\n","        general_loss = []\n","        accuracies_source = []\n","        val_accuracies = []\n","        min_acc = self._INF\n","        patience = self.patience \n","        epochs_iter = tqdm(\n","            range(self.tot_epochs), \n","            unit = \"epoch\",\n","            desc = \"SOURCE-ONLY TRAINING\")\n","        for e in epochs_iter:\n","            # Train current epoch\n","            loss, acc = self.train_epoch(source_dataloader)\n","            \n","            # Store training results \n","            general_loss.append(loss)\n","            accuracies_source.append(acc)\n","            \n","            # Show training results\n","            logs = {\n","                \"general_loss\": round(loss, 3), \n","                \"accuracy\": round(acc,3) \n","                }\n","            epochs_iter.set_postfix(logs)\n","            \n","            # Validate current epoch\n","            acc_val = self.val_epoch(target_dataloader)\n","            \n","            # Store validation result \n","            val_accuracies.append(acc_val)\n","            \n","            # Show validation results\n","            epochs_iter.set_postfix({\n","                \"val_accuracy_domain_shift\": round(acc_val, 3)\n","            })\n","\n","            # Manage patience for early-stopping\n","            if acc_val > min(val_accuracies[-self._LAST:]):\n","                # Decrease current patience\n","                patience = patience - 1\n","                print(f'\\n--- PATIENCE={patience} ---') \n","                if self.patience == 0:\n","                    print('\\n--- EARLY STOPPING ---') \n","                    break # Interrupt iteration\n","            else:\n","                # Reset current patience\n","                patience = self.patience\n","                if acc_val < min_acc:\n","                    min_acc = acc_val\n","                    self.loader_so.model_save(\n","                        model = self.model.model,\n","                        optimizer = self.model.optim)\n","                print('--- SAVED NEW BEST MODEL ---')\n","        # TODO: Save parameters + training and validation history\n","        self.loader.save_hyperparam(params_dict) # da definire il params_dict "]},{"cell_type":"code","execution_count":null,"metadata":{"id":"u4P3VCrH0G9u"},"outputs":[],"source":["source_only_generator = FeatureExtractor(n_classes=n_classes, model=model, optimizer=optim, freeze=False, source_only=True)\n","source_only_classifier = SourceModelTrainer(model=generator, n_classes=n_classes, epochs=n_epochs)\n","source_only_classifier.train(source_train_loader)"]},{"cell_type":"markdown","metadata":{"id":"UTFRGI4mEo3A"},"source":["# Overall Gain"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Ez-7_SG70G9v"},"outputs":[],"source":["# TODO: definire classe? per il GAIN\n","# Overall gain with source-only model and UDA model\n","def overall_gain(acc_so, acc_uda):\n","    return (acc_uda - acc_so)*100"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"DZdpu2Wn0G9v"},"outputs":[],"source":["print(overall_gain(acc_so, acc_uda))\n","print(acc_so)\n","print(acc_uda)"]}],"metadata":{"accelerator":"GPU","colab":{"collapsed_sections":["W5if2JDi0ic6","veDaOcVb0G9h","eG_KkWoF1iLA","zzwgGaFW2VpR","QhWEa-5C0G9l","ya6nhiHiDhkM","PzHiEzWR0G9p","lS8yTMu10G9r"],"machine_shape":"hm","name":"SymNets.ipynb","provenance":[{"file_id":"https://github.com/detch-for-shor/deep-learning-proj/blob/main/symNets.ipynb","timestamp":1658945225451}],"toc_visible":true},"gpuClass":"standard","kernelspec":{"display_name":"Python 3.9.13 ('venv': venv)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.13"},"orig_nbformat":4,"vscode":{"interpreter":{"hash":"6f0d290f0742685d306541f8dcebbe79a177e37269f78587a0fc5052fa8d446c"}}},"nbformat":4,"nbformat_minor":0}
