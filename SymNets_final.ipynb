{{
      "cell_type": "markdown",
      "metadata": {
        "id": "T9uO6jyHEeKs"
      },
      "source": [
        "# UDA Architecture "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PzHiEzWR0G9p"
      },
      "source": [
        "## Losses"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The SymNet architecture presents an overall training function based on compositionality. Hence, two different modules are embedded, following the original implementation of [Zhang](https://arxiv.org/abs/1904.04663) <i>et al.</i> (see § 3.3). \n",
        "\n",
        "$$ \\displaystyle\\min_{C^s, C^t, C^{st}} \\large \\mathcal{E}_{task}^{(s)}(G, C^{(s)}) +  \\large\\mathcal{E}_{task}^{(t)}(G, C^{(t)}) + \\large\\mathcal{E}_{domain}^{(st)}(G, C^{(st)})$$\n",
        "$$\\displaystyle\\min_{G} \\large \\mathcal{F}_{category}^{(st)}(G, C^{(st)}) + \\lambda [\\large \\mathcal{F}_{domain}^{(st)}(G, C^{(st)}) + \\large \\mathcal{M}^{(st)}(G, C^{(st)})]$$\n",
        "\n",
        "The multiple elements (i.e., losses) have been organized in a tree-wise structure, so to avoid redundancy in the code and provide a user-friendly objected-oriented framework. The root is the class <code>_Loss</code>, which inherits from the <code>[torch.nn.Module](https://pytorch.org/docs/stable/generated/torch.nn.Module.html)</code>. Its two children are the <code>EntropyMinimizationLoss</code> and <code>SplitLoss</code>. Whereas the former is used to implement the <i>Entropy Minimization Principle</i> (§ 3.2.1 [Zhang](https://arxiv.org/abs/1904.04663) <i>et al.</i>), the latter needed further submodules and brings together the functionalities of different losses, which can are characterized by the two couples of mutually exclusive features: <code>source/target</code> and <code>split_first/split_after</code>. The inheritance is represented by the tree below:\n",
        "\n",
        "```\n",
        "Loss\n",
        "│\n",
        "├── EntropyMinimizationLoss\n",
        "│ \n",
        "└── SplitLoss\n",
        "    │   \n",
        "    └── SplitCrossEntropyLoss\n",
        "    |\n",
        "    └── DomainDiscriminationLoss\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 82,
      "metadata": {
        "id": "66HGz_c30G9p"
      },
      "outputs": [],
      "source": [
        "class _Loss(nn.Module):\n",
        "    '''\n",
        "    This is a base abstract class representing a generic loss.\n",
        "    A few methods of common use are implemented for the benefit\n",
        "    of the child classes.\n",
        "    \n",
        "    Methods\n",
        "    -------\n",
        "    add_threshold:\n",
        "        Adds threshold to avoid log(0) cases.\n",
        "    to_softmax:\n",
        "        Computes post-softmax tensor on a \n",
        "        given output (features) tensor.\n",
        "    '''\n",
        "    \n",
        "    _THRESHOLD = 1e-20\n",
        "    \n",
        "    def __init__(self):\n",
        "        super(_Loss, self).__init__()\n",
        "        \n",
        "    def forward(self, input: Tensor):\n",
        "        prob = self.to_softmax(input)\n",
        "        loss = self.compute_loss(prob)\n",
        "        if cuda.is_available():\n",
        "            loss = loss.cuda()\n",
        "        return loss\n",
        "        \n",
        "    ### @final\n",
        "    def add_threshold(self, prob: Tensor):\n",
        "        '''\n",
        "        Check whether the probability distribution after the softmax \n",
        "        is equal to 0 in any cell. If this holds, a standard threshold\n",
        "        is added in order to avoid log(0) case. \n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        prob: Tensor\n",
        "            output tensor of the softmax operation\n",
        "\n",
        "        Returns\n",
        "        -------\n",
        "        Tensor\n",
        "            updated tensor (in case the condition above holds)\n",
        "        '''\n",
        "        zeros = (prob == 0)\n",
        "        if torch.any(zeros):\n",
        "            thre_tensor = torch.zeros(zeros.shape)\n",
        "            if cuda.is_available():\n",
        "                thre_tensor = thre_tensor.cuda()\n",
        "            thre_tensor[zeros] = self._THRESHOLD\n",
        "            prob = prob + thre_tensor\n",
        "        if cuda.is_available():\n",
        "            prob = prob.cuda()\n",
        "        return prob\n",
        "    \n",
        "    def to_softmax(self, features: Tensor):\n",
        "        '''\n",
        "        Apply the softmax operation on the features tensor \n",
        "        (i.e., the output of a feature extractor).\n",
        "        \n",
        "        Parameters\n",
        "        ----------\n",
        "        features: Tensor\n",
        "            output of a feature extractor (assuming that dim=1\n",
        "            is as long as the number of classes in your task)\n",
        "\n",
        "        Returns\n",
        "        -------\n",
        "        Tensor\n",
        "            probability distribution with (possible) threshold\n",
        "        '''\n",
        "        prob = F.softmax(features, dim=1)\n",
        "        prob = self.add_threshold(prob)\n",
        "        if cuda.is_available():\n",
        "            prob = prob.cuda()\n",
        "        return prob\n",
        "    \n",
        "    @abstractmethod\n",
        "    def compute_loss(self, prob: Tensor):\n",
        "        pass"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Entropy Minimization Principle"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The <b>Entropy Minimization</b> objective is here ([Zhang](https://arxiv.org/abs/1904.04663) <i>et al.</i>, § 3.2.1) adopted as regularizer. Specifically, it updates the feature extractor (<i>G</i>) and enhances the discrimination among task categories. This enhancement is achieved by minimizing the entropy of the total (sum) probabiility of a category: \n",
        "\n",
        "$$\\displaystyle\\min_{G}\\mathcal{M}^{(st)}(G, C^{(t)}) = -\\frac{1}{n_t}\\sum_{j=1}^{n_t}\\sum_{k=1}^{K}q_k^{(st)}(x_j^{(t)})log\\bigg(q_k^{(st)}(x_j^{(t)})\\bigg)$$\n",
        "\n",
        "$$where: \\quad \\displaystyle q_k^{(st)}(x_j^{(t)}) = p_k^{(st)}(x_j^{(t)}) + p_{k+K}^{(st)}(x_j^{(t)}) \\quad k\\in{1,...,K}$$\n",
        "\n",
        "We propose a possible interpretation for this regularization approach. We argue that minimizing the entropy of the total probability (which can be seen here as a <i>confidence</i>) over the source and target classifiers actually corresponds to minimizing the <i>surprise</i> over the the source and target outcomes for each task category $k$. Hence, this approach seems to enforce a <i>balanced</i> training behavior between source and target. In fact, if any of them was left behind in favor of the other one &ndash; that is, for example, being strongly <i>overconfident</i> about the source with $p_k^{(st)}(\\underline{x}^{(t)}) \\approx 1.0$ while being strongly <i>underconfident</i> about the target with $p_{k+K}^{(st)}(\\underline{x}^{(t)}) \\approx 0.0$ or vice versa &ndash; then the entropy minimization objective function $\\mathcal{M}^{(st)}(G, C^{(t)})$ will return zero at best. But this is very far from its minimum, which is clearly a negative value. Hence, it seems reasonable to say that the constraint of minimizing this function encourages the architecture to get good confidence results for both source and target, and thus discourages any unbalanced training behavior."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 83,
      "metadata": {
        "id": "EhXN9HF-0G9q"
      },
      "outputs": [],
      "source": [
        "class EntropyMinimizationLoss(_Loss):\n",
        "    '''\n",
        "    This class implements the loss for \n",
        "    the entropy minimization principle\n",
        "    '''\n",
        "    \n",
        "    def __init__(self, n_classes: int):\n",
        "        super(EntropyMinimizationLoss, self).__init__()\n",
        "        self.n_classes = n_classes\n",
        "    \n",
        "    ### @overrides\n",
        "    def compute_loss(self, prob: Tensor):\n",
        "        prob_source = prob[:, :self.n_classes]\n",
        "        prob_target = prob[:, self.n_classes:]\n",
        "        prob_sum = prob_source + prob_target\n",
        "        loss = -(prob_sum.log().mul(prob_sum).sum(dim=1).mean())\n",
        "        if cuda.is_available():\n",
        "            loss = loss.cuda()\n",
        "        return loss"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Cross-Entropy-based Losses"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The losses contained in the overall training objective described aforeahead are implemented starting from the class <code>CrossEntropyLoss</code>. Thus, they are either standard cross-entropy losses or a combination of two losses (i.e., <i>two-way cross entropy losses</i>). More specifically, two classifiers have been implemented to solve the classification task on the <i>source</i> ($C^{(s)}$) and <i>target</i> ($C^{(t)}$) domains respectively. A <b>task classifier</b> $C^{(s)}$ is trained using the following cross-entropy loss over the <i>labeled</i> source samples:\n",
        "\n",
        "$$ \\displaystyle\\min_{C^{(s)}}\\mathcal{E}_{task}^{(s)}(G, C^{(s)})=-\\frac{1}{n_{s}}\\sum_{i=1}^{n_{s}}log\\bigg(p_{y_{i}^{(s)}}^{(s)}({\\bf x}_{i}^{(s)})\\bigg) $$\n",
        "\n",
        "In the formula above, $G$ represents the <b>feature extractor</b>, ${\\bf x}_{i}^{(s)}$ is a source sample, and $p_{y_{i}^{(s)}}^{(s)}({\\bf x}_{i}^{(s)}) \\in [0,1]^{K}$ the distribution of probability after the <code>[softmax](https://pytorch.org/docs/stable/generated/torch.nn.functional.softmax.html)</code> operation. \n",
        "\n",
        "Since target samples are <i>unlabeled</i>, there exists no direct\n",
        "supervision signals to learn a task classifier $C^{(t)}$. Therefore, [Zhang](https://arxiv.org/abs/1904.04663) <i>et al.</i> leverage the <i>labeled</i> source samples to train $C^{(t)}$ for patterns that can be used on the taregt domain as well. This can be achieved by using the following cross-entropy loss:\n",
        "\n",
        "$$ \\displaystyle\\min_{C^{(t)}}\\mathcal{E}_{task}^{(t)}(G, C^{(t)})=-\\frac{1}{n_{s}}\\sum_{i=1}^{n_{s}}log\\bigg(p_{y_{i}^{(s)}}^{(t)}({\\bf x}_{i}^{(s)})\\bigg) $$\n",
        "\n",
        "It is worth noticing that $C^{(t)}$ will be distinguishable from $C^{(s)}$ through the domain discrimination training of the classifier $C^{(st)}$, reported below. Moreover, [Zhang](https://arxiv.org/abs/1904.04663) <i>et al.</i> stress the use of <i>labeled</i> source samples to enhance $C^{(t)}$ performance in discriminating among task categories.\n",
        "\n",
        "$$ \\displaystyle\\min_{C^{(st)}}\\mathcal{E}_{domain}^{(st)}(G, C^{(t)})=-\\frac{1}{n_{t}}\\sum_{j=1}^{n_t}log\\bigg(\\sum_{k=1}^{K} p_{k+K}^{(st)}({\\bf x}_{j}^{(t)})\\bigg)-\\frac{1}{n_s}\\sum_{i=1}^{n_s}log\\bigg(\\sum_{k=1}^{K}p_{k}^{(st)}({\\bf x}_{i}^{(s)})\\bigg) $$\n",
        "\n",
        "Furthermore a two-level confusion loss is applied. Whereas a first loss is category-level and relies on the source labels, the second loss is domain-level and focuses on the target: \n",
        "\n",
        "$$ \\displaystyle\\min_{G}\\mathcal{F}_{category}^{(st)}(G, C^{(t)})=-\\frac{1}{2n_{s}}\\sum_{i=1}^{n_s}log(p_{y_i^s+K}^{(st)}({\\bf x}_{i}^{(s)}))-\\frac{1}{2n_s}\\sum_{i=1}^{n_s}log(p_{y_i^s}^{(st)}({\\bf x}_{i}^{(s)})) $$\n",
        "\n",
        "$$ \\displaystyle\\min_{G}\\mathcal{F}_{domain}^{(st)}(G, C^{(t)})=-\\frac{1}{2n_{t}}\\sum_{j=1}^{n_t}log\\bigg(\\sum_{k=1}^{K} p_{k+K}^{(st)}({\\bf x}_{j}^{(t)})\\bigg)-\\frac{1}{2n_t}\\sum_{j=1}^{n_t}log\\bigg(\\sum_{k=1}^{K}p_{k}^{(st)}({\\bf x}_{j}^{(t)})\\bigg) $$"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 84,
      "metadata": {
        "id": "C5zkjQbG0G9q"
      },
      "outputs": [],
      "source": [
        "class SplitLoss(_Loss):\n",
        "    '''\n",
        "    This abstract class represents a generic loss\n",
        "    such that the features array needs to be splitted\n",
        "    either before or after the softmax function is applied\n",
        "    \n",
        "    Methods\n",
        "    -------\n",
        "    split_vector:\n",
        "        returns the proper half of the input array \n",
        "        according to initialization settings\n",
        "    '''\n",
        "    \n",
        "    def __init__(self, n_classes: int, source: bool, split_first: bool):\n",
        "        super(SplitLoss, self).__init__()\n",
        "        self.n_classes = n_classes\n",
        "        self._is_source = source\n",
        "        self._split_first = split_first\n",
        "    \n",
        "    ### @overrides\n",
        "    def to_softmax(self, features: Tensor):\n",
        "        if self._split_first:\n",
        "            prob = self.split_vector(features)\n",
        "            prob = F.softmax(prob, dim=1)\n",
        "        else:\n",
        "            prob = F.softmax(features, dim=1)\n",
        "            prob = self.split_vector(prob)\n",
        "        prob = self.add_threshold(prob)\n",
        "        if cuda.is_available():\n",
        "            prob = prob.cuda()\n",
        "        return prob\n",
        "    \n",
        "    ### @final\n",
        "    def split_vector(self, prob: Tensor):\n",
        "        return prob[:,:self.n_classes] if self._is_source else prob[:,self.n_classes:]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 85,
      "metadata": {
        "id": "aDGpTlic0G9q"
      },
      "outputs": [],
      "source": [
        "class SplitCrossEntropyLoss(SplitLoss):\n",
        "    '''\n",
        "    This class realizes the abstract class SplitLoss\n",
        "    for the case in which a cross-entropy loss should be\n",
        "    applied between the softmax and the ground truth\n",
        "    \n",
        "    Properties\n",
        "    ----------\n",
        "    y_labels:\n",
        "        ground truth is transformed properly \n",
        "        so to ease the accuracy computation\n",
        "    '''\n",
        "    \n",
        "    def _get_y_labels(self) -> Tensor:\n",
        "        return self._y_labels\n",
        "    def _set_y_labels(self, y_labels: list):\n",
        "        # Set the proper device \n",
        "        dev = 'cuda:0' if cuda.is_available() else 'cpu:0'\n",
        "        # Transform the target labels into a long tensor object\n",
        "        y_labels_tns = torch.tensor(y_labels, dtype=torch.long, device=dev)\n",
        "        # One-hot encode the target labels \n",
        "        self._y_labels = F.one_hot(y_labels_tns, num_classes=self.n_classes)\n",
        "        # Cast the one-hot-encoded long tensor object into a float tensor object\n",
        "        # to ensure type compatibility with the probabilities tensor object (i.e., prob)\n",
        "        self._y_labels = self._y_labels.type(torch.float)\n",
        "    y_labels = property(fget=_get_y_labels, fset=_set_y_labels)\n",
        "    \n",
        "    def __init__(self, n_classes: int, source: bool, split_first: bool):\n",
        "        super(SplitCrossEntropyLoss, self).__init__(n_classes, source, split_first)\n",
        "        # TODO: Apply label smoothing for ... \n",
        "        self.cross_entropy_loss = CrossEntropyLoss(reduction='mean', label_smoothing=0.2)\n",
        "        if cuda.is_available():\n",
        "            self.cross_entropy_loss = self.cross_entropy_loss.cuda()\n",
        "    \n",
        "    ### @overrides\n",
        "    def compute_loss(self, prob: Tensor):\n",
        "        loss = self.cross_entropy_loss(prob, self.y_labels)\n",
        "        if cuda.is_available():\n",
        "            loss = loss.cuda()\n",
        "        return loss"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "TODO: define why the label smoothing has been introduced"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 86,
      "metadata": {
        "id": "9WJRrEK20G9r"
      },
      "outputs": [],
      "source": [
        "class DomainDiscriminationLoss(SplitLoss):\n",
        "    '''\n",
        "    This class realizes the abstract class SplitLoss\n",
        "    for the case in which a cross-entropy loss should be\n",
        "    applied on the total probability for one of the two\n",
        "    domains (hence, now the input of the cross-entropy is \n",
        "    actually the probability of classifying a sample as \n",
        "    belonging to that domain)\n",
        "    '''\n",
        "    \n",
        "    def __init__(self, n_classes: int, source: bool):\n",
        "        super(DomainDiscriminationLoss, self).__init__(n_classes, source, False)\n",
        "        \n",
        "    ### @overrides\n",
        "    def compute_loss(self, prob: Tensor):\n",
        "        loss = -(prob.sum(dim=1).log().mean())\n",
        "        if cuda.is_available():\n",
        "            loss = loss.cuda()\n",
        "        return loss"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Accordingly to the [Zhang](https://arxiv.org/abs/1904.04663) <i>et al.</i> (see § 3.3) definition of the overall model's objective, the class <code>TrainingObjectives</code> recalls the aforementioned losses. Moreover, the $\\lambda$ trade-off parameter is introduced to suppress noisy signals for domain confusion loss and entropy. $\\lambda$ value depends on the number of epochs, as it is iteratively computed through the following formula:\n",
        "$$\\lambda = \\frac{2}{1 + e^{(- \\gamma \\cdot \\frac{ep}{n_{ep}})}} - 1$$\n",
        "where $\\gamma$ is usually set to $10$ ([Zhang](https://arxiv.org/abs/1904.04663) <i>et al.</i>, see § 4.1) and $\\frac{ep}{n_{ep}}$ is iteratively updated at each epoch, as it represents the current epoch over the total. Therefore, $\\lambda$ parameter will start from $0$ and gradually increase (i.e., $\\displaystyle\\lim_{ep \\rightarrow n_{ep}}\\lambda = 1$). Thus, the penalty provided by $\\lambda$ on domain confusion loss and entropy decreases over time, coherently with the fact that the latter might be pretty noisy in the first few epochs, and thus they shouldn't be much relevant on the overall generator loss. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 87,
      "metadata": {
        "id": "rSp7_hu20G9r"
      },
      "outputs": [],
      "source": [
        "class TrainingObjectives:\n",
        "    \n",
        "    @staticmethod\n",
        "    def domain_discrimination_loss(src_dom_discrim_loss, tgt_dom_discrim_loss):\n",
        "        return src_dom_discrim_loss + tgt_dom_discrim_loss\n",
        "    \n",
        "    @staticmethod\n",
        "    def category_confusion_loss(src_cat_conf_loss, tgt_cat_conf_loss):\n",
        "        return 0.5 * (src_cat_conf_loss + tgt_cat_conf_loss)\n",
        "    \n",
        "    @staticmethod\n",
        "    def domain_confusion_loss(src_dom_conf_loss, tgt_dom_conf_loss):\n",
        "        return 0.5 * (src_dom_conf_loss + tgt_dom_conf_loss)\n",
        "    \n",
        "    @staticmethod\n",
        "    def overall_classifier_loss(src_task_class_loss, tgt_task_class_loss, domain_discrim_loss):\n",
        "        return src_task_class_loss + tgt_task_class_loss + domain_discrim_loss\n",
        "    \n",
        "    @staticmethod\n",
        "    def overall_generator_loss(cat_conf_loss, dom_conf_loss, tgt_entropy_loss, curr_epoch, tot_epochs):\n",
        "        # trade off parameter as formula above \n",
        "        lambda_trade_off = 2 / (1 + math.exp(-1 * 10 * curr_epoch / tot_epochs)) - 1\n",
        "        return cat_conf_loss + lambda_trade_off * (dom_conf_loss + tgt_entropy_loss)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lS8yTMu10G9r"
      },
      "source": [
        "## Feature Extractor"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The feature extractor embeds the model responsible for the computation of the latent representation of the images. For many image-related tasks, <code>ResNet</code> is a quite popular choice. Here we modify the output layer in order to allow for <b>Unsupervised Domain Adaptation</b> (<b>UDA</b>). More specifically, the linear output layer is duplicated in order to represent the results for the two domain-specific classification tasks: on source and target respectively. Each half is as long as the number of categories ($=20$ for this specific application).\n",
        "\n",
        "A <b>dropout layer</b> is applied to the latent representations before the linear output layer. The reason behind this choice is both empirical and architectural: After a few attempts, we noticed that the model was always ending up relying on just a small number of recurrent patterns. In some cases, it even happened to collapse in a statein which it returned the very same latent representation for each image, if for images belonging to different categories. This seems compatible with the well-known <b>mode collapse</b> phenomenon, which is typical of the adversarial-based architectures. In order to tackle this issue, we introduced a dropout layer, together with <b>label smoothing</b> in the cross-entropy-based losses. This combined approach solved the issue immediately.\n",
        "\n",
        "Finally, we introduced <b>layer-wise learning rate decay</b> as an alternative to <b>freezing</b> the first layers. In the first case, we assign a different learning rate to each parameter (here, group of parameters). We designed a decay function that assigns learning rates according to the following map:\n",
        "\n",
        "$$ \\displaystyle lr(x) = lr_{max} \\cdot \\sigma\\bigg(\\frac{k}{n-1} \\cdot x - \\frac{k}{2}\\bigg) = \\frac{lr_{max}}{1+\\mathcal{e}^{-(\\frac{k}{n-1} \\cdot x - \\frac{k}{2})}} $$\n",
        "\n",
        "Where $n$ is number of parameters (here, groups of parameters), $lr_{max}$ is the maximum possible learning rate assigned, and $k=10$ is a fixed coefficient, chosen so to guarantee that the outer layers have $\\approx 100%$ the maximum learning rate applied. Moreover &ndash; no matter the choice of $k$ &ndash; we will have that $x=\\frac{n-1}{2}$ implies exactly $lr(x)=\\frac{lr_{max}}{2}$. Also, it's worth noticing that this function shows a central symmetry &ndash; thanks to the use of the logistic function &ndash; therefore the higher the percentage of maximum learning rate applied to the outer layers, the lesser the percentage for the inner ones."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "AfuKcRs50G9r"
      },
      "outputs": [],
      "source": [
        "class FeatureExtractor:\n",
        "    \n",
        "    def __init__(self, n_classes: int, model='resnet50', optimizer='rmsprop', n_params_trained=None, lr=0.01, weight_decay=0.0, dropout=0.0, source_only=False):\n",
        "        '''\n",
        "        This class implements the internal model (a.k.a. feature extractor).\n",
        "        For image classification tasks, ResNet is a popular choice.\n",
        "        This class slightly differs from pretrained model due to a modified\n",
        "        output layer, which allows for unsupervised domain adaptation (UDA).\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        n_classes: int\n",
        "            number of classes in the dataset\n",
        "        model: str, optional\n",
        "            pretrained model to import as feature extractor [default='resnet50']\n",
        "        optimizer: str, optional\n",
        "            optimizer to apply on the model [default='rmsprop']\n",
        "        n_params_trained: int, optional\n",
        "            number of parameters (i.e., layers to be trained) [default=None]\n",
        "        lr: float, optional\n",
        "            learning rate [default=0.01]\n",
        "        weight_decay: float, optional\n",
        "            weight decay [default=0.0]\n",
        "        dropout: float, optional\n",
        "            dropout to apply to the latent representation \n",
        "            before linear output is applied [default=0.0]\n",
        "        source_only: boolean, optional\n",
        "            source-only training mode requires output \n",
        "            as long as n_classes [default=False]\n",
        "        '''\n",
        "        self.learning_rate = lr\n",
        "        self.weight_decay = weight_decay\n",
        "        self.dropout = dropout\n",
        "        \n",
        "        # Upload pretrained model \n",
        "        if model.lower() == 'resnet18': \n",
        "            self.model = models.resnet18(pretrained=True)\n",
        "        elif model.lower() == 'resnet50': \n",
        "            self.model = models.resnet50(pretrained=True)\n",
        "        else:\n",
        "            raise ValueError('Unknown model')\n",
        "        \n",
        "        # Modify last layer (classification)\n",
        "        n_feat_in = self.model.fc.in_features\n",
        "        n_feat_out = 2*n_classes if not source_only else n_classes\n",
        "        \n",
        "        self.model.fc = nn.Sequential(\n",
        "            nn.Dropout(p=self.dropout, inplace=True),\n",
        "            nn.Linear(n_feat_in, n_feat_out))\n",
        "        \n",
        "        if cuda.is_available():\n",
        "            self.model = self.model.cuda()\n",
        "\n",
        "        count = 0 \n",
        "        n_params = len(list(self.model.parameters()))\n",
        "        first_param_trained = n_params - n_params_trained if n_params_trained else 0\n",
        "        if n_params_trained is not None:\n",
        "            # Freeze first layers\n",
        "            for param in self.model.parameters():\n",
        "                param.requires_grad = (count >= first_param_trained)\n",
        "                count = count + 1 \n",
        "            params_to_train = filter(lambda p: p.requires_grad, self.model.parameters())\n",
        "        else:\n",
        "            # Layer-wise Learning Rate Decay\n",
        "            i = -1 \n",
        "            params_to_train = []\n",
        "            name_prev_group = None\n",
        "            groups = set([name.split('.')[0] for name, _ in self.model.named_parameters()])\n",
        "            for name, param in self.model.named_parameters():\n",
        "                name_cur_group = name.split('.')[0]\n",
        "                if name_cur_group != name_prev_group or name_prev_group is None:\n",
        "                    i = i + 1\n",
        "                    # NOTE: At half groups corresponds half learning rate\n",
        "                    lr_group = self.decay(i, len(groups)-1, self.learning_rate)\n",
        "                name_prev_group = name_cur_group\n",
        "                params_to_train.append({'params': param, 'lr': lr_group})\n",
        "            \n",
        "        # Initialize optimizer\n",
        "        if optimizer.lower() == 'rmsprop':\n",
        "            self.optim = RMSprop(\n",
        "                params = params_to_train,\n",
        "                lr = self.learning_rate,\n",
        "                weight_decay = self.weight_decay)\n",
        "        elif optimizer.lower() == 'adadelta':\n",
        "            self.optim = Adadelta(\n",
        "                params = params_to_train,\n",
        "                lr = self.learning_rate,\n",
        "                weight_decay = self.weight_decay)\n",
        "        elif optimizer.lower() == 'sgd':\n",
        "            self.optim = SGD(\n",
        "                params = params_to_train,\n",
        "                lr = self.learning_rate,\n",
        "                weight_decay = self.weight_decay,\n",
        "                nesterov = True)\n",
        "        else:\n",
        "            raise ValueError('Unknown optimizer')\n",
        "    \n",
        "    def decay(self, index: int, n_groups: int, lr: float):\n",
        "        lr_steep = 10\n",
        "        sigmoid = lambda x: 1/(1 + np.exp(-x)) \n",
        "        return lr * sigmoid(lr_steep/n_groups * (index - (n_groups/2))) "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "En10ic2_0G9s"
      },
      "source": [
        "## Loader "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "yci2ngnwbGhp"
      },
      "outputs": [],
      "source": [
        "class ModelLoader:\n",
        "    \n",
        "    def __init__(self, base_dir: str, source_only = False):\n",
        "        self.base_dir = base_dir\n",
        "        self.filename = source.split(\"_\")[0] + \"2\" + target.split(\"_\")[0]\n",
        "        self.fullpath = f'{base_dir}/{self.filename}'\n",
        "        self.source_only = source_only\n",
        "    \n",
        "    def check_base_dir(self):\n",
        "        try:\n",
        "            if not os.path.exists(self.base_dir):\n",
        "                os.makedirs(self.base_dir)\n",
        "            return True\n",
        "        except OSError:\n",
        "            return False\n",
        "\n",
        "    def model_save(self, model, optimizer):\n",
        "        if self.check_base_dir(): \n",
        "            with open(f'{self.fullpath}.pickle', 'wb') as f:\n",
        "                torch.save([model, optimizer], f)\n",
        "        else:\n",
        "            raise Exception('Model saving failed')\n",
        "    \n",
        "    def save_hyperparam(self):\n",
        "        params_dict = {\n",
        "            'seed': seed,\n",
        "            'source': source,\n",
        "            'target': target,\n",
        "            'resize_dim': resize_dim,\n",
        "            'crop_dim': crop_dim,\n",
        "            'grayscale': grayscale,\n",
        "            'crop_center': crop_center,\n",
        "            'batch_size': batch_size,\n",
        "            'test_split': test_split,\n",
        "            'n_classes': n_classes,\n",
        "            'model': model,\n",
        "            'optimizer': optim,\n",
        "            'n_params_trained': n_params_trained,\n",
        "            'weight_decay': weight_decay,\n",
        "            'learning_rate': lr,\n",
        "            'dropout': dropout,\n",
        "            'n_epochs': n_epochs,\n",
        "            'patience': patience\n",
        "        }\n",
        "        if self.check_base_dir():\n",
        "            with open(f'{self.fullpath}_params.json', 'w') as f:\n",
        "                json.dump(params_dict, f, indent=4)\n",
        "        else:\n",
        "            raise Exception('Hyperparams saving failed')\n",
        "            \n",
        "    def save_results(self, results: dict):\n",
        "        if self.check_base_dir():\n",
        "            if self.source_only:\n",
        "                self.fullpath = self.fullpath + \"_so\"\n",
        "            with open(f'{self.fullpath}_results.json', 'w') as f:\n",
        "                json.dump(results, f, indent=4)\n",
        "        else:\n",
        "            raise Exception('Results saving failed')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wfsk22OulkK0"
      },
      "source": [
        "## Trainer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 79,
      "metadata": {
        "id": "xNNN7Mwo0G9s"
      },
      "outputs": [],
      "source": [
        "class ModelTrainer:\n",
        "    \n",
        "    def __init__(self, model: FeatureExtractor, loader: ModelLoader, n_classes=20, epochs=250, patience=10):\n",
        "        \"\"\"Initialize the SymsNet model\n",
        "        Args:\n",
        "            model (FeatureExtractor): _description_\n",
        "            n_classes (int): _description_\n",
        "            epochs (int): _description_\n",
        "        \"\"\"\n",
        "        self.curr_epoch = 0\n",
        "        self.tot_epochs = epochs\n",
        "        self.n_classes = n_classes\n",
        "        self.model = model\n",
        "        self.loader = loader\n",
        "        self.patience = patience \n",
        "        self.src_task_class_loss = SplitCrossEntropyLoss(n_classes=n_classes, source=True, split_first=True)\n",
        "        self.tgt_task_class_loss = SplitCrossEntropyLoss(n_classes=n_classes, source=False, split_first=True)\n",
        "        # Domain discrimination losses\n",
        "        self.src_dom_discrim_loss = DomainDiscriminationLoss(n_classes=n_classes, source=True)\n",
        "        self.tgt_dom_discrim_loss = DomainDiscriminationLoss(n_classes=n_classes, source=False)\n",
        "        # Category-level confusion losses\n",
        "        self.src_cat_conf_loss = SplitCrossEntropyLoss(n_classes=n_classes, source=True, split_first=False)\n",
        "        self.tgt_cat_conf_loss = SplitCrossEntropyLoss(n_classes=n_classes, source=False, split_first=False)\n",
        "        # Domain-level confusion losses\n",
        "        self.src_dom_conf_loss = DomainDiscriminationLoss(n_classes=n_classes, source=True)\n",
        "        self.tgt_dom_conf_loss = DomainDiscriminationLoss(n_classes=n_classes, source=False)\n",
        "        # Entropy minimization loss\n",
        "        self.tgt_entropy_loss = EntropyMinimizationLoss(n_classes=n_classes)   \n",
        "\n",
        "        if cuda.is_available():\n",
        "            # Task classifier losses\n",
        "            self.src_task_class_loss = self.src_task_class_loss.cuda()\n",
        "            self.tgt_task_class_loss = self.tgt_task_class_loss.cuda()\n",
        "            # Domain discrimination losses\n",
        "            self.src_dom_discrim_loss = self.src_dom_discrim_loss.cuda()\n",
        "            self.tgt_dom_discrim_loss = self.tgt_dom_discrim_loss.cuda()\n",
        "            # Category-level confusion losses\n",
        "            self.src_cat_conf_loss = self.src_cat_conf_loss.cuda()\n",
        "            self.tgt_cat_conf_loss = self.tgt_cat_conf_loss.cuda()\n",
        "            # Domain-level confusion losses\n",
        "            self.src_dom_conf_loss = self.src_dom_conf_loss.cuda()\n",
        "            self.tgt_dom_conf_loss = self.tgt_dom_conf_loss.cuda()\n",
        "            # Entropy minimization loss\n",
        "            self.tgt_entropy_loss = self.tgt_entropy_loss.cuda()    \n",
        "\n",
        "    def train_step(self, X_source: Tensor, y_source: Tensor, X_target: Tensor):\n",
        "        # Tell model go training mode\n",
        "        self.model.model.train()\n",
        "        # Compute features for both inputs\n",
        "        y_source_pred = self.model.model(X_source)\n",
        "        y_target_pred = self.model.model(X_target)\n",
        "        # Compute overall training objective losses\n",
        "        classifier_loss, generator_loss = self.overall_losses(\n",
        "            y_source_pred, \n",
        "            y_target_pred, \n",
        "            y_source)\n",
        "        # Compute gradients w.r.t. classifier loss\n",
        "        self.model.optim.zero_grad()\n",
        "        classifier_loss.backward(retain_graph=True)\n",
        "        grad_classifier_tmp = []\n",
        "        for p in self.model.model.parameters():\n",
        "            if p.grad is not None:\n",
        "                grad_classifier_tmp.append(p.grad.data.clone())\n",
        "        # Compute gradients w.r.t. generator loss\n",
        "        self.model.optim.zero_grad()\n",
        "        generator_loss.backward()\n",
        "        grad_generator_tmp = []\n",
        "        for p in self.model.model.parameters():\n",
        "            if p.grad is not None:\n",
        "                grad_generator_tmp.append(p.grad.data.clone())\n",
        "        # Update gradient data for each parameter \n",
        "        count = 0 \n",
        "        appended = 0 \n",
        "        n_classification_params = 2 \n",
        "        n_params = len(list(self.model.model.parameters()))\n",
        "        for p in self.model.model.parameters():\n",
        "            if p.grad is not None:\n",
        "                grad_tmp = p.grad.data.clone()\n",
        "                grad_tmp.zero_() \n",
        "                if count < (n_params - n_classification_params): \n",
        "                    grad_tmp = grad_tmp + grad_generator_tmp[appended]\n",
        "                else: \n",
        "                    grad_tmp = grad_tmp + grad_classifier_tmp[appended]\n",
        "                appended = appended + 1 \n",
        "                p.grad.data = grad_tmp\n",
        "            count = count + 1\n",
        "        # Perform optimizer step    \n",
        "        self.model.optim.step()\n",
        "        # Calculate accuracy\n",
        "        y_source_true = y_source.clone().tolist()\n",
        "        y_source_pred = y_source_pred.clone()\n",
        "        y_source_pred1 = torch.argmax(y_source_pred[:,:self.n_classes], dim=1).tolist()\n",
        "        y_source_pred2 = torch.argmax(y_source_pred[:,self.n_classes:], dim=1).tolist()\n",
        "        acc_on_source_half = accuracy_score(y_source_true, y_source_pred1)\n",
        "        acc_on_target_half = accuracy_score(y_source_true, y_source_pred2)\n",
        "\n",
        "        # Return losses and accuracies\n",
        "        return classifier_loss.item(), generator_loss.item(), acc_on_source_half, acc_on_target_half\n",
        "       \n",
        "    def val_step(self, X_target: Tensor, y_target: Tensor):\n",
        "        # Tell model go validation mode\n",
        "        self.model.model.eval()\n",
        "        # Get outputs for both inputs\n",
        "        with torch.no_grad():\n",
        "            y_target_pred = self.model.model(X_target)\n",
        "        \n",
        "        # Calculate accuracy\n",
        "        y_target_pred = y_target_pred.clone()\n",
        "        y_target_pred1 = torch.argmax(y_target_pred[:,:self.n_classes], dim=1).tolist()\n",
        "        y_target_pred2 = torch.argmax(y_target_pred[:,self.n_classes:], dim=1).tolist()\n",
        "        y_target_true = y_target.clone().tolist()\n",
        "        acc_on_source_half = accuracy_score(y_target_true, y_target_pred1)\n",
        "        acc_on_target_half = accuracy_score(y_target_true, y_target_pred2)\n",
        "\n",
        "        # Return accuracies\n",
        "        return acc_on_source_half, acc_on_target_half\n",
        "        \n",
        "    def train_epoch(self, source_train: DataLoader, target_train: DataLoader):\n",
        "        end_of_epoch = False\n",
        "        source_batch_loader = enumerate(source_train)\n",
        "        target_batch_loader = enumerate(target_train)\n",
        "        gen_losses = []\n",
        "        cl_losses = []\n",
        "        accuracies_src = []\n",
        "        accuracies_tgt = []\n",
        "        # Train current epoch\n",
        "        while not end_of_epoch:\n",
        "            try:\n",
        "                # Get next batch for both source and target\n",
        "                (X_source, y_source) = next(source_batch_loader)[1]\n",
        "                (X_target, _) = next(target_batch_loader)[1]\n",
        "                if cuda.is_available():\n",
        "                    X_source = X_source.cuda()\n",
        "                    y_source = y_source.cuda()\n",
        "                    X_target = X_target.cuda()\n",
        "                # Apply training step\n",
        "                cl_loss, gen_loss, acc_src, acc_tgt = self.train_step(X_source, y_source, X_target)\n",
        "                # Append losses and accuracies\n",
        "                cl_losses.append(cl_loss)\n",
        "                gen_losses.append(gen_loss)\n",
        "                accuracies_src.append(acc_src)\n",
        "                accuracies_tgt.append(acc_tgt)\n",
        "            except StopIteration: \n",
        "                end_of_epoch = True\n",
        "        # Return average training losses and accuracies for this epoch\n",
        "        return mean(cl_losses), mean(gen_losses), mean(accuracies_src), mean(accuracies_tgt)\n",
        "    \n",
        "    def val_epoch(self, target_val: DataLoader):\n",
        "        end_of_epoch = False\n",
        "        target_batch_loader = enumerate(target_val)\n",
        "        accuracies_src = []\n",
        "        accuracies_tgt = []\n",
        "        # Validate current epoch\n",
        "        while not end_of_epoch:\n",
        "            try:\n",
        "                # Get next batch for both source and target\n",
        "                (X_target, y_target) = next(target_batch_loader)[1]\n",
        "                if cuda.is_available():\n",
        "                    X_target = X_target.cuda()\n",
        "                    y_target = y_target.cuda()\n",
        "                # Apply validation step\n",
        "                acc_src, acc_tgt = self.val_step(X_target, y_target)\n",
        "                # Append accuracies\n",
        "                accuracies_src.append(acc_src)\n",
        "                accuracies_tgt.append(acc_tgt)\n",
        "            except StopIteration: \n",
        "                end_of_epoch = True\n",
        "        # Return average validation accuracies for this epoch\n",
        "        return mean(accuracies_src), mean(accuracies_tgt)\n",
        "    \n",
        "    def train_validate(self, source_train: DataLoader, target_train: DataLoader, target_val: DataLoader):\n",
        "        tr_cl_losses = []\n",
        "        tr_gen_losses = []\n",
        "        tr_src_accs = []\n",
        "        tr_tgt_accs = []\n",
        "        val_src_accs = []\n",
        "        val_tgt_accs = []\n",
        "        best_acc_tgt = 0.0\n",
        "        patience = self.patience\n",
        "        epochs_iter = tqdm(\n",
        "            range(self.tot_epochs), \n",
        "            unit = \"epoch\",\n",
        "            desc = \"TRAINING\")\n",
        "        \n",
        "        # Train and validate for each epoch\n",
        "        for epoch in epochs_iter:\n",
        "            self.curr_epoch = epoch\n",
        "            # Train current epoch\n",
        "            cl_loss, gen_loss, tr_acc_src, tr_acc_tgt = self.train_epoch(source_train, target_train)\n",
        "            # Store training results\n",
        "            tr_cl_losses.append(cl_loss)\n",
        "            tr_gen_losses.append(gen_loss)\n",
        "            tr_src_accs.append(tr_acc_src)\n",
        "            tr_tgt_accs.append(tr_acc_tgt)\n",
        "            # Show training results\n",
        "            epochs_iter.set_postfix({\n",
        "                \"tr_cl_loss\": round(cl_loss, 3), \n",
        "                \"tr_gen_loss\": round(gen_loss, 3),\n",
        "                \"tr_acc_on_src\": round(tr_acc_src, 3),\n",
        "                \"tr_acc_on_tgt\": round(tr_acc_tgt, 3)\n",
        "            })\n",
        "            # Validate current epoch\n",
        "            val_acc_src, val_acc_tgt = self.val_epoch(target_val)\n",
        "            # Store validation results\n",
        "            val_src_accs.append(val_acc_src)\n",
        "            val_tgt_accs.append(val_acc_tgt)\n",
        "            # Show validation results\n",
        "            epochs_iter.set_postfix({\n",
        "                \"tr_cl_loss\": round(cl_loss, 3), \n",
        "                \"tr_gen_loss\": round(gen_loss, 3),\n",
        "                \"tr_acc_on_src\": round(tr_acc_src, 3),\n",
        "                \"tr_acc_on_tgt\": round(tr_acc_tgt, 3),\n",
        "                \"val_acc_on_src\": round(val_acc_src, 3),\n",
        "                \"val_acc_on_tgt\": round(val_acc_tgt, 3)\n",
        "            })\n",
        "            # Manage patience for early-stopping\n",
        "            if epoch > self.patience and val_acc_tgt < max(val_tgt_accs[-self.patience:]):\n",
        "                # Decrease current patience\n",
        "                patience = patience - 1\n",
        "                print(f'\\n--- PATIENCE={patience} ---') \n",
        "                if patience == 0:\n",
        "                    print('\\n--- EARLY STOPPING ---')\n",
        "                    break # Interrupt iteration\n",
        "            else:\n",
        "                # Reset current patience\n",
        "                patience = self.patience\n",
        "                if val_acc_tgt > best_acc_tgt:\n",
        "                    best_acc_tgt = val_acc_tgt\n",
        "                    self.loader.model_save(\n",
        "                        model = self.model.model,\n",
        "                        optimizer = self.model.optim)\n",
        "                    print('\\n--- SAVED NEW BEST MODEL ---')\n",
        "        # Save hyperparameters \n",
        "        self.loader.save_hyperparam()\n",
        "        # Save training results\n",
        "        self.loader.save_results({\n",
        "            'tr_cl_losses': tr_cl_losses,\n",
        "            'tr_gen_losses': tr_gen_losses,\n",
        "            'tr_src_accs': tr_src_accs,\n",
        "            'tr_tgt_accs': tr_tgt_accs,\n",
        "            'val_src_accs': val_src_accs,\n",
        "            'val_tgt_accs': val_tgt_accs\n",
        "        })\n",
        "\n",
        "        # Return best avg accuracy on target  \n",
        "        return max(val_tgt_accs)\n",
        "\n",
        "    def overall_losses(self, y_source_pred, y_target_pred, y_source_true):\n",
        "        # Source task classifier loss\n",
        "        self.src_task_class_loss.y_labels = y_source_true\n",
        "        _src_task_class_loss = self.src_task_class_loss(y_source_pred)\n",
        "        # (Cross-domain) Target task classifier loss\n",
        "        self.tgt_task_class_loss.y_labels = y_source_true\n",
        "        _tgt_task_class_loss = self.tgt_task_class_loss(y_source_pred)\n",
        "        # Domain discrimination loss\n",
        "        _src_dom_discrim_loss = self.src_dom_discrim_loss(y_source_pred)\n",
        "        _tgt_dom_discrim_loss = self.tgt_dom_discrim_loss(y_target_pred)\n",
        "        _domain_discrim_loss = TrainingObjectives.domain_discrimination_loss(\n",
        "            _src_dom_discrim_loss, \n",
        "            _tgt_dom_discrim_loss)\n",
        "        # Category-level confusion loss\n",
        "        self.src_cat_conf_loss.y_labels = y_source_true\n",
        "        self.tgt_cat_conf_loss.y_labels = y_source_true\n",
        "        _src_cat_conf_loss = self.src_cat_conf_loss(y_source_pred)\n",
        "        _tgt_cat_conf_loss = self.tgt_cat_conf_loss(y_source_pred)\n",
        "        _category_conf_loss = TrainingObjectives.category_confusion_loss(\n",
        "            _src_cat_conf_loss, \n",
        "            _tgt_cat_conf_loss)\n",
        "        # Domain-level confusion loss\n",
        "        _src_dom_conf_loss = self.src_cat_conf_loss(y_target_pred)\n",
        "        _tgt_dom_conf_loss = self.tgt_cat_conf_loss(y_target_pred)\n",
        "        _domain_conf_loss = TrainingObjectives.domain_confusion_loss(\n",
        "            _src_dom_conf_loss, \n",
        "            _tgt_dom_conf_loss)\n",
        "        # Entropy minimization loss\n",
        "        _tgt_entropy_loss = self.tgt_entropy_loss(y_target_pred)\n",
        "        # Overall classifier loss\n",
        "        _overall_classifier_loss = TrainingObjectives.overall_classifier_loss(\n",
        "            _src_task_class_loss, \n",
        "            _tgt_task_class_loss, \n",
        "            _domain_discrim_loss)\n",
        "        # Overall feature extractor loss\n",
        "        _overall_generator_loss = TrainingObjectives.overall_generator_loss(\n",
        "            _category_conf_loss, \n",
        "            _domain_conf_loss, \n",
        "            _tgt_entropy_loss, \n",
        "            self.curr_epoch, \n",
        "            self.tot_epochs)\n",
        "        # Return obtained overall losses\n",
        "        return _overall_classifier_loss, _overall_generator_loss"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Jwa_O2xT0G9u"
      },
      "source": [
        "\n",
        "# Source-Only Architecture"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZtZKQbfAgV6Z"
      },
      "source": [
        "Suppose you’re working on the direction product → real world. Then the first thing you will do is train your model on $P_{train}$. Since this is your <i>source domain </i>, you are allowed to use label information (e.g. use a cross entropy loss in your training step). In your test step, you are going to evaluate the model on $RW_{test}$. This will achieve a certain accuracy; since we only trained on the source domain, and not on the target domain, this accuracy refers to the source-only scenario. We call it $acc_{so}$. Now you want to evaluate your UDA component which, differently from the former case, implies training on the target domain. Since you are not allowed to use labels there, here you will use any UDA device of your choice. So, in this case, in your training step, you will train supervisedly on $P_{train}$ (like you did before) and simultaneously train unsupervisedly on $RW_{train}$. In your test step, once again, you want to evaluate on $RW_{test}$. This will achieve a new accuracy $acc_{uda}$, which hopefully will be higher than $acc_{so}$ since this time you also trained on the target domain, even if without label information. At this point you can compute your gain $G$:\n",
        "$$G = acc_{uda} − acc_{so}$$"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "s7bKkf9g0G9u"
      },
      "outputs": [],
      "source": [
        "class SourceModelTrainer:\n",
        "    \n",
        "    def __init__(self, model: FeatureExtractor, so_loader: ModelLoader, n_classes: int, epochs: int, patience: int):\n",
        "        \"\"\"Initialize the SymsNet model\n",
        "        Args:\n",
        "            model (FeatureExtractor): _description_\n",
        "            n_classes (int): _description_\n",
        "            epochs (int): _description_\n",
        "        \"\"\"\n",
        "        self.patience = patience\n",
        "        self.tot_epochs = epochs\n",
        "        self.n_classes = n_classes\n",
        "        self.so_loader = so_loader\n",
        "        self.model = model \n",
        "        self.loss = CrossEntropyLoss()\n",
        "        if cuda.is_available(): \n",
        "            self.loss = self.loss.cuda()\n",
        "            \n",
        "    def train_step(self, X_source: Tensor, y_source: Tensor):\n",
        "        # Tell model go training mode\n",
        "        self.model.model.train()\n",
        "        # Compute features for both inputs\n",
        "        y_source_pred = self.model.model(X_source)\n",
        "        # Compute overall training objective losses\n",
        "        y_src = onehot_encoding(y_source, self.n_classes)\n",
        "        general_loss = self.loss(y_source_pred, y_src)\n",
        "        # Compute gradients w.r.t. classifier loss\n",
        "        self.model.optim.zero_grad()\n",
        "        general_loss.backward()\n",
        "        # Perform optimizer step    \n",
        "        self.model.optim.step()\n",
        "        y_true = y_source.clone().tolist()\n",
        "        y_pred = y_source_pred.clone()\n",
        "        y_pred = torch.argmax(y_pred, dim=1).tolist()\n",
        "        return accuracy_score(y_true, y_pred)\n",
        "    \n",
        "    def train_epoch(self, source_train: DataLoader):\n",
        "        end_of_epoch = False\n",
        "        source_batch_loader = enumerate(source_train)\n",
        "        acc = []\n",
        "        # Train for current epoch\n",
        "        while not end_of_epoch:\n",
        "            try:\n",
        "                # Get next batch for both source and target\n",
        "                (X_source, y_source) = source_batch_loader.__next__()[1]\n",
        "                if cuda.is_available():\n",
        "                    X_source = X_source.cuda()\n",
        "                    y_source = y_source.cuda()\n",
        "                accuracy = self.train_step(X_source, y_source)\n",
        "                acc.append(accuracy)\n",
        "            except StopIteration: \n",
        "                end_of_epoch = True\n",
        "        return mean(acc)\n",
        "    \n",
        "    \n",
        "    def val_step(self, X_target: Tensor, y_target: Tensor):\n",
        "        self.model.model.eval()\n",
        "        with torch.no_grad():\n",
        "            y_target_pred = self.model.model(X_target)\n",
        "        # Calculate accuracy\n",
        "        y_target_true = y_target.clone().tolist()\n",
        "        y_target_pred = y_target_pred.clone()\n",
        "        y_target_pred = torch.argmax(y_target_pred, dim=1).tolist()\n",
        "        return accuracy_score(y_target_true, y_target_pred)\n",
        "    \n",
        "    \n",
        "    def val_epoch(self, target_val: DataLoader):\n",
        "        end_of_epoch = False\n",
        "        target_batch_loader = enumerate(target_val)\n",
        "        accuracies = []\n",
        "        # Validate current epoch\n",
        "        while not end_of_epoch:\n",
        "            try:\n",
        "                # Get next batch for target\n",
        "                (X_target, y_target) = next(target_batch_loader)[1]\n",
        "                if cuda.is_available():\n",
        "                    X_target = X_target.cuda()\n",
        "                    y_target = y_target.cuda()\n",
        "                # Apply validation step\n",
        "                acc = self.val_step(X_target, y_target)\n",
        "                # Append accuracy\n",
        "                accuracies.append(acc)\n",
        "            except StopIteration: \n",
        "                end_of_epoch = True\n",
        "        # Return average validation accuracy for this epoch\n",
        "        return mean(accuracies)\n",
        "    \n",
        "        \n",
        "    def train_validate(self, source_train: DataLoader, target_val: DataLoader):\n",
        "        val_accuracies = []\n",
        "        epochs_iter = tqdm(\n",
        "            range(self.tot_epochs), \n",
        "            unit = \"epoch\",\n",
        "            desc = \"SOURCE-ONLY TRAINING\")\n",
        "        for e in epochs_iter:\n",
        "            # Train current epoch\n",
        "            self.train_epoch(source_train)\n",
        "            # Validate current epoch\n",
        "            acc_val = self.val_epoch(target_val)\n",
        "            # Store validation result \n",
        "            val_accuracies.append(acc_val)\n",
        "            # Show validation results\n",
        "            epochs_iter.set_postfix({\n",
        "                \"val_accuracy_domain_shift\": round(acc_val, 3)\n",
        "            })\n",
        "\n",
        "            if e > self.patience and acc_val < max(val_accuracies[-self.patience:]):\n",
        "                # Decrease current patience\n",
        "                patience = patience - 1\n",
        "                print(f'\\n--- PATIENCE={patience} ---') \n",
        "                if patience == 0:\n",
        "                    print('\\n--- EARLY STOPPING ---') \n",
        "                    break # Interrupt iteration\n",
        "            else: \n",
        "              patience = self.patience\n",
        "        \n",
        "        self.so_loader.save_results({\n",
        "            'val_accuracies': val_accuracies\n",
        "            })\n",
        "        # Return best avg accuracy on target  \n",
        "        return max(val_accuracies)\n",
        "\n",
        "\n",
        "def onehot_encoding(input: Tensor, n_classes: int) -> Tensor:\n",
        "    # set the device \n",
        "    dev = 'cuda:0' if cuda.is_available() else 'cpu:0'\n",
        "    # transform the target labels into a long tensor object\n",
        "    input = torch.tensor(input, dtype=torch.long, device=dev)\n",
        "    # One-hot encode the target labels \n",
        "    input = F.one_hot(input, num_classes=n_classes)\n",
        "    # cast the hot encoded long tensor object into a float tensor object\n",
        "    # to ensure type compatibibility with the probabilities tensor object (i.e., prob)\n",
        "    output = input.type(torch.float)\n",
        "    return output"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qvrTPgYTbGh2"
      },
      "source": [
        "# Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 946
        },
        "id": "Lpm0K41_0G9t",
        "outputId": "7e61d718-4555-41db-ac92-17cdff64b507"
      },
      "outputs": [],
      "source": [
        "loader = ModelLoader('/content')\n",
        "\n",
        "uda_generator = FeatureExtractor(n_classes, model=model, optimizer=optim, \n",
        "                                n_params_trained=n_params_trained, \n",
        "                                lr=lr, weight_decay=weight_decay, \n",
        "                                dropout=dropout, source_only=False)\n",
        "\n",
        "symnet = ModelTrainer(uda_generator, loader, n_classes, n_epochs, patience)\n",
        "\n",
        "# Train on both source and target, validate only on target \n",
        "acc_uda = symnet.train_validate(source_train_loader, target_train_loader, target_val_loader)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "u4P3VCrH0G9u"
      },
      "outputs": [],
      "source": [
        "so_loader = ModelLoader('/content', source_only = True)\n",
        "so_generator = FeatureExtractor(n_classes, model=model, optimizer=optim, \n",
        "                                n_params_trained=n_params_trained, \n",
        "                                lr=lr, weight_decay=weight_decay, \n",
        "                                source_only=True)\n",
        "\n",
        "source_only = SourceModelTrainer(so_generator, so_loader, n_classes, n_epochs, patience)\n",
        "\n",
        "# Train only on source, validate only on target \n",
        "acc_so = source_only.train_validate(source_train_loader, target_val_loader)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zeibzHYibGh7"
      },
      "outputs": [],
      "source": [
        "# Evaluate the performance gain between the two architectures \n",
        "overall_gain_perc = lambda acc_uda, acc_so: (abs(acc_so - acc_uda)/acc_so)*100\n",
        "overall_gain_abs = lambda acc_uda, acc_so: (acc_uda - acc_so)\n",
        "\n",
        "print(f'Percentage gain = {round(overall_gain_perc(acc_uda, acc_so), 2)} %')\n",
        "print(f'Absolute gain = {round(overall_gain_perc(acc_uda, acc_so), 2)}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R9SsQkRz1zSl"
      },
      "source": [
        "##### Memory Tools"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 89,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QraheZZx0sre",
        "outputId": "9788119b-f026-4455-9c73-b4b3afc09e69"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Sun Aug  7 10:38:38 2022       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 460.32.03    Driver Version: 460.32.03    CUDA Version: 11.2     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla P100-PCIE...  Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   39C    P0    34W / 250W |   1827MiB / 16280MiB |      0%      Default |\n",
            "|                               |                      |                  N/A |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "+-----------------------------------------------------------------------------+\n",
            "Deleting loader...\n",
            "Deleting generator...\n",
            "Deleting symnet...\n",
            "Sun Aug  7 10:38:38 2022       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 460.32.03    Driver Version: 460.32.03    CUDA Version: 11.2     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla P100-PCIE...  Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   39C    P0    34W / 250W |   1447MiB / 16280MiB |      0%      Default |\n",
            "|                               |                      |                  N/A |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "+-----------------------------------------------------------------------------+\n"
          ]
        }
      ],
      "source": [
        "import gc\n",
        "\n",
        "!nvidia-smi\n",
        "\n",
        "if 'loader' in globals() or 'loader' in locals():\n",
        "    print('Deleting loader...')\n",
        "    del loader\n",
        "if 'uda_generator' in globals() or 'uda_generator' in locals():\n",
        "    print('Deleting generator...')\n",
        "    del uda_generator\n",
        "if 'symnet' in globals() or 'symnet' in locals():\n",
        "    print('Deleting symnet...')\n",
        "    del symnet\n",
        "\n",
        "gc.collect()\n",
        "torch.cuda.empty_cache()\n",
        "\n",
        "!nvidia-smi"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Show Results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "filenames = [\n",
        "    'all',\n",
        "    'no_lr_reduct',\n",
        "    'no_asgd',\n",
        "    'no_dropout_embed',\n",
        "    'no_dropout_hidden',\n",
        "    'no_dropout_input',\n",
        "    'no_dropout_layer',\n",
        "    'no_tied_weights',\n",
        "    'no_weight_decay',\n",
        "]\n",
        "to_label = {\n",
        "    'all': 'all regularizations',\n",
        "    'no_lr_reduct': 'no learning rate reduction',\n",
        "    'no_asgd': 'no averaged sgd',\n",
        "    'no_dropout_embed': 'no embedding dropout',\n",
        "    'no_dropout_hidden': 'no hidden-to-hidden dropout',\n",
        "    'no_dropout_input': 'no input dropout',\n",
        "    'no_dropout_layer': 'no final layer dropout',\n",
        "    'no_tied_weights': 'no weights tying',\n",
        "    'no_weight_decay': 'no weight decay'\n",
        "}\n",
        "to_color = {\n",
        "    'all': 'xkcd:black',\n",
        "    'no_lr_reduct': 'xkcd:neon purple',\n",
        "    'no_asgd': 'xkcd:blue',\n",
        "    'no_dropout_embed': 'xkcd:grass green',\n",
        "    'no_dropout_hidden': 'xkcd:lime green',\n",
        "    'no_dropout_input': 'xkcd:bright yellow',\n",
        "    'no_dropout_layer': 'xkcd:dirty yellow',\n",
        "    'no_tied_weights': 'xkcd:tangerine',\n",
        "    'no_weight_decay': 'xkcd:bright red'\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "results = {}\n",
        "for fn in filenames:\n",
        "    with open(f'tests/{fn}_results.json', 'r') as f:\n",
        "        results[fn] = json.load(f)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "_BASELINE = 90.7\n",
        "max_len = max(len(results[k]['avg_val_ppls']) for k in results.keys())\n",
        "sns.set_theme()\n",
        "fig, ax = plt.subplots()\n",
        "for k in results.keys():\n",
        "    val_ppls = results[k]['avg_val_ppls']\n",
        "    epochs = range(len(val_ppls))\n",
        "    ax.plot(epochs, val_ppls, label=to_label[k], color=to_color[k])\n",
        "    if len(val_ppls) < max_len:\n",
        "        x = val_ppls[-1]\n",
        "        y = len(val_ppls)-1\n",
        "        ax.scatter([y], [x], marker='x', color=to_color[k])\n",
        "epochs = range(max_len)\n",
        "baseline = [_BASELINE for _ in epochs]\n",
        "ax.plot(epochs, baseline, '--', label='baseline', color='xkcd:light grey')\n",
        "fig.set_figwidth(15)\n",
        "fig.set_figheight(10)\n",
        "fig.set_dpi(300)\n",
        "plt.rc('font', size=20)\n",
        "plt.rc('axes', titlesize=20)\n",
        "plt.rc('axes', labelsize=20)\n",
        "plt.rc('xtick', labelsize=20)\n",
        "plt.rc('ytick', labelsize=20)\n",
        "plt.rc('legend', fontsize=20)\n",
        "plt.xlim(right=max_len)\n",
        "plt.ylim(top=300)\n",
        "plt.legend(loc='upper right')\n",
        "plt.show()\n",
        "fig.savefig(f'tests/{fn}_plot.png')"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "W5if2JDi0ic6",
        "eG_KkWoF1iLA",
        "zzwgGaFW2VpR",
        "QhWEa-5C0G9l",
        "ya6nhiHiDhkM",
        "PzHiEzWR0G9p",
        "lS8yTMu10G9r",
        "En10ic2_0G9s",
        "Jwa_O2xT0G9u"
      ],
      "machine_shape": "hm",
      "name": "SymNets_v05.ipynb",
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3.9.13 ('venv': venv)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.13"
    },
    "vscode": {
      "interpreter": {
        "hash": "6f0d290f0742685d306541f8dcebbe79a177e37269f78587a0fc5052fa8d446c"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
